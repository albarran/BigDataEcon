---
subtitle: "Técnicas para 'Big Data' en Economía - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Análisis de Datos Multivariantes aplicado al Marketing - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Muestreo y Análisis de Datos - Universidad de Alicante"
# subtitle: "Econometría II - Curso 2023/24 \n\n Universidad de Alicante"
title    :  "Tema 07 - Selección y Regularización"
author:  
    - "Pedro Albarrán"
#    - "Teresa Molina"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
   
# institute: 
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  # beamer:
  #   handout: false
  #   logo: figure/by-nc-sa2.png
  #   titlegraphic: figure/by-nc-sa.png
  #   theme:  Boadilla # Copenhagen # CambridgeUS #
  #   outertheme: miniframes
  #   colortheme: crane
  #   section-titles: false
  #   fontsize: 10pt
  #   header-includes: |
  #       \setbeamertemplate{footline}
  #       {
  #       \leavevmode%
  #       \hbox{%
  #       \begin{beamercolorbox}[wd=.30\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
  #       \usebeamerfont{author in head/foot}\insertshortauthor%
  #       \end{beamercolorbox}%
  #       \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
  #       \usebeamerfont{title in head/foot}\insertshorttitle%
  #       \end{beamercolorbox}%
  #       \begin{beamercolorbox}[wd=.15\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
  #       \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber
  #       \end{beamercolorbox}}%
  #       }
  #      # - \setbeamertemplate{navigation symbols}{}
  #      # - \setbeamertemplate{caption}[numbered]
  #      # - \setbeamertemplate{headline}[page number]
  #      # - \setbeameroption{show notes}
  #      # - \setbeameroption{show notes on second screen}
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:  
        - beige # sky # serif # simple # default # moon #  
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide # concave # 
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    # width: 3000
    # height: 2000
    # margin: 0.05
execute:
  enabled: true      # (no) ejecutar code chunks
  eval: false        # por defecto, evalúa y muestra códido de code chunks
  echo: true
  warning: false    # pero no los mensajes ni warnings
  message: false
knitr:
  opts_chunk:
    results: hide     # ni muestra resultados ni figuras
    fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---

```{r setup, include=FALSE}
#```{r setup, message=FALSE, warning=FALSE, include=FALSE} 
# include=F es suficiente para no incluir mensajes, etc.

# Opciones por defecto para los fragmentos de código
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      results = "hide", fig.show="hide")
# se muestra y evalúa el código,
# no se muestran mensajes, ni avisos (warnings)
# no se muestran los resultados de código (tampoco gráficos)
#     en los códigos que considere necesarios los mostraré

# Elimino todo del Entorno (del documento)
rm(list = ls())       

# Cargo todas las bibliotecas necesarias
# (se podría hacer cuando cada una sea necesaria)
library(tidyverse)
# library(tidymodels)
# library(printr)
# library(skimr)
# library(dlookr)
# library(broom)
library(kableExtra)
# library(rpart.plot)
# library(vip)

#fijo el directorio de trabajo
#setwd("/home/albarran/Dropbox/MAD/00.TEC")
library(rmarkdown)
#render("filename.Rmd")     
#browseURL("filename.html")
```

## Introducción

<!--
$\small Y = \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k + \varepsilon$
-->

* MCO puede estimar modelos con muchos regresores (ej., polinomios e interacciones para relaciones no lineales), pero ¿qué variables incluimos?

* Cuando crece el número de parámetros $\small k$ relativo al de observaciones $\small n$:
    - menor **precisión** (+ varianza) $\Rightarrow$ no solución única de MCO con $\small k>n$
    - modelo complejo y menos **interpretable**$\Rightarrow$ selección de variables

<!--
*  **Precisión**: incluso si la verdadera relación es lineal, mayor varianza  cuando crece el número de parámetros/regresores relativo al de observaciones $\Rightarrow$ no solución única de MCO con $\small k>n$ 


    + con $\small k>n$ no existe una estimación única de mínimos cuadrados 


* **Interpretación**: un modelo con variables irrelevantes <!-- no asociadas a la de respuesta es más complejo y menos interpretable $\Rightarrow$ selección de variables
-->

1. **Selección** de variables: excluir variables irrelevantes y  ajustar ese modelo reducido por mínimos cuadrados ordinarios

2. **Restricción** de los coeficientes estimados puede reducir la varianza, a costa de un aumento insignificante del sesgo (Regresión Regularizada/Penalizada) <!-- *Ridge Regression*/LASSO -->

3. **Reducción de la dimensionalidad**: usar $\small M<k$ combinaciones lineales (proyecciones) y estimar por mínimos cuadrados  (*PCR*, *PLS*)

<!-- *Partial Least Square*-->

<!-- PCR: Principal Components Regression -->

<!--

## Métodos de selección de variables 

* Selección del **mejor subconjunto**: estimar $\small 2^k$ modelos posibles con cada combinación de regresores y elegir aquel con menor error $\Rightarrow$ prohibitivo!

* Alternativas factibles (consideran un menor número de modelos)

    1. Selección paso a paso **hacia adelante**: añadir un regresor cada vez, eligiendo aquel con menor error en cada paso 
 o eligiendo según p-valor 

    2. Selección paso a paso **hacia atrás** (no factible si $\small k>n$)

    3. **Selección mixta**: en cada paso se añaden variables de forma secuencial, pero también se eliminan algunas (p-valor alto)

* No tienen criterio riguroso, no llevan a la misma solución y no garantizan encontrar el mejor subconjunto (ej., elimina pronto un regresor importante)
-->

<!--
* Se pueden usar también en regresión logística
-->

<!--
## Selección  del Mejor Subconjunto 

* Debemos estimar $\small 2^k$ modelos posibles con cada combinación de regresores (desde un solo regresor hasta todos a la vez)

* Usar SCR en entrenamiento lleva a elegir el modelo con $\small k$ parámetros

* Procedimiento:

  1. Para cada $\small p=1,\dots,k$, estimar todos los modelos con $p$ parámetros y elegir aquel con menor error (ej., SCR): $\small M^*_p$
  
  2. Elegir entre los modelos $\small M^*_1, \dots, M^*_k$ usando validación cruzada o similar

* No validamos todos, pero estimarlos es prohibitivo para $\small k$ moderada
-->

<!--
## Selección paso a paso hacia adelante 

* Procedimiento: empezando por modelo sin regresores $\small M_0$

  1. Para cada $\small p=0, 1,\dots,k-1$, estimar todos los modelos que añadan UN regresor a $\small M^f_p$
  2. Elegir como modelo $\small M^f_{p+1}$ el que tiene menor SCR
  3. Elegir entre  $\small M_0, M^f_1, \dots, M^f_k$ con validación cruzada o similar
-->

<!--
* Muchos menos modelos: $\small k-p$ por iteración, en total $\small 1+\frac{p(p+1)}{2}$
-->

<!--
* Solo $\small 1+\frac{k(k+1)}{2}$ modelos
* Factible aunque $\small k>n$ pero para modelo $M_0,\dots,M^f_{n-1}$

* No se garantiza encontrar el mejor subconjunto, por eliminar pronto un regresor importante
    + ej. el mejor $\small M^*_2$ no usa el regresor de  mejor modelo de un regresor $\small M^f_1=M^*_1$

-->

<!--
## Selección paso a paso hacia atrás

* Procedimiento: empezando por modelo con todos los regresores $\small M_k$
  1. Para cada $\small p=k, k-1, \dots,1$, estimar todos los modelos que eliminan UN regresor a $\small M^b_p$
  2. Elegir como modelo $\small M^b_{p-1}$ el que tiene menor SCR
  3. Elegir entre $\small M_0, M^b_1, \dots, M^b_k$ usando validación cruzada o similar

* Solo $\small 1+\frac{k(k+1)}{2}$ modelos
* Pero no factible si $\small k>n$ (no se puede ajustar $M_{k}$)

* NADA garantiza acabar con el mejor subconjunto de regresores
-->

<!--
## Otros procedimientos

* **Selección mixta** de subconjuntos: en cada iteración se añaden variables de forma secuencial, pero también se eliminan las que ya no mejoren el ajuste

    + simulan la selección de mejores subconjuntos, con las ventajas computacionales de selección por pasos.

* Estimar *directamente* el error de prueba mediante validación cruzada

* Estimar *indirectamente* el error de prueba mediante **ajustes** en el error de entrenamiento para tener en cuenta el sesgo por "overfitting"

* Todos estos métodos (y los anteriores de selección) se pueden usar también en regresión logística
-->

## Ajustes mediante penalización 

<!--
* Sea $\small n$ el número de observaciones, $\small k$ el número de parámetros y $\small \widehat{\sigma}^2$ un estimación de la varianza del error $\small \varepsilon$
-->
* Los métodos de ajuste ofrecen una ***estimación indirecta** del error de prueba, en la muestra de entrenamiento mediante supuestos (erróneos?) 

* El $\small SCR$ de entrenamiento siempre se reduce si el modelo es más flexible $\Rightarrow$ añadir una penalización por número de parámetros

    * Criterio de Información de Akaike: $\small AIC = \frac{1}{n}\left( SCR + 2 k \widehat{\sigma}^2 \right)$
    
        + $\small \widehat{\sigma}^2$ un estimación de la varianza del error

    * Criterio de Información Bayesiano: $\small BIC =  \frac{1}{n}\left( SCR + log(n) k \widehat{\sigma}^2 \right)$

    * $\small R^2-ajustado = 1- \frac{SCR/(n-k-1)}{SCT/(n-1)}$ <!--o $\small SCR/(n-k-1)$ -->


<!--
* ¿Ajustar o Validar?
-->

  + Validación cruzada ofrece una estimación **más directa** pero es **más costosa** (computacionalmente, menor tamaño muestral para estimar)

<!--
  + Validación cruzada NO necesita estimar $\small \widehat{\sigma}^2$ (difícil en algunos modelos)
-->

<!--
## Métodos de regularización

* Alternativa a mínimos cuadrados con selección de regresores

* Ajustar un modelo que contenga **todos** los regresores, PERO con una técnica que limite las estimaciones de los coeficientes, o las reduzca a cero.

* A priori, NO es obvio por qué esa restricción debería mejorar el ajuste, pero esto reduce su varianza

* Dos enfoques
    + "Ridge regression": se reducen los coeficientes
    + LASSO ("least absolute shrinkage and selection operator"): selección automática de regresores

* Regresión de red elástica, incorpora ambos

-->

## Métodos de regularización

* En *MCO*: $\small \min_{\beta} SCR={\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$

* En Regresión Penalizada o Regularizada, se añade una restricción que limite (reduzca) los coeficientes estimados $$\small \min_{\beta} SCR \text { sujeto a }R(\beta) \leq t$$

  + donde $R(\cdot)$ es <!--una función de penalización--> una medida del tamaño de los coeficientes

  + NO se penaliza a la constante (media de $\small Y$), solo el impacto de $\small X$  


* La restricción limita la importancia de las $\small X$ para explicar $\small Y$: empeora el ajuste (sesgo), pero reduce la varianza $\Rightarrow$ previene "overfitting"

* Permite ajustar un modelo que contenga **todos** los regresores



## Métodos de regularización (cont.)

* Reescribiendo el problema (Lagrangiano): $\small \min_{\beta} SCR+\lambda R(\beta)$

* $\lambda \geq 0$ es un parámetro de ajuste ("tuning parameter")


|  Método          |  Penalización por tamaño = $R(\boldsymbol{\beta})$                                      |    Norma                                    |
|:-----------------|:----------------------------------------------------------------------------------------|:--------------------------------------------|
| MCO              | 0                                                                                       |                                             |
| LASSO            | $\lVert\boldsymbol{\beta}\rVert_1=\sum_{j=1}^{k}|\beta_j|$                              |$\ell_1$: $||\boldsymbol{\beta}||_1=\sum_{j=1}^{k}|\beta_j|$ |
| Ridge Regression | $\lVert\boldsymbol{\beta}\rVert_2^2 =\sum_{j=1}^{k}\beta_j^2$                           |$\ell_2$: $||\boldsymbol{\beta}||_2=\sqrt{\sum_{j=1}^{k}\beta_j^2}$     |
| Red Elásica      | $\alpha\lVert\boldsymbol{\beta}\rVert_1 + (1-\alpha)\lVert\boldsymbol{\beta}\rVert_2^2$ |                                             |


<!--
  - $\ell_1$-norm: $||\boldsymbol{\beta}||_1=\sum_{j=1}^{k}|\beta_j|$.  

  - $\ell_2$-norm: $||\boldsymbol{\beta}||_2=\left(\sum_{j=1}^{k}|\beta_j|^2\right)^{\frac{1}{2}}$, i.e., Euclidean norm.  

* LASSO = *least absolute shrinkage and selection operator*
-->

<!--
## "Ridge Regression"

* En *MCO*: $\small \min_{\beta}=SCR={\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$

* Añadir restricciones para prevenir "overfitting"  $\small \sum_{j=1}^{k}\beta_j^2 \leq c$

* Se obtiene un coeficiente estimado $\small \widehat{\beta}^R_{\lambda}$ que minimiza
\[
SCR + \lambda \sum_{j=1}^{k}\beta_j^2 = SCR + \lambda ||\beta||_2^2
\]

  + $||\beta||_2 = \sqrt{\sum_{j=1}^{k}\beta_j^2}$ es la norma L2 ($\ell_2$) del vector de coeficientes

  + $\lambda \geq 0$ es un parámetro de ajuste ("tuning parameter")
-->

## Penalización de contracción



:::: {.columns}

::: {.column}

* Para un $\small \lambda$ dado:

$\small \widehat{\beta}^R_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{k}\beta_j^2$

:::

::: {.column}

$$
\small
\widehat{\beta}^L_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{k}|\beta_j| 
$$

::: 
::::

*  Tratamos de ajustarnos a los datos minimizando SCR, PERO se recompensa a los coeficientes cercanos a cero

*  Para que todos los coeficientes estén en la **misma escala** (misma "cercanía a cero), debemos **estandarizar los regresores**: $\small \widetilde{x}_{ij} = \frac{x_{ij}}{\sqrt{ \frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}$

    * Recordar: en MCO, el coeficiente $\small \beta_j$ cambia si cambiamos las unidades de $\small X_j$

<!--
    * En MCO, si multiplicamos $\scriptsize X_j$ por $\scriptsize c$, su coeficiente estimado se reescala por $\scriptsize 1/c$: el valor predicho $\scriptsize \widehat{\beta_j}X_j$ no cambia $\Rightarrow$ $SCR$ no cambia

    * En Regresión Regularizada, la penalización sí cambia al reescalar $\Rightarrow$ los coeficientes estimados pueden cambiar drásticamente
-->


<!--
* Los coeficientes estimados por regresión regularizada pueden cambiar drásticamente después de reescalar cualquier variable
-->




<!--
## "Ridge Regression": penalización de contracción

\[\small
\widehat{\beta}^R_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{k}\beta_j^2 
\]

* Tratamos de ajustarnos a los datos minimizando SCR, PERO se recompensa a los coeficientes cercanos a cero: **penalización de contracción** 

* NO se penaliza a la constante (media de $\small Y$), solo el impacto de $\small X$

* $\small \lambda$= importancia de la penalización (cuanto se contraen los coeficientes)

    + $\small \lambda \approx 0$, cercano a MCO 
    
    + $\small \lambda >> 0$, todos los coeficientes se van a cero

* Ventaja sobre la selección de regresores: SOLO necesitamos ajustar un modelo para cada valor de $\small \lambda$
-->

<!--
## Advertencia: estandarizar los regresores


* En MCO los coeficientes estimados son equivariantes a la escala de los regresores. 

  * Si multiplicamos $\small X_j$ por una constante, $\small c$, el coeficiente estimado se reescala por $\small 1/c$ y el valor predicho $\widehat{\beta_j}X_j$ sigue siendo el mismo. 

  * $SCR$ no cambia cuando se reescala un regresor

* En Regresión Regularizada, NO, porque la penalización sí cambia al reescalar $\Rightarrow$ los coeficientes pueden cambiar drásticamente


* Los coeficientes estimados por regresión regularizada pueden cambiar drásticamente después de reescalar cualquier variable


* Se recomienda ajustar **después de estandarizar** los regresores:
\[
\small
\widetilde{x}_{ij} = \frac{x_{ij}}{\sqrt{ \frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}
\]

-->

## Regularización y "Trade-off" sesgo-varianza

* ¿Por qué la regularización mejoraría el ajuste sobre MCO? 

* $\small \lambda$= importancia de la penalización (cuanto se contraen los coeficientes)

    + $\small \lambda$ pequeño (cercano a MCO): mayor flexibilidad ($-$ sesgo, $+$ varianza)
    
    + $\small \lambda >> 0$, todos los coeficientes a cero: menor flexibilidad ($+$ sesgo, $-$ varianza)

* Regularización funciona mejor cuando MCO tiene alta varianza: intercambia un poco más de sesgo por una gran reducción de la varianza

* "Ridge Regression" <!--NO realiza selección de variables:--> sigue incluyendo *todos* los regresores (ningún coeficiente exactamente cero): puede complicar la interpretación con muchos

* LASSO (*least absolute shrinkage and selection operator*): también contrae <!--los coeficientes estimados--> hacia cero, algunos **exactamente cero**  (selección de variables)


<!--
## LASSO

* Idea similar $\small \min_{\beta}=SCR={\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$, sujeto a $\small \sum_{j=1}^{k}|\beta_j| \leq c$

<!--
* Selección de mejor conjunto impone restricción
$\small \sum_{j=1}^{k} I(\beta_j \neq 0) \leq s$

Tampoco es factible: requiere considerar todos los modelso que tiene s regresores

LASSO/ridge  más factibles computacionalmente: sustituyen unas restricciones intratablse por alternativas mucho más fáciles de resolver
-->

<!--
\[
\widehat{\beta}^L_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{k}|\beta_j| 
\]

* LASSO utiliza una penalización basada en la norma L1 ($\small \ell_1$): 
\[\small
||\beta||_1 = \sum_{j=1}^{k} |\beta_j|
\]

* También contrae los coeficientes estimados hacia cero, PERO obliga algunos a ser **exactamente iguales a cero** cuando $\small \lambda$ es grande

* LASSO realiza la selección de variables.

-->

## "Ridge Regression" y LASSO

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px; "}

::: {}
<center>
![](figure/lasso.png){width=100%}
</center>
::: 

::: {}
* "Ridge regression" domina con muchos regresores igualmente importantes

* LASSO con pocos regresores importantes y muchos inútiles

* LASSO es una alternativa a los contrastes de significatividad (sin formalización estadística)
::: 
::::



* Notad que LASSO es un método orientado a la predicción: NO se debe usar para afirmaciones de causalidad (los coeficientes están sesgados)

* PERO se puede estimar por MCO la especificación de variables seleccionada por LASSO

<!--
## LASSO vs. "Ridge Regression"

* Ambos reducen significativamente la varianza a expensas de un pequeño aumento del sesgo 

* "Ridge regression" domina cuando hay muchos regresores igualmente importantes

* LASSO domina cuando hay un pequeño número de regresores importantes y muchos otros que no son útiles

* Generalización: Regresión de red elástica

\[\small
\min_{\beta}=SCR+\lambda \big[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\big]
\]
-->

## Eligiendo el (hiper-)parámetro de ajuste

1. Elegir un rango de valores para $\small \lambda$ 
  
2. Calcular el error mediante **validación cruzada** para cada valor de $\small \lambda$
  
3. Seleccionar el valor con menor error (probar varios rangos para encontrar forma de U)
  
4. Volver a ajustar el modelo usando todas las observaciones y el valor del parámetro de ajuste seleccionado.


* Ventaja sobre la selección de regresores: SOLO necesitamos ajustar un modelo para cada valor de $\small \lambda$

* **Regla de parquedad paramétrica**<!-- o **de un error estándar**-->: dado un conjunto de modelos igualmente buenos (dentro de un error estándar del menor error), elegir el más simple 

<!--
    + seleccionar el modelo con menos variables que esté dentro de un error estándar del menor error de prueba estimado. 
-->
  
## `glmnet` para regresión lineal

```{r}
library(mosaicData)
library(glmnet)

x <- model.matrix(data = RailTrail, 
          volume ~ cloudcover + weekday + precip + poly(hightemp, 6))

fit.lmreg <- glmnet(x = x, y = RailTrail$volume, 
                    family="gaussian", 
                    lambda=2, alpha=.5)

fit.lmreg$beta
```

* Elegimos el parámetro de regularización mediante validación cruzada

```{r}
set.seed(1)
cv.glmnet(x, RailTrail$volume) %>% plot()
cv.glmnet(x, RailTrail$volume)
```

## `glmnet` para regresión logística

```{r}
#| echo: false

censo <- read_csv("data/census.csv") %>%
  mutate(across(where(is.character), ~parse_factor(.x)),
         income = if_else(income == ">50K", 1,  0))

x <- model.matrix(data = censo, 
        income ~ race + poly(age, 3) + log(hours_per_week)*sex + 
          education*race + occupation)

fit.glmreg <- glmnet(x = x, y = censo$income, 
                     family = "binomial",
                     lambda=0.001, alpha=1)
fit.glmreg$beta

# validación cruzada para elegir parámetro de regularización
set.seed(1)  
cv.glmnet(x, censo$income)
cv.glmnet(x, censo$income) %>% plot()
```


```{r}
censo <- read_csv("data/census.csv") %>%
  mutate(across(where(is.character), ~parse_factor(.x)),
         income = if_else(income == ">50K", 1,  0))

x <- model.matrix(data = censo, 
      income ~ poly(age, 3) + log(hours_per_week)*sex +
        education_1*race + occupation*workclass)

fit.glmreg <- glmnet(x = x, y = censo$income,
                     family = "binomial", 
                     lambda=0.001, alpha=1)
fit.glmreg$beta
```

* Validación cruzada para elegir parámetro de regularización

```{r}
set.seed(1)  
cv.glmnet(x, censo$income)
cv.glmnet(x, censo$income) %>% plot()
```
