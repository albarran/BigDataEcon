---
title: "Tema 07 - Modelización con `tidymodels`"
subtitle: "Flujo Completo de Especificación, Ajuste y Evaluación de Modelos"
author:
    - "Pedro Albarrán"
    - "Alberto Pérez"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:
        - beige
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    show-notes: false
    notes-format: html
execute:
  enabled: true
  eval: false
  echo: true
  warning: false
  message: false
  output: false
  fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---

```{r}
#| label: setup
#| include: false
#| eval: true

# Elimino todo del Entorno
rm(list = ls())

# Cargo bibliotecas necesarias
library(tidyverse)
library(tidymodels)
library(kableExtra)
```

```{r}
#| label: datos
#| echo: false
#| eval: false

source("Tema07datos.R")
```

# Introducción

## Proceso de modelización

* `tidymodels` es una colección de paquetes para el **proceso de modelización** (NO implementa modelos) <!--en aprendizaje automático--> con los principios de `tidyverse`

<!--
<https://www.tidymodels.org/start/>
<https://www.tmwr.org/resampling.html#resampling-performance>
-->

```{r}
library(tidymodels)
```

![](figure/tidymodels_process.png){width=90% fig-align="center"}




<!-- * Las acciones del proceso <!--(preparación de datos, entrenamiento del modelo, validación, ...)--> <!--no se ejecutan directamente: primero se define cada paso y se ejecutan todos al final -->

<!--
* `workflows`: combinar todos los pasos anteriores en un único objeto
-->

* Otros paquetes "similares": `mlr3`, `caret`, `H2O`

* `tidymodels` se llama `vetiver` en python

# Pre-procesado

## Partición inicial

* NO usamos todos los datos para estimar

* `initial_split()`: particionar los datos en prueba y entrenamiento.

    + primer argumento: datos (se pasa por tubería `|>`)
    
    + segundo argumento: proporción de datos dedicados a entrenamiento

<!-- Ver el objeto -->

<!-- Ej. strata -->

* `tidymodels` crea objetos (listas) con toda la información 
     y ofrece funciones para acceder a ella

```{r}
#| eval: false
library(mosaicData)
library(tidyverse)
set.seed(101)
railtrailPart <- RailTrail |> 
                    mutate(dayType = parse_factor(dayType)) |> 
                    initial_split(prop = .8)
```


* Las funciones `training()` y `testing()` acceden a cada submuestra


## Recetas de transformación

* Aunque ya hemos limpiado los datos de forma general, podemos considerar **transformaciones** específicas para algunas estimaciones

    1. Algunos modelos necesitan que las variables sean transformadas de una forma concreta (ej., estandarizar)
        
    2. Incluso para un mismo modelo, podemos considerar diferentes especificaciones diferentes: sin usar `NA` o imputándolos, usando distintas variables (ej., con o sin interacciones)

. . .

* Las recetas, `recipe()`, definen las transformaciones a aplicar:

  + Su principal argumento es una *fórmula*, que define el rol de las variables: variable dependiente y predictores

  + Se encadenan (con `|>`) *pasos* con  `step_*` para las transformaciones

* Una referencia completa [aquí](https://recipes.tidymodels.org/reference/index.html) y [aquí](https://www.tidymodels.org/find/recipes/) 

<!-- ej. recipes::Roles -->

## Algunos pasos habituales

* Para un *modelo concreto*, podemos realizar cambios (como en `tidyverse`): `step_select()`, `step_filter()`, `step_mutate()`, ...

    * También transformaciones directas: `step_log()` 

<!--

  * `step_sqrt()`, `step_boxcox()`, ... 

  * `step_rm()`, `step_arrange()` 
  
-->

* ¿Cómo tratar los valores ausentes en *este modelo*?

  - elimina las observaciones con valores ausentes: `step_naomit()`

  - imputar los valores ausentes de los *predictores*: `step_impute_mean()`, `step_impute_linear()`, `step_impute_knn()`, etc..
  
  <!-- 
  
  * imputar media condicional mejor que incondicional: individuos son similares características 
  
  * Nota: imputar la variable dependiente no tiene mucho sentido  
  
   `step_impute_median()`, `step_impute_bag()`, `step_impute_lower()`,  `step_impute_mode()`, `step_impute_roll()`
   
   -->

. . .

* Estandarizar variables para que tengan media cero, `step_center()`, varianza uno, `step_scale()`, o ambas `step_normalize()`

<!--    
    +  `step_standardize()` 
-->

* Crear polinomios, `step_poly()`, y discretizar<!-- variables continuas-->, `step_cut()`

  <!-- `step_discretize()` -->

<!--
# https://www.tmwr.org/pre-proc-table
# 
# https://recipes.tidymodels.org/articles/Dummies.html
# 
-->


## Algunos pasos habituales (cont.)

* Para variables cualitativas, podemos agrupar categorías poco frecuentes: `step_other()`

* En general, `tidymodels` exige crear explícitamente variables binarias (dummy) para las categorías de las variables cualitativas: `step_dummy()`

  * algunos modelos necesitan dummies para *todas* las categorías: se añade la opcion `one_hot = TRUE`

. . . 

* NOTA: algunas fuentes (ej., chatGPT) sugieren eliminar variables con poca variabilidad, `step_zv()` y `step_nzv()`, o con alta correlación, `step_corr()`. Esto **NO** es estrictamente necesario para ningún modelo

    + Esto lo debemos haber detectado en el AED.


## Algunos pasos habituales (y 3)

* Las interacciones también funcionan de forma diferente en `tidymodels`

* Para variables cuantitativas, *x1* y *x2*, `step_interact(~ x2:x1)` añade **solo** una nueva variable: la inteacción *x2:x1*
  
    + con `step_interact(~ (x2 + x3):x1)` creamos múltiples interacciones

* Para variables binarias, `step_interact(~ d1:d2)` añade la interacción

* Para variables cualitativas, hemos debido crear *antes* las dummies. Para evitar escribir manualmente cada combinación podemos usar `start_with()`:

    + `step_interact(~ starts_with("F1"):x1)` para una variable categórica *F1* y una continua *x1*

    + `step_interact(~ starts_with("F1_"):starts_with("F2_"))` para dos variables categóricas *F1* y *F2*

## Preparando la receta

::: {style="font-size: 95%;"}

* **TODOS** los pasos se pueden aplicar a una variable o varias con `all_outcomes()`, `all_predictors()`, `all_numeric()`, `all_nominal()`, `all_nominal_predictors()`, `all_numeric_predictors()`, `all_of(c("var1", "var2")`,  `contains()`,  etc.

    <!--  matches("edad|renta") -->

<!--
* También hay pasos de control como `check_missing()`
-->


* Se crea un objeto de receta para usarlo después con otras partes del proceso

```{r}
#| eval: false
receta1 <-railtrailPart |> training() |>             
  recipe(volume ~ cloudcover + precip + hightemp + dayType) |>
  step_scale(all_predictors(), -all_nominal()) |>
# step_scale(all_numeric(), -all_outcomes()) |> 
  step_dummy(dayType) |> 
  step_poly(hightemp, degree = 6)  
```


<!--

dummies no se normalizan
la normalización debe ser ANTES de la creación de polinomios, logaritmos o interacciones

-->

```{r}
#| echo: false
#| eval: false
receta2 <- training(RailTrail_part) |>            # Datos: NO crucial
  recipe(volume ~ cloudcover + precip + avgtemp + dayType) |>
  step_poly(avgtemp, degree = tune("lambda")) |>                              # tuning
  step_corr(all_predictors(), -all_nominal(), threshold = 0.9) |>
  step_center(all_predictors(), -all_nominal()) |>
  step_scale(all_numeric(), -all_outcomes(), -all_nominal()) |> 
  step_dummy(dayType)
```

* Podría `prep()`ararse y aplicar las transformaciones con `juice()` o `bake()` (veremos otra alternativa más general)

* NOTA: los pasos no funcionan aplicados a variable dependiente cuando aplicamos la receta en nuevos datos

<!-- los datos de la receta NO son importantes: se definirán al usarla luego -->

<!--

```{r}
#| echo: false
#| eval: false
receta1_prep <- receta1 |>  prep()
re1_entrena <- receta1_prep |> juice()                             # extraer
re1_prueba  <- receta1_prep |> bake(railtrailPart |> testing())   # aplicar a unos nuevos

re1_prueba |> map(mean)   # media cero y varianza uno
re1_prueba |> map(sd)
```
-->

:::

# Entrenamiento del modelo

## Definir el modelo


<!-- cada motor puede llamar parámetros de forma distinta-->

* `tidymodels` define un modelo de un tipo con una interfaz unificada para distintas bibliotecas <!--(con otros nombres de argumentos)-->

1. Una función que define el tipo de modelo, con argumentos específicos de ese modelo (la lista de funciones/modelos [aquí](https://www.tidymodels.org/find/parsnip/))

2. Se fija el tipo de problema (*mode*): regresión o clasificación

3. Se establece la biblioteca (*engine*) con la que se implementará


```{r}
#| eval: false
modelo_lm1  <- linear_reg(mode= "regression", engine = "lm", 
                             penalty = 0)

modelo_glm1 <- linear_reg(mode= "regression", engine = "glmnet", 
                             penalty = 0)
```



```{r}
#| echo: false
#| eval: false
modelo_lm1     <- linear_reg(mode= "regression", penalty = 0) |>
                    set_engine("lm") 

modelo_glmnet1 <- linear_reg(penalty = 0)  |> set_mode("regression") |> 
                        set_engine("glmnet")

modelo_logit1 <- logistic_reg(mode= "classification", penalty = 0) |> set_engine("glm")
```


* Se puede estimar (entrenar) el modelo con `fit()`


## Flujos de trabajo: `workflow()`

* Un flujo de trabajo combina la receta de preprocesado y la definición del modelo en un único objeto para su uso posterior


```{r}
#| eval: false
flujo_lm1 <- workflow() |>
  add_recipe(receta1) |>      
  add_model(modelo_lm1)
```

```{r}
#| echo: false
#| eval: false
flujo_lm1 <- workflow() |>
  add_recipe(receta1) |>       # add_formula() si no procesamos los datos
  add_model(modelo_lm1)
```


* Podríamos modificar un flujo existente con `update_recipe()` , `update_model()`, etc.

<!--
```{r}
#| echo: false
#| eval: false
flujo_lm2 <- flujo_lm1 |> 
                update_recipe( receta1 |> 
                    step_log(volume, skip=TRUE)  )
```
-->

. . .

* Usando la función `fit()` con unos datos y un flujo de trabajo,

  - la receta aplica el preprocesado a los datos definidos aquí
  
  - se estima el modelo definido con los datos procesados

```{r}
#| eval: false
flujo_lm1_est <- flujo_lm1 |> 
                   fit(data = railtrailPart |> training()) 
```


## Flujo de trabajo estimado

* El objeto del flujo estimado almacena información previa (receta, modelo, datos), resultados (estimaciones, predicciones) para usar de varias formas

<!-- https://workflows.tidymodels.org/reference/extract-workflow.html -->

1. Extraer la receta y aplicar la transformación a unos datos 

<!-- (p.e., comprobamos que tienen varianza 1 con `var()`) -->
```{r}
#| eval: false
receta_extr <- flujo_lm1_est |> extract_recipe() 

receta_extr |> bake(railtrailPart |> training()) 
receta_extr |> bake(railtrailPart |> testing())
```

```{r}
#| echo: false
#| eval: false
flujo_lm1_est |> extract_recipe() |> bake(RailTrail_part |> training()) |> map(sd)
flujo_lm1_est |> extract_recipe() |> bake(RailTrail_part |> testing())
```

2. Extraer los resultados de la estimación; pueden convertirse a conjunto de datos (y mostrar en tablas con `kable()`)

```{r}
#| eval: false
estim_extraida <- flujo_lm1_est |> extract_fit_parsnip() 

estim_extraida |> tidy()      # resultados de la estimación
estim_extraida |> glance()    # otros detalles de la estimación
```

<!--
* `broom::augment()` calcula predicciones del modelo, residuos, etc.

```{r}
#| echo: false
#| eval: false
modelo <- flujo_lm1_est |> extract_fit_parsnip()
modelo$fit |> augment()      

augment(modelo_lm1_est$fit) |> select(volume, .fitted:.std.resid)

```
-->

## Flujo de trabajo estimado (cont.)

::: {style="font-size: 90%;"}

* El proceso es igual para problemas de clasificación. 

```{r}
#| eval: false
censo <- read_csv("data/census.csv") |> select(-1) |> 
  mutate(across(where(is.character),~as.factor(.x)))
set.seed(8697)
censoPart <- censo |> initial_split(prop = .8)

receta_log1 <- training(censoPart) |>
  recipe(income ~ age + education_1 + capital_gain)
modelo_log1 <- logistic_reg(mode= "classification", engine = "glm", 
                             penalty = 0)

flujo_log1 <- workflow() |>
  add_recipe(receta_log1) |>      
  add_model(modelo_log1)

flujo_log1_est <- flujo_log1 |> 
                   fit(data = censoPart |> training()) 
```


```{r}
#| eval: false
estim_extraida_log <- flujo_log1_est |> extract_fit_parsnip() 

estim_extraida_log |> tidy() 
estim_extraida_log |> glance()
```

:::

# Validación del modelo

## Predicción 

::: {style="font-size: 95%;"}

* La función `predict()` (de `parsnip`) tiene dos argumentos principales: un <!--objeto--> flujo de trabajo estimado y un conjunto de datos (nuevo como los datos de prueba u otros valores)
    
  + El flujo estimado almacena la receta para transformar los datos
  
  + También contiene los resultados de la estimación ("parámetros" del modelo) que se aplican a los datos transformados para predecir

```{r}
#| eval: false
flujo_lm1_est |> 
  predict(new_data = railtrailPart |> testing())
``` 


* Devuelve un conjunto de datos (*tibble*) con [predicciones](https://parsnip.tidymodels.org/reference/predict.model_fit.html):

    + para problemas de regresión: valores numéricos (`type = "numeric"`, por defecto), intervalos de confianza (`type = "conf_int"`), etc.
    + para problemas de clasificación: clases/categorías (`type = "class"`, por defecto), las probabilidades de cada categoría (`type = "prob"`)
 

<!--
* Devuelve un `tibble` (no un vector), lo que permite añadir la predicción que podemos añadir a los datos originales con `bind_cols()` 


```{r}
#| eval: false
#| echo: false
 lm1_flujo_est |> 
  predict(new_data = RailTrail_prueb) |> 
  bind_cols(RailTrail_prueb |> select(volume))     # Variable predicha .pred

logit1_flujo_est |> 
  predict(censo_part |> testing())                # clase predicha .pred_class

logit1_flujo_est |> 
  predict(censo_part |> testing(), type = "prob")    # probabilidades de cada categoría
```

* Notad que se procesan *automáticamente* los datos donde se predice

-->

:::

## Métricas de error

::: {style="font-size: 95%;"}

<!--
* La función `last_fit()` ajusta el flujo en los datos de entrenamiento y obtiene predicciones en los de prueba.

 * Calcula la predicción en la muestra de prueba de los valores (para regresión) o de las clases y la probabilidad de cada clase (para clasificación) 

* Existen [funciones para calcular](https://yardstick.tidymodels.org/articles/metric-types.html) las métricas de error usando un conjunto de datos valores/clases observados y predichos

* Pero también se pueden obtener directamente las métricas
-->

* La función `last_fit()` ajusta el flujo en los datos de entrenamiento y obtiene predicciones en los de prueba, para calcular las métricas (ver [listado](https://yardstick.tidymodels.org/articles/metric-types.html)):

```{r}
#| eval: false
flujo_lm1_finalfit <- flujo_lm1 |> 
                    last_fit(split = railtrailPart,
                             metrics = metric_set(rmse, mae)) 
flujo_lm1_finalfit |> collect_metrics()

flujo_log1_finalfit <- flujo_log1 |> 
                      last_fit(split = censoPart,
                             metrics = metric_set(accuracy, roc_auc))
flujo_log1_finalfit |> collect_metrics()
```

* También podemos trabajar con las predicciones en la muestra de prueba:

```{r}
#| eval: false
predic_log1 <- flujo_log1_finalfit |> collect_predictions() 
predic_log1 |> roc_auc(income, `.pred_<=50K`) 
predic_log1 |> roc_curve(income, `.pred_<=50K`) |> autoplot()
```



```{r}
#| eval: false
#| echo: false
predic_lm1 <- flujo_lm1_finalfit |> collect_predictions()
predic_lm1 |> mae(truth=volume, estimate= .pred) 

predic_log1 <- flujo_log1_finalfit |> collect_predictions() 
predic_log1 |> roc_auc(income, `.pred_<=50K`) 
predic_log1 |> roc_curve(income, `.pred_<=50K`) |> autoplot()
```

* NOTA: con más de dos clases, la matriz de confusión tiene dimensión $\small k \times k$ y se calcula una ROC-AUC para cada clase frente a las demás 


<!--

* Para calcular las métricas de error de un modelo, necesitamos un conjunto de datos con valores/clases observados (*truth*) y predichos (*estimate*)

* Añadimos la columna con la predicción a los datos y se los pasamos a una función que [calcule la métrica](https://yardstick.tidymodels.org/articles/metric-types.html): una en concreto o varias a la vez

```{r}
#| eval: false
datosMetricas <- flujo_lm1_est |> 
  predict(new_data = railtrailPart |> testing()) |> 
  bind_cols(railtrailPart |> testing())  
  
datosMetricas |> mae(truth=volume, estimate= .pred)  

datosMetricas |> metrics(truth=volume, estimate= .pred)    

mis_metricas <- metric_set(mae, rmse)
datosMetricas |> mis_metricas(truth=volume, estimate= .pred)    
```
-->

<!--
## Métricas de error para problemas de clasificación

::: {style="font-size: 90%;"}

* Podemos calcular métricas para clases predichas (matriz de confusión y derivadas) ...

```{r}
#| eval: false
datosMetricas_log1 <- flujo_log1_est |> 
  predict(new_data = censoPart |> testing()) |> 
  bind_cols(censoPart |> testing())
datosMetricas_log1 |>   
  conf_mat(truth=income, estimate= .pred_class)           
mis_metricas <- metric_set(accuracy, spec, sens)      
datosMetricas_log1 |>
  mis_metricas(truth=income, estimate= .pred_class)   
```
-->

<!--
*  Nota: para predecir clases se utiliza por defecto la clase con mayor probabilidad predicha (la más probable); es decir, en el caso binario un umbral de $\small 0.5$
-->

<!--
* ... o métricas basadas probabilidades predichas (ROC-AUC)

```{r}
#| eval: false
datosMetricas_log2 <- flujo_log1_est |> 
  predict(new_data = censoPart |> testing(), type = "prob") |> 
  bind_cols(censoPart |> testing())
datosMetricas_log2 |> roc_auc(income, `.pred_<=50K`) 
datosMetricas_log2 |> roc_curve(income, `.pred_<=50K`) |> autoplot()
```
-->

<!--
o la curvas de ganancias
```{r}
#| echo: false
#| eval: false
logit1_probs |>
  gain_curve(income, `.pred_<=50K`) |>
  autoplot()
```

* Se pueden combinar clases y probabilidades predichas

```{r}
#| echo: false
#| eval: false
modelo_logit1_est |> predict(logit1_prueba) |> 
  bind_cols(logit1_prueba) |>  
  bind_cols(logit1_probs |> select(1:2)) |> 
  metrics(truth=income, `.pred_<=50K`, estimate= .pred_class)
```
-->

<!--

* Con más de dos clases, la matriz de confusión tiene dimensión $\small k \times k$ y se calcula una ROC-AUC para cada clase frente a las demás 

-->

<!--

* Con más de dos clases, se predice la probabilidad de cada clase y la clase predicha es la más frecuente

  * La matriz de confusión es similar, pero de dimensiones $\small k \times k$

  * La *accuracy* sigue teniendo la misma interpretación
  
  * La ROC-AUC se calcula para cada clase frente a las demás 
-->

:::

## Validación cruzada

* En lugar de *initial_split()*, podemos usar `vfold_cv()` para crear particiones

<!--
    + ver ayuda para opciones adicionales (ej. *estratos*)
-->

```{r}
#| eval: false
set.seed(101)
railtrailPartCV <- RailTrail |> 
                    mutate(dayType = parse_factor(dayType)) |> 
                    vfold_cv(v=10) 
```

* Podemos acceder a la información de cada bloque (*splits*)

<!--

* Podemos extraer cada uno de los bloques (*splits*) y recuperar sus datos de entrenamiento (`analysis()`) y  prueba (`assessment()`):

```{r}
#| eval: false
#| echo: false
railtrailPartCV$splits[[1]] |> analysis()
railtrailPartCV$splits[[1]] |> assessment()
```
-->

```{r}
#| eval: false
#| echo: false
railtrailPartCV$splits[[1]] |> analysis()
```

* El flujo de trabajo puede ser el mismo que ya hemos visto <!--(aunque la receta use que no sean de estas particiones)-->

* La estimación del modelo se realiza usando `fit_resamples()` <!--sobre el flujo de trabajo y la partición de validación cruzada-->

<!-- no solo a datos de entrenamiento, porque aquí varían -->

```{r}
#| eval: false
#| echo: false
lm1_flujo_cv_est <- lm1_flujo  |> 
                        fit_resamples(RailTrail_cv)
```

<!-- 
* Se pueden cambiar varias opciones del proceso 

como las métricas calculadas y otros elementos de control 
-->

```{r}
#| eval: false
flujo_lm1_CVest <- flujo_lm1  |> fit_resamples(
                      resamples = railtrailPartCV, 
                      metrics   = metric_set(rmse, mae) )
```

```{r}
#| eval: false
#| echo: false
flujo_lm1_CVest <- flujo_lm1  |> 
                    fit_resamples(
                      resamples = railtrailPartCV, 
                      metrics   = metric_set(rmse, mae),
                      control   = control_resamples(save_pred = T)
                      )
```


* ... y el objeto creado contiene los valores de las métricas 

```{r}
#| eval: false
flujo_lm1_CVest |> collect_metrics()      # promedio 
```

```{r}
#| eval: false
#| echo: false
flujo_lm1_CVest |> collect_metrics()      # promedio sobre 10 iteraciones
flujo_lm1_CVest$.metrics |> bind_rows()   # valores en cada iteración
```


# Modelos con hiperparámetros

## Selección de hiperparámetros: *tuning*

<!--
* Algunos parámetros, denominados **hiperparámetros**, no se aprenden (estima) junto con el resto de parámetros del modelo (ej., $\small \lambda$ en LASSO)
-->

* Los hiperparámetros (ej., $\small \lambda$ en LASSO) no se aprenden junto con el resto de parámetos sino mediante un proceso de **ajuste** (*tuning*), dividiendo la muestra de **entrenamiento** en dos

<!--
* Estimamos el hiperparámetro con los datos de **entrenamiento**, pero la subdividimos en dos
-->

<!-- 

NOTA IMPORTANTE: el ajuste es ENTRENAMIENTO

Si eligieramos usando la muestra de prueba, habríamos "aprendido" el hiperparámetro usandola ya, y no sería válido para generalizar por overfitting

-->

:::: {.columns}

::: {.column width="50%"}
<!--
* Proceso de **ajuste** (*tuning*): 


  - en una parte de la muestra de entrenamiento, se estiman los parámetros dado un valor del hiper-parámetro 

  - en la otra parte, se mide el error asociado a ese hiper-parámetro para validar el mejor valor
  
  - se elige el valor con mejor métrica de error en validación
  
-->

  1. En una se ajusta el modelo dado un valor del hiperparámetro
  
  2. En la otra se mide el error asociado a cada valor para elegir el hiperparámetro con mejor métrica

:::

::: {.column width="50%"}

![](figure/train_test.png){width=90% fig-align="center"}

$\Downarrow$

![](figure/train_validate.png){width=90% fig-align="center"}


<!--
$\hspace{0.1cm}$

![](figure/train_cross_validate.png){width=90% fig-align="center"}

-->
::: 
::::


<!--

![](figure/resampling.svg){width=90% fig-align="center"}

-->

* En la muestra de entrenamiento usaremos validación cruzada:

![](figure/train_cross_validate.png){width=45% fig-align="center"}


## Proceso de *tuning*

* Usamos una partición de *initial_split()* y la misma receta anterior (aunque no era necesario antes, el pre-procesado ya estandarizaba los regresores)

* En la definición del modelo debemos identificar los hiperparámetros a ajustar

```{r}
#| eval: false
modelo_LASSO <- linear_reg(mode= "regression", engine = "glmnet",
                           penalty = tune(), mixture = 1 ) 

flujo_LASSO <- workflow() |>
  add_recipe(receta1) |> 
  add_model(modelo_LASSO)

flujo_LASSO |> extract_parameter_set_dials()
```

* Definimos un conjunto de valores a probar (*grid*); p.e., con `grid_regular()` buscamos en un intervalo un número de valores ([otras opciones aquí](https://dials.tidymodels.org/reference/grid_regular.html))
```{r}
#| eval: false
LASSO_grid <- grid_regular(penalty(range = c(0, 15), trans = NULL), 
                                   levels = 51)
```

<!--

Tres estrategias 

* `grid_regular()` : combinaciones están igualmente espaciadas para cada hiperparámetro 

* `grid_random()` :  los valores son aleatorios dentro de unos límites preestablecidos

* `grid_max_entropy()`: los valores son aleatorios pero intentan cubrir todo lo posible el espacio de búsqueda

-->

## Proceso de *tuning* (cont.)

* Se usa `tune_grid()` de forma a similar a *fit_resamples()*, usando una sub-partición de la *muestra de entrenamiento* por validación cruzada

<!-- alternativa: tune_bayesian() -->

<!--
* Se obtienen remuestras por Validación Cruzada de la muestra de *entrenamiento* 
-->

```{r}
#| eval: false
set.seed(9753)
railtrail_entrenCV <- railtrailPart |> training() |> 
                          vfold_cv(v=10)
flujo_LASSO_ajust <- flujo_LASSO |> 
                        tune_grid(resamples = railtrail_entrenCV, 
                                  metrics   = metric_set(rmse, mae),
                                  grid      = LASSO_grid            )
```

<!--
* La validación cruzada es computacionalmente intensiva, especialmente con varios hiperparámetros

  + se suelen realizar mediante computación en paralelo

```{r}
#| echo: false
#| eval: false
receta_lm2 <- training(RailTrail_part) |> recipe(volume ~ avgtemp) |>
  step_poly(avgtemp, degree = tune("lambda")) |> 
  step_center(all_predictors()) |> step_scale(all_predictors()) 

flujo_LASSO_tuning2 <- workflow() |> add_recipe(receta_lm2) |> add_model(modelo_LASSO)
flujo_LASSO_tuning2 |> parameters() 

LASSO_grid2 <- grid_regular(penalty(range = c(0,15), trans = NULL), 
                            degree_int(range=c(1,8)), levels = 51 )

```

-->

* Podemos visualizar el ajuste y obtener los mejores valores (por la variabilidad, varios son igualmente aceptables)

<!-- select_by_one_std_err()  -->

```{r}
#| eval: false
flujo_LASSO_ajust |> autoplot()
flujo_LASSO_ajust |> show_best(metric = "mae")
mejor_lambda <- flujo_LASSO_ajust |> select_best(metric = "rmse")
```


```{r}
#| eval: false
#| echo: false
flujo_LASSO_ajust |> autoplot()

penalty <- flujo_LASSO_ajust |> collect_metrics() |> filter(.metric == "mae")
penalty |> ggplot(aes(x=penalty, y=mean)) +  scale_x_log10() +
              geom_line() + geom_point(color="red") + 
              geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), color="gray")
```


* NOTA: debemos probar manualmente varios rangos y valores buscando la <!--típica forma de--> U


<!-- ## Estimación del modelo completo -->

## Finalizando y evaluando el modelo

::: {style="font-size: 95%;"}

* Actualizamos el flujo de trabajo para dar un valor a los hiperparámetros

```{r}
#| eval: false
flujo_LASSO_final <- flujo_LASSO |> 
        finalize_workflow(mejor_lambda)  
        # finalize_workflow(parameters = list(penalty=8.5))
```

* Debemos estimar el modelo en los datos *completos* de entrenamiento, para  mostrar resultados de estimación o predecir

```{r}
#| eval: false
flujo_LASSO_final_est <-  flujo_LASSO_final |>  
              fit(data = railtrailPart |> training())
flujo_LASSO_final_est |>  extract_fit_parsnip() |> tidy()
```


* Para obtener las métricas, usamos `last_fit()`<!--: ajusta al modelo finalizado en los datos de entrenamiento y lo evalúa en los de prueba.-->

```{r}
#| eval: false
LASSO_final_fit <- flujo_LASSO_final |> 
                    last_fit(split = railtrailPart,
                             metrics = metric_set(rmse, mae)) 
LASSO_final_fit |> collect_metrics()
```


```{r}
#| eval: false
#| echo: false
LASSO_final_fit <- flujo_final |> last_fit(RailTrail_part)

LASSO_final_fit |> collect_metrics()
LASSO_final_fit |> collect_predictions()


LASSO_final_fit <- flujo_final |> last_fit(split   = RailTrail_part,
                                      metrics = metric_set(rmse, mae))
LASSO_final_fit |> collect_metrics()
```

:::
