---
title: "Predicción de precios de las casas en Boston"
author:
    - "Pedro Albarrán"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
    theme: cosmo
    css: styles.css
execute:
  enabled: true
  eval: true
  echo: true
  warning: false
  message: false
  output: false
  fig.show: hide
lang: es
strip-comments: true
params:
  soln: false
---


```{r}
#| label: setup
#| include: false
#| eval: true
# se evalua pero no incluye output (mensajes, etc.)

# Elimino todo del Entorno (del documento)
rm(list = ls())

# Working directory
#setwd("/home/albarran/Dropbox/MAD/00.TEC")

# Cargo todas las bibliotecas necesarias
# (se podría hacer cuando cada una sea necesaria)
library(tidyverse)
library(tidymodels)
library(rio)
library(printr)
library(skimr)
library(dlookr)
library(broom)
library(kableExtra)
library(rpart.plot)
library(vip)
```


# Introducción

## Comentario General

Este documento debe entenderse como **un ejemplo**, no *la* guía única para el proyecto. 

1. Aquí muestro algunas cosas que en vuestros trabajos probablemente NO queréis mostrar. Por ejemplo, he incluido la opción de que se muestre o se oculte el código de todo lo que he hecho, pero vosotros debéis pensar qué y cuándo queréis mostrar algo. También muestro algunos resultados que, como se ha discutido en clase, probablemente tampoco queráis mostrar (o no de la misma manera).
    
2. He omitido muchos detalles de algunas fases del trabajo que ya se han ido comentando y trabajando en clase (características de los datos, procedimientos, tablas con encabezados adecuados, gráficos con ejes correctamente nombrados, comentario de resultados, etc.)  

3. Cada conjunto de datos es diferente y cada análisis es diferente. Se requiere un tratamiento distinto de los datos: (eliminar o imputar valores ausentes, transformar variables, agrupar categorías, discretizar, etc.), diferentes algoritmos y especificaciones (combinaciones de variables a incluir y sus transformaciones).<!--[^1]--> De hecho, **se espera** que probéis distintas opciones y discutáis los resultados.

<!--
[^1]: Por eso existen competiciones ("hackatones") para ver quién predice mejor con los mismos datos: si existiera una receta, sería trivial. <!-- ganaría un ordenador.-->

En resumen, este ejemplo incluye pruebas, gráficos y tablas que vosotros no incluiréis. También deberéis probar otras que aquí no he probado. Pero también se espera discusiones algo más detalladas (sin ser excesivas).

## Introducción y Objetivos

En este trabajo, se analizará un conjunto de datos con información sobre precios y otros atributos de una muestra de viviendas en Boston. Por un lado, el objetivo es examinar la influencia de varios atributos del vecindario en los precios de la vivienda, en un intento por descubrir las variables explicativas más adecuadas. Por otro lado, la construcción de un modelo de predicción permitirá determinar el valor por el que se puede poner en el mercado una vivienda o detectar si alguna está infravalorada o sobrevalorada dadas sus características. 

El objetivo del trabajo es un elemento importante que debéis desarrollar adecuadamente. Todo el análisis y comentarios posteriores deben estar orientados a responder a las preguntas planteados por los objetivos.

<!--
Para realizar este análisis utilizaremos lo que hemos aprendido del lenguaje de programación R. En particular usaremos las herramientas de las bibliotecas `tidyverse` y `tidymodels`, además de otras puntualmente.
-->

# Datos

En este trabajo vamos a utilizar un conjunto de datos, "The Boston Housing Price", derivados de la información recopilada por el Servicio de Censos de los Estados Unidos sobre las viviendas en el área de Boston (Massachusetts). Podemos encontrar algunos detalles adicionales sobre estos datos [aquí](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). Los datos pueden obtenerse desde esa misma página, pero por sencillez los leemos en formato de valores separados por comas (CSV) desde [aquí](https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv).

```{r datos0}
#| echo: false
#| eval: false
Boston <- read_tsv("http://lib.stat.cmu.edu/datasets/boston", 
                   skip = 22,
                   col_names = c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv"))
```

```{r datos}
#| eval: true
#| echo: false
Boston <- tryCatch(
  read_csv("data/BostonHousing.csv"),
  error = function(e) read_csv("https://raw.githubusercontent.com/albarran/00datos/main/BostonHousing.csv")
)
```

Los datos tienen `r dim(Boston)[1]` observaciones y `r dim(Boston)[2]` variables. (NOTA: <!--esta información se ha incluido usando el código de R en línea "r dim(Boston)[1]" y "r dim(Boston)[2]") y también se puede obtener con el siguiente código; --> NO está claro que queráis mostrar este código.)

```{r dim-datos}
#| results: asis
dim(Boston)
```

Una descripción completa los atributos disponibles puede encontrarse en [Apéndice A]. Estos datos fueron originalmente utilizados en un estudio sobre el impacto de la contaminación del aire (utilizando las concentraciones de óxido de nitrógeno). En este trabajo, consideramos el efecto de otras características de la zona donde se encuentra la casa como la proximidad al río Charles, la distancia a los principales centros de empleo, la calidad de las escuelas (medida por la ratio de número de alumnos por maestro) y los niveles de delincuencia. Nuestra variable de interés para predecir es el valor mediano del precio de la vivienda en mil dólares (denotado por MEDV). Notad que nos centramos en las viviendas ocupadas por sus propietarios, es decir, consideramos que el valor de las casas destinadas al alquiler se determina según un proceso diferente.

# Exploración Inicial

Debemos considerar si los datos están listos para trabajar o requieren algún tipo de limpieza, ordenación o transformación. En primer lugar, comprobamos si el tipo de datos de cada variable es el adecuado. En este caso, todas las variables son numéricas, lo cual se corresponde con la información cuantitativa de la mayoría de ellas. La variable que nos dice si la zona de la casa está cerca del río es una variable binaria, es decir, aporta información cualitativa. En este caso, no es crucial convertirla en un factor (en los modelos y para otras cuestiones, convertimos los factores en variables binarias). En otros casos, podemos necesitar convertir más variables con información cualitativa a factores o eliminar variables de tipo carácter o con información que no podamos procesar. NOTA: para esto habremos comprobado el tipo de cada variable, habremos mirado los valores y contrastado con la información que según su descripción debería tener. Utilizaríamos un código como el siguiente, aunque probablemente NO queráis incluir en el documento ni lo uno ni lo otro.

```{r explo-datos}
#| results: markup
glimpse(Boston)

head(Boston) |> kbl() |> kable_paper("hover")
```


```{r factor-chas}
Boston <- Boston |> mutate(chas = factor(chas, 
                         levels=c(0,1), labels = c("No", "Yes")))
```

(NOTA: los nombres de las columnas y de las filas/variables son "mejorables": si el documento está en castellano las columnas no deberían tener nombres en inglés y sería preferible que apareciera un nombre más descriptivo de las variables. Pero no voy a exigir esto en el plazo que tenemos. Eso sí, al menos que en algún sitio aparezca la descripción completa del nombre abreviado de las variables, como hago aquí con el Apéndice A.)

# Análisis Exploratorio y Visualización de los datos

La siguiente fase consiste en analizar la distribución de valores de cada variable (análisis de variación) y las posibles relaciones entre ellas (análisis de covariación). Esto nos puede llevar a realizar limpieza adicional de los datos (en particular, relacionada con valores ausentes y puede que atípicos) o transformaciones de los datos (como tomar logaritmos o discretizar alguna variable). También podemos encontrar características de los datos que sean de interés por sí mismas como para especificar los modelos. Para este análisis nos podemos ayudar en bibliotecas que realizan algunas de las tareas de forma automatizada. PERO recordad que NO queremos en general mostrar la salida directa de estos paquetes, sino que la utilizaremos para aprender nosotros y luego mostrar aquello que consideremos más interesante.

Por ejemplo, usando la biblioteca `skimr` podemos ver

```{r skimr}
#| results: markup
skim(Boston)
```

o usando `dlookr`

```{r describe1}
#| results: markup
Boston |> 
  describe() |>
  select(described_variables:kurtosis) |> 
  kbl(digits = 2) |> kable_paper("hover")
```

```{r describe2}
#| results: markup
Boston |> 
  describe() |>
  select(described_variables, p00:p100) |> 
  kbl(digits = 2) |> kable_paper("hover")
```

En este caso, el conjunto de datos está ya bastante limpio, por lo que apenas necesitamos realizar cambios y podemos dedicar más tiempo al resto del proceso. En otros casos, deberemos realizar más trabajo en esta parte; en particular, relacionado con valores ausentes, errores en los datos, etc. como hemos visto a lo largo del curso y, en particular, en el tema de análisis exploratorio de datos.

## Análisis de variación

Como primer elemento a destacar, estos datos no contiene valores ausentes en ninguna de las variables. En caso contrario, deberíamos identificar cuántas observaciones y qué variables están afectadas. Sabemos que podemos posponer la imputación de valores a una fase posterior (como un paso del pre-procesado antes de estimar un modelo), pero es conveniente tener una visión general y pensar si algunas observaciones probablemente serán descartadas (si tienen muchos valores ausentes y sobre todo afectan a la variable dependiente).

Podemos centrarnos en describir con más detalles algunas distribuciones. Esto nuevamente es un EJEMPLO dependiendo de las variables que tengamos y de qué observemos. En general, caracterizar la variable dependiente suele ser una buena idea. Visualizamos la distribución y densidad del precio mediano de las viviendas. La curva negra representa la densidad. Vemos que el valor medio del precio de la vivienda está sesgado a la derecha. Es decir, observamos precios muy altos con una frecuencia mayor de la esperada en una distribución simétrica donde existiría la misma proporción por encima y debajo de la media.

```{r graf-precio, fig.show='asis', fig.cap="Figura 1. Distribución del precio de la vivienda"}
Boston |>  ggplot(aes(x=medv)) + geom_histogram(aes(y=..density..))+ geom_density() + ggtitle("Distribución del Precio") + xlab("Precio de las casas") + ylab("Densidad")
```

<!-- 
cambiar geom_histogram(..density..)
--> 

Dada esta asimetría, quizás debamos considerar modelizar posteriormente esta variable transformada en logaritmos. La razón: se aprecia un comportamiento que puede modelizarse mejor de forma no lineal. También se puede nota una acumulación de valores en 50 mil dólares. Se puede observar en los resultados de `describe()` que ese valor exacto se repite varias veces, NO es producto de la discretización del gráfico en la que se acumulan varios valores diferentes en un intervalo en torno a 50; también debemos probar distintos anchos de intervalo como se ha discutido en clase.

También podemos representar gráficamente o en un tabla la única variable categórica que tenemos. La conclusión no es particularmente interesante: solo unas pocas zonas de la ciudad están cerca del río.

```{r tabla-rio}
#| results: markup
Boston |>  count(chas) |> 
  mutate(freq=n/sum(n)) |> 
  kbl(col.names=c("Casa cercana al río","Número de casos", "Frecuencia"),
                  caption = "Tabla 1. Distribución de Casas según cercanía al río") |> kable_paper("hover")
```


En el caso de variables binarias las podemos representar de varias maneras: como una distribución o con una sola barra (NOTA: los gráficos siguientes son redundantes en esta caso, con uno de ellos sería más que suficiente en caso de considerar relevante esta información.)
```{r graf-rio, fig.show='hold', fig.cap="Figura 2. Distribución de la cercanía al río", out.width="50%"}
Boston |>  ggplot() + geom_bar(aes(x=chas)) +  xlab("Zona cercana al río") +ylab("Número de casos")

Boston |>  ggplot() + geom_bar(aes(x="",fill=chas)) + labs(fill="Zona cercana al río") +ylab("Número de casos")
```

También podemos mostrar algunas otras características interesantes mediante gráficos y/o tablas de estadísticos descriptivos. Algunas variables como el número de habitaciones tienen distribuciones bastante simétricas. Mientras que otras, como la edad o el porcentaje de población desfavorecida muestran claras asimetrías: hay una alta concentración de casas "viejas" y de zonas no desfavorecidas. NOTA: Recordad que habría que probar varios anchos de intervalos (*binwidth*) para asegurarnos de entender la forma de la distribución. También debéis poner nombres suficientemente descriptivos e informativos a los gráficos, los ejes, la leyenda, etc. (Quizás no es el caso en algunos de los que presento aquí).

```{r graf-varias, fig.show='hold', fig.cap="Figura 3. Distribuciones", out.width="50%"}
Boston |>  ggplot(aes(x=age)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Edad") + ylab("Densidad")

Boston |>  ggplot(aes(x=lstat)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Porcentaje de población desfavorecida") + ylab("Densidad")
```

Notad que en la variable edad nuevamente hay una concentración de valores en 100; en la salida `describe()` mostrada anteriormente, se aprecia mejor que ese valor exacto está en los datos originales, no resulta de que se agrupen valores en el gráfico.

El caso de la distancia a los centros de empleo es similar a las dos anteriores: una gran concentración en zonas bien conectadas, aunque una cola de zonas alejadas. Se podría omitir: no hay que mostrar gráficos o tablas de cada variable ni comentar necesariamente las características de la distribución de todas, solo de aquellas con rasgos interesante o relevantes.

Otra variable en principio relacionada, el índice de accesibilidad, muestra una distribución "poco continua": además de un cúmulo de valores en la cola derecha, hay muchos huecos vacíos. Si probáis un transformación logarítmica, veréis que no cambia en esencia. Las variables con este forma en su distribución suelen ser candidatas a ser discretizadas.

```{r graf-dists, fig.show='hold', fig.cap="Figura 4. Distribuciones", out.width="50%"}
Boston |>  ggplot(aes(x=dis)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Distancia a centro de trabajo") + ylab("Densidad")

Boston |>  ggplot(aes(x=rad)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Índice de accesibilidad") + ylab("Densidad")
```

Algunas variables tienen distribuciones con características poco reseñables: unas con valores distribuidos de forma relativamente homogénea, otras dispersas, con concentraciones en valores aislados en medio o en los extremos de la distribución, pero no aportan mucho información (se podrían omitir). En este caso, quizás se podría notar una concentración de zonas con altos impuestos, muy diferenciadas del resto.

```{r graf-varias2, fig.show='hold', fig.cap="Figura 5. Distribuciones", out.width="33.33%"}
Boston |>  ggplot(aes(x=nox)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Concentración de óxidos nítricos") + ylab("Densidad")

Boston |>  ggplot(aes(x=ptratio)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Ratio de alumnos por profesor") + ylab("Densidad")

Boston |>  ggplot(aes(x=tax)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Impuesto de la propiedad") + ylab("Densidad")

```

Algo más interesantes son algunas variables que muestran polaridad en sus valores o una excesiva acumulación en algunos. Por ejemplo, la criminalidad y el porcentaje de población de color tienen distribuciones muy asimétricas y, en el segundo caso, persiste incluso tras transformar en logaritmos.

```{r graf-varias3, fig.show='hold', fig.cap="Figura 6a. Distribuciones", out.width="50%"}
Boston |>  ggplot(aes(x=crim)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Criminalidad") + ylab("Densidad")
Boston |>  ggplot(aes(x=crim)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Criminalidad") + ylab("Densidad") + scale_x_log10()

```

```{r graf-varias4, fig.show='hold', fig.cap="Figura 6b. Distribuciones", out.width="50%"}
Boston |>  ggplot(aes(x=b)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Población de color") + ylab("Densidad") 
Boston |>  ggplot(aes(x=b)) + geom_histogram(aes(y=..density..))+ geom_density() + xlab("Población de color") + ylab("Densidad") + scale_x_log10()

```

Estas variables y algunas otras anteriores son candidatas a ser discretizadas. La criminalidad, por ejemplo, no solo muestra una concentración en unos pocos valores, sino que una vez transformada en logaritmos se aprecian dos grupos diferenciados, como también pasaba con los impuestos. En el caso de la ratio de profesor/alumno también unos valores con gran concentración de frecuencia y muy pocos por encima de este por lo que podrían agruparse juntos. En el caso de la población de color, vemos que a partir del percentil 75, los valores son prácticamente iguales y antes del percentil 10 son mucho menores que en el resto de la distribución. En estos casos de variables con valores concentrados o infrecuentes y con saltos o huecos, no podemos decir que la variable que observamos en nuestra muestra tenga una apariencia de variable continua, aunque en principio lo sea. Por tanto, agrupar y discretizar es una buena opción. En particular, es más fácil identificar el efecto medio sobre el precio de la vivienda de un rango de valores (ej., zonas de baja criminalidad frente a alta) que el efecto de incrementar en un punto la variable (cuando en los datos no observamos valores con ese punto más). En otras palabras, vamos a modelizar efectos flexibles no lineales.

### MUY IMPORTANTE: 

NO mostréis todos los gráficos o tablas que se os ocurran o solo porque los veáis aquí. ELEGID adecuadamente aquellos que consideréis más relevantes o interesantes y aporten información útil para responder a los objetivos planteados.

## Análisis de covariación

Empezamos analizando la relación entre nuestra variable de interés y la única variable categórica que tenemos inicialmente, para lo que podríamos presentar alguna (NO todas) de las siguientes figuras

```{r graf-precio-rio, fig.show='hold', fig.cap="Figura 7a. Distribución del precio por cercanía al rio", out.width="50%"}
Boston |> ggplot(aes(y = medv, x = chas)) +  geom_boxplot() + ylab("Densidad") + xlab("Cerca del río") + ylab("Precio") 
Boston |> ggplot(aes(x = medv)) + geom_density(mapping = aes(colour = chas)) + xlab("Precio") + ylab("Densidad")  + labs(color = "Cerca del río")
```

```{r graf-precio-riob, fig.show='hold', fig.cap="Figura 7b. Distribución del precio, según cercanía al rio"}
Boston |> ggplot(aes(x = medv)) + geom_density() + facet_wrap(~chas) + ylab("Densidad")  + xlab("Precio")
```

Parece que las casa cercanas al río tienen un precio superior, aunque la diferencia no parece grande. Ambas distribuciones son asimétricas, aunque en el caso de casas cercanas al río la cola derecha no es tan larga. Mediante una regresión simple o calculando las medias podemos comprobar si existen diferencias en media y si son significativas:

```{r precio-rio1}
#| results: asis
lm(data = Boston, medv ~ chas) |> broom::tidy() |>  kbl(digits = 2, caption = "Tabla 2a. Precio según cercanía al río") |> kable_paper("hover")
```


```{r precio-rio2}
#| results: asis
Boston |> group_by(chas) |> describe(medv) |> select(described_variables:n, mean, se_mean) |>  kbl(digits = 2, caption = "Tabla 2b. Precio según cercanía al río") |> kable_paper("hover")
```

También podríamos analizar si la variable binaria de cercanía al río está relacionada. (NOTA: esto nuevamente es un código de ejemplo para hacer de forma fácil este proceso. NO es necesario que vosotros lo hagáis así.)

```{r regresores-rio-con-map, echo=FALSE, eval=FALSE}
#| results: asis
dif.rio <- function(var) {
  formula <- paste0(var," ~ chas")
  lm(data = Boston, formula) |> tidy() |> filter(term=="chasYes") |> select(estimate, p.value)
}

vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

tabla <- vars |> map_df(dif.rio)
bind_cols(Variable=vars, tabla) |>  kbl(digits = 2, caption = "Tabla 2c. Diferencias en variable por cercanía al río") |> kable_paper("hover")
```

```{r regresores-rio-con-for}
#| results: asis
vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

tabla <-list()
for (var in vars) {
  formula <- paste0(var," ~ chas")
  tabla[[var]] <- lm(data = Boston, formula) |> tidy() |> filter(term=="chasYes") |> select(estimate, p.value)
}

tabla |> bind_rows(.id="Variable")  |>  kbl(digits = 2, caption = "Tabla 2c. Diferencias en variable por cercanía al río") |> kable_paper("hover")
```

Vemos que las casas cercanas al rio son más antiguas, con más habitaciones, más cercanas al centro de trabajo, con más contaminación y mejores condiciones escolares.

A continuación podemos analizar rápidamente si las variables continuas están relacionadas con nuestra variable de interés, precio de la vivienda, y entre ellas. Lo podemos hacer mediante distintos análisis de correlación, en una tabla (excesivamente larga) o visualmente.

```{r tabla-correl}
Boston |> correlate() |> 
  filter(as.integer(var1) > as.integer(var2)) |> 
  kbl(digits = 2, caption = "Tabla 3. Correlaciones") |> kable_paper("hover")
```

```{r graf-correl, fig.show='hold', out.width="50%", fig.cap="Figura 8. Correlaciones entre variables continuas"}
Boston |> correlate() |> plot() 

Boston |> mutate(logmedv=log(medv)) |> select(-medv) |> correlate() |> plot()
```

Hemos considerado la correlación tanto con el precio como con su logaritmo, dado lo discutido anteriormente. Sin embargo, apenas se aprecian diferencias.

Vemos que existe una fuerte correlación (positiva o negativa) entre el precio y varias variables que intuitivamente consideraríamos como importantes. El número de habitaciones tiene la correlación positiva más fuerte con el valor medio del precio de la vivienda, mientras que el porcentaje de la población desfavorecida y el número de alumnos por docente tienen una correlación negativa fuerte. También es evidente que las zonas más industriales y la contaminación están fuertemente correlacionados positivamente entre sí, puesto que los niveles de óxido nítrico tienden a aumentar con el aumento de las industrias. También vemos que las zonas con más población desfavorecida son las más industriales y contaminadas, con casas más antiguas y de menos habitaciones y con escuelas con un mayor ratio de alumnos por profesor. Debemos recordar esto de cara a la especificación de los modelos de regresión lineal.

Sin embargo, esto no considera posibles relaciones no lineales. Para ello vamos a representar varios gráficos de dispersión y un ajuste no lineal. Nuevamente, en vuestro trabajo no mostraréis necesariamente todos estos gráficos sino una selección después de haberlos vistos.


```{r graf-disper-con-map, echo=FALSE, eval=FALSE, fig.cap="Figura 9a. Gráficos de dispersión", fig.show='hold', out.width="33.33%"}
vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

migraf <- function(v) {
  Boston |> 
      ggplot(aes_string(x = v, y = "medv")) +
      geom_point() +  geom_smooth() +
      labs(x = v, y = "Precio de las casas ($1000s)") 
}

vars |> map(migraf)
```


```{r graf-disper-con-for, fig.cap="Figura 9a. Gráficos de dispersión", fig.show='hold', out.width="33.33%"}
vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

for (v in vars){
  graf <- Boston |> 
            ggplot(aes_string(x = v, y = "medv")) +
            geom_point() +  geom_smooth() +
            labs(x = v, y = "Precio de las casas ($1000s)")
  print(graf)
}
```


```{r graf-disper-log-con-map, echo=FALSE, eval=FALSE, fig.cap="Figura 9b. Gráficos de dispersión (en escala logaritmica)", fig.show='hold', out.width="33.33%"}
vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

migraf <- function(v) {
  Boston |> 
      ggplot(aes_string(x = v, y = "medv")) +
      geom_point() +  geom_smooth() +
      labs(x = v, y = "Precio de las casas ($1000s)")  +
      scale_y_log10()
}

vars |> map(migraf)
```


```{r graf-disper-log-con-for, fig.cap="Figura 9b. Gráficos de dispersión (en escala logaritmica)", fig.show='hold', out.width="33.33%"}
vars <- c("crim", "zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")

for (v in vars) {
  migraf <-  Boston |> 
              ggplot(aes_string(x = v, y = "medv")) +
              geom_point() +  geom_smooth() +
              labs(x = v, y = "Precio de las casas ($1000s)")  +
              scale_y_log10() 
  print(migraf)
}
```


<!--
```{r, echo=FALSE, eval=FALSE}
Boston |>
  select(-chas) |>
  pivot_longer(cols = -medv, names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = medv, colour = variable)) +
  geom_point() +
  geom_smooth(colour = "black") +
  facet_wrap(~variable, scales = "free", ncol = 2) +
  labs(x = "Variable", y = "Precio de las casas ($1000s)") +
  theme_minimal()
```
-->

En primer lugar, no se aprecian grandes diferencias entre el modelo con el precio sin transformar o en logaritmos. En segundo lugar, sí se aprecia cierta no linealidad en la relación con las variables de edad, número de habitaciones y porcentaje de población desfavorecida. En el resto de relaciones, no están tan claras por la  acumulación de valores. 

También se podrían haber probado si este análisis de covariación es distinto según distintos valores de una varible categórica. Por ejemplo, visualizar la relación entre precio y tasa de pobreza cuando la zona está cerca del rio y cuando no está cerca.

Podemos probar discretizando algunas de las variables comentadas anteriormente. Por ejemplo, hacemos dos grupos de criminalidad; los umbrales para discretizar no tienen una justificación muy formal: se basan en lo que aproximadamente hemos visto. 

```{r graf-crim, fig.cap="Figura 10. Gráficos Criminalidad Discreta", fig.show='hold', out.width="50%"}
  Boston |> 
      mutate(crim.alta = cut(crim, breaks = c(0,1,Inf), 
                             include.lowest = TRUE,
                             labels = c("Baja","Alta") ) ) |> 
      ggplot(aes(y = medv, x = crim.alta)) +  geom_boxplot() + ylab("Densidad") + xlab("Criminalidad alta") + ylab("Precio") 
```


```{r precio-crimalta1}
#| results: asis
#| echo: false
#| eval: false

Boston |> 
  mutate(crim.alta = cut(crim, breaks = c(0,1,Inf), 
                         include.lowest = TRUE,
                         labels = c("Baja","Alta"))) |> 
  {\(data) lm(medv ~ crim.alta, data = data)}() |> 
  tidy() |>  
  kbl(digits = 2, caption = "Tabla 4. Precio según criminalidad") |> 
  kable_paper("hover")
```

```{r precio-crimalta}
#| results: asis
#| echo: true
#| eval: true

Boston_temp <- 
  Boston |> 
    mutate(crim.alta = cut(crim, breaks = c(0,1,Inf), 
                         include.lowest = TRUE,
                         labels = c("Baja","Alta"))) 

lm(data = Boston_temp, medv ~ crim.alta) |> 
  tidy() |>  
  kbl(digits = 2, caption = "Tabla 4. Precio según criminalidad") |> 
  kable_paper("hover")

rm(Boston_temp)
```

Nota: existe una forma alternativa más limpia (para no tener que borrar un objeto temporal). El símbolo `_` se refiere al "data frame" que viene de la tubería. En general, NO es necesario (sobre todo con funciones de `tidyverse`), pero con `lm()` sí.

```{r precio-crimalta2}
#| results: asis
#| echo: false
#| eval: false

Boston |> 
  mutate(crim.alta = cut(crim, breaks = c(0,1,Inf), 
                         include.lowest = TRUE,
                         labels = c("Baja","Alta"))) |> 
  lm(data = _, medv ~ crim.alta) |> 
  tidy() |>  
  kbl(digits = 2, caption = "Tabla 4. Precio según criminalidad") |> 
  kable_paper("hover")
```

Tanto el gráfico como la regresión apuntan a un efecto significativo de la criminalidad sobre los precios. En principio deberíamos probar con otros puntos de corte para discretizar, pero por simplicidad utilizaremos este obtenido a partir del análisis exploratorio.

Podemos proceder de manera similar con otras variables. Nuevamente, tanto el número de grupos como los valores de corte no se derivan de forma super rigurosa, sino en base al análisis exploratorio. Debería probarse con otras variantes.

```{r precio-otras.discretizadas}
#| results: asis
Boston |> 
  mutate(dis.alta = cut(dis, breaks = c(0,3,Inf), 
                        include.lowest = TRUE,
                        labels = c("Baja","Alta") ) ) |> 
  lm(data = _, medv ~ dis.alta) |> tidy() |>
  kbl(digits = 2, caption = "Tabla 5. Precio según distancia") |> kable_paper("hover")

Boston |> 
  mutate(rad.alta = cut(dis, breaks = c(0,10,Inf), 
                        include.lowest = TRUE,
                        labels = c("Baja","Alta") ) ) |> 
  lm(data = _, medv ~ rad.alta) |> tidy() |>  
  kbl(digits = 2, caption = "Tabla 6. Precio según accesibilidad") |> kable_paper("hover")

Boston |> 
  mutate(tax.alta = cut(tax, breaks = c(0,350,500,Inf), 
                        include.lowest = TRUE, 
                        labels = c("Baja","Media","Alta")) ) |>
  lm(data = _, medv ~ tax.alta) |> tidy() |>  
  kbl(digits = 2, caption = "Tabla 7. Precio según impuestos") |> kable_paper("hover")

Boston |> 
  mutate(black.cat = cut(b, breaks = c(0,100, 395,Inf), 
                         include.lowest = TRUE,
                         labels = c("Baja","Media","Alta")) ) |>
  lm(data = _, medv ~ black.cat) |> tidy() |>  
  kbl(digits = 2, caption = "Tabla 8. Precio según población de color") |> kable_paper("hover")

Boston |> 
  mutate(black.alta = cut(b, breaks = c(0, 100,Inf), 
                          include.lowest = TRUE,
                          labels = c("Baja","Alta")) ) |> 
  lm(data = _, medv ~ black.alta) |> tidy() |>  
  kbl(digits = 2, caption = "Tabla 8b. Precio según población de color") |> kable_paper("hover")
```

(NOTA: estas tablas se podrían haber hecho con un bucle. También notad que NO he mantenido la variable discretizada en los datos, aunque podría haberlo hecho.)

## IMPORTANTE

La fase de análisis exploratorio puede tener resultados interesantes por sí mismos. Es importante comentarlos y discutirlos adecuadamente, en relación con el objetivo del trabajo.

Pero **sobre todo** esta fase tiene como objetivo aprender de los datos de cara a la siguiente: modelización. Por tanto, es mucho más importante **comentar**, aquí o en la fase de modelización cuestiones cómo 

  a. qué variables incluir en los modelos: aunque esto no es crucial para algunos algoritmos que seleccionan.
  
  b.cómo las vamos a incorporar: en función de los resultados previos podemos queremos incluir transformaciones no lineales, discretizaciones, agrupando categorías, etc.

# Modelos


### NOTA IMPORTANTE

En general, hemos usuado habitualmente `step_log()` para transformar en logaritmos la variable dependiente de los modelos. Si al final queremos **predecir** la variable dependiente es preferible generar la variable en logaritmos y usarla directamente en los modelos.

```{r logs}
Boston <- Boston |> mutate(lmedv = log(medv))
```


## Muestras de entrenamiento y prueba

En el análsisis exploratorio, hemos visto que varias variables explicativas podrían ser transformadas tomando logaritmos o polinomios, discretizando, etc. Todo esas transformaciones se pueden hacer en el pre-procesado de `tidymodels`. 

A continuación, hacemos la partición de los datos reservando una proporción del 80% como conjuntos de datos de entrenamiento y el 20% restante como prueba.


```{r particion}
set.seed(1)
bostonPart <- Boston |> initial_split(prop = .8)
```


## Modelos de regresión lineal

Se pueden probar muchisimo modelos de regresión lineal. Demasiados, como deberiáis saber.[^1] Si nuestro objetivo es predecir, es preferible estimar, como hemos discutivos, un modelo (o modelos) más "complejos" mediante regresión regularizada (LASSO y/o "ridge regression"); por ejemplo, un modelos con un polinomio de orden grande o muchas interacciones en lugar de ir probando modelos cuadráticos o cúbicos o solo unas pocas interacciones. Por contra, si nuestro objetivo es interpretar qué variables afectan y cuánto a la variables dependiente, podemos estimar el modelo de regresión lineal después de haber estimado mediante LASSO para aprender qué variables parecen ser relevantes.

[^1]: Todo esta discusión sobre modelos de regresión lineal aplica a regresión logística para problemas de clasificación.

En general, no haremos esto, pero aquí probaremos un modelo de regresión lineal para el precio como variable dependiente y todas las variables restantes como variables independientes; es decir, sin ninguna relación no lineal. Estimamos este modelo aquí como una extensión del análisis exploratorio, pero considerando correlaciones parciales (*ceteris paribus*): el efecto de cada variable, manteniendo el resto constante. Entrenamos el modelo con el conjunto de datos de entrenamiento. A continuación se muestran todos los coeficientes. <!--Finalmente, utilizamos el modelo entrenado para predecir en el conjunto de datos de la prueba y calcular sus métricas.-->

```{r modelo-lineal1}
#| results: asis
lm1_receta <- training(bostonPart) |>            
  recipe(medv ~ chas + crim + zn + indus + nox + rm + 
           age + dis + rad + tax + ptratio + b + lstat) 

lm1_modelo <- linear_reg(mode= "regression", penalty = 0) |>
                    set_engine("lm")

lm1_flujo <- workflow() |>
  add_recipe(lm1_receta) |>
  add_model(lm1_modelo)

lm1_flujo_est <- lm1_flujo |> fit(data = training(bostonPart)) 

lm1_flujo_est |> extract_fit_parsnip() |> tidy() |> 
   kbl(digits = 2, caption = "Tabla 9. Modelo de Regresión") |> kable_paper("hover")
```

Los resultados del modelo con todas las variables son similares a los anteriores con solo una variable cada vez, aunque la edad NO es significativa (*ceteris paribus*). <!--Notad que se ha quitado la variable de impuestos por estar muy correlacionada con otras. -->

<!--
Finalmente, calculamos las métricas de error de este modelo.
```{r modelo-lineal1-metrics}
#| results: asis
lm_metricas <- lm1_flujo |> 
                    last_fit(split = bostonPart,
                             metrics = metric_set(rmse, mae)) |>
                    collect_metrics()
lm_metricas
```
-->

```{r modelo-lineal2}
#| results: asis
lm2_receta <- training(bostonPart) |>            
                recipe(medv ~ chas + crim + zn + indus + nox + rm + 
                    age + dis + rad + tax + ptratio + b + lstat) |> 
                step_log(medv)

lm2_flujo <- workflow() |>
  add_recipe(lm2_receta) |>
  add_model(lm1_modelo)

lm2_flujo_est <- lm2_flujo |> fit(data = training(bostonPart)) 

lm2_flujo_est |> extract_fit_parsnip() |> tidy() |> 
   kbl(digits = 3, caption = "Tabla 10. Modelo de Regresión (en logaritmos)") |> kable_paper("hover")
```

<!--
lm_metricas[[2]] <- lm2_flujo |> 
                    last_fit(split = bostonPart,
                             metrics = metric_set(rmse, mae)) |>
                    collect_metrics()
-->

Notad que la edad sigue sin ser significativa. Pero esto puede deberse a que la relación es no lineal en edad.

<!--
step_poly(age, lstat, degree = 4) |> 
-->

<!--
En primer lugar, consideramos algunas transformaciones no lineales según hemos visto en el análisis exploratorio: el precio en logaritmos y polinomios para edad y porcentaje de población desfavorecida. Luego, incluimos también algunas de las discretizaciones vistas antes e interacciones.

Ahora vamos a incluir versiones discretas de algunas variables, bien nuevas o bien reemplazando otras transformaciones no lineales previas de las mismas.

```{r modelo-lineal3orig}
#| echo: false
#| eval: false
#| results: asis
lm3_receta <- training(bostonPart) |>            
                recipe(medv ~ chas + crim + zn + indus + nox + rm + 
                    age + dis + rad + tax + ptratio + b + lstat) |> 
                step_log(medv) |>
                step_poly(lstat, degree = 4) |> 
                step_cut(dis, breaks = c(0,3,Inf), 
                         include_outside_range = TRUE) |> 
                step_cut(b, breaks = c(0,375, 395,Inf), 
                         include_outside_range = TRUE) |> 
                step_cut(age, breaks = c(0,25,75,Inf), 
                         include_outside_range = TRUE) |> 
                step_dummy(dis, b, age, chas) |> 
                step_interact(terms = ~ rm:nox + starts_with("b"):starts_with("age"))

lm3_flujo <- workflow() |>
  add_recipe(lm3_receta) |>
  add_model(lm1_modelo)

lm3_flujo_est <- lm3_flujo |> fit(data = training(bostonPart)) 

lm3_flujo_est |> extract_fit_parsnip() |> tidy() |> 
   kbl(digits = 2, caption = "Tabla 11. Modelo de Regresión (var. discretas e interacciones)") |> kable_paper("hover")

lm_metricas[[3]] <- lm3_flujo |> 
                    last_fit(split = bostonPart,
                             metrics = metric_set(rmse, mae)) |>
                    collect_metrics()
```

```{r modelo-lineal3}
#| echo: false
#| eval: false
#| results: asis
lm3_receta <- training(bostonPart) |>            
                recipe(medv ~ chas + crim + zn + indus + nox + rm + 
                    age + dis + rad + tax + ptratio + b + lstat) |> 
                step_log(medv) |>
                step_poly(lstat, degree = 4) |> 
                step_cut(dis, breaks = c(0,3,Inf), 
                         include_outside_range = TRUE) |> 
                step_cut(b, breaks = c(0,100, 395,Inf), 
                         include_outside_range = TRUE) |> 
                step_cut(age, breaks = c(0,25,75,Inf), 
                         include_outside_range = TRUE) |> 
                step_dummy(dis, b, age) |> 
                step_interact(terms = ~ rm:nox + starts_with("lstat"):nox + crim:starts_with("b_") )

lm3_flujo <- workflow() |>
  add_recipe(lm3_receta) |>
  add_model(lm1_modelo)

lm3_flujo_est <- lm3_flujo |> fit(data = training(bostonPart)) 

lm3_flujo_est |> extract_fit_parsnip() |> tidy() |> 
   kbl(digits = 2, caption = "Tabla 11. Modelo de Regresión (var. discretas e interacciones)") |> kable_paper("hover")

lm_metricas[[3]] <- lm3_flujo |> 
                    last_fit(split = bostonPart,
                             metrics = metric_set(rmse, mae)) |>
                    collect_metrics() 
```


Un cambio importante: la categoría de mayor edad sí que tiene un efecto significativo sobre el precio. Esto muestra que, a veces, NO debemos descartar una variable por no ser significativa: simplemente refleja que el modelo no es lo suficientemente flexible (no lineal) para capturar la relación.

Si comparamos las métricas de los tres modelos, vemos que el mejor modelo es el segundo, a pesar de que el tercero es más flexible. NOTA: **no podemos usar el $R^2$** para comparar el modelo 1 y los demás porque la variable dependiente no es la misma.
```{r modelo-lineal-metricas}
#| echo: false
#| eval: false
#| results: asis
lm_metricas |> bind_rows(.id = "modelo") |> 
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 12. Métricas de los Modelos de Regresión Lineal") |> kable_paper("hover")
```

También podemos ver la importancia que cada variable tiene en los distintos modelos.

```{r model-lineal-importanc}
#| echo: false
#| eval: false
#| results: hold
#| out.width: "33.33%"
#| fig.show: asis
#| fig.cap: "Figura 11. Imporancia en los tres modelos de regresión lineal"
lm1_flujo_est |> extract_fit_parsnip() |> 
  vip(num_features = 14)

lm2_flujo_est |> extract_fit_parsnip() |> 
  vip(num_features = 14)

lm3_flujo_est |> extract_fit_parsnip() |> 
  vip(num_features = 14)
```

-->

## Modelos de regresión lineal regularizada

Aquí vamos a considerar solamente un variante de regresión regularizada, LASSO, pero debéis recordar que también existe "ridge regression". De hecho, hemos discutido que en determinadas situaciones puede ser preferible a LASSO.

A partir del análisis exploratorio de datos deberíamos decidir tanto qué variables considerare en el modelo como la forma de incluirla (no linealidad, interacciones, etc.). En el caso de LASSO, no es muy crucial puesto que el propio algoritmo se encargará de no seleccionar aquellas no relevantes, así que podemos incluir más variables de las que a priori consideremos más útiles. En otros algoritmos, esto puede tener efectos negativos sobre el funcionamiento del modelo (ej. "overfitting").

En cualquier caso, existen varias cuestiones a tener en cuenta. En primer lugar, tenemos indicios de que puede ser conveniente transformar la variables dependiente en logaritmos. Esto implica que consideraremos estimar los modelos tanto para la variable en niveles como en logaritmos. También tenemos que considerar distintas variantes de transformaciones para las variables explicativas: ¿qué interacciones considerar? ¿qué formas de no linealidad: polinomios o discretización de variables discretas? Para variables categóricas, ¿reagrupamos categorías?

<!--Vamos a considerar algunas otras especificaciones (diferentes combinaciones de variables) dentro del modelo lineal. -->
Por simplicidad, solo considero unas pocas especificaciones (diferentes combinaciones de variables); vosotros **debéis considerar más**, aunque quizás no tengáis que reportar todas. Lo importante es **justificar** por qué se ha decidido probar esas variantes (en particular, en función del análsis exploratorio) y por qué se han elegido las que consideréis. NOTAD que podemos partir directamente de un modelo LASSO muy general sin haber hecho ningún modelo de regresión lineal previo.

<!--Para reducir la complejidad de la elección de variables en el modelo lineal podemos considerar modelos de red elástica y/o LASSO. Nuevamente, existen muchas combinaciones iniciales de variables y aquí solo estudiaremos un par de ellas: las del primer y segundo modelos anteriores. NOTAD que podemos elegir modelos para LASSO mucho más complejos y NO necesariamente basados en modelos de regresión previos. De hecho, podemos partir directamente de un modelo LASSO muy general sin haber hecho ningún modelo de regresión lineal previo.-->

En particular, en los modelos LASSO solo mostraremos resultados para la variable dependiente en logaritmos; pero vosotros debéis probar y mostrar alguno con la variable dependiente en niveles. Sí consideraré algunas especificaciones usando polinomios o discretizando algunas variables continuas para capturar no linealidades y usando interacciones.

Además de especificar las variables dependiente y explicativas del modelo, la receta debe incluir un preprocesado de las variables adecuado a este algoritmo. No se pueden pasar factores a LASSO, por lo que debemos convertirlos en dummies con `step_dummy`, y las variables continuas se deben estandarizar.

```{r modelos-LASSO}
#| results: asis
recetaLASSO1 <- training(bostonPart) |>            
  recipe(lmedv ~ chas + crim + zn + indus + nox + rm + 
           age + dis + rad + tax + ptratio + b + lstat) |> 
  step_cut(b, breaks = c(0, 350, 395, 500), 
           include_outside_range = TRUE) |> 
  step_normalize(all_predictors(), -all_nominal()) |>
  step_poly(lstat, age, degree = 8) |> 
  step_dummy(b, chas) |> 
  step_interact(terms = ~ rm:(nox + starts_with("age_") + starts_with("lstat_"))) |>  
  step_interact(terms = ~ nox:(crim + starts_with("age_") + starts_with("lstat_"))) |> 
  step_interact(terms = ~ starts_with("b_"):(crim + starts_with("age_")))

recetaLASSO2 <- training(bostonPart) |>            
                recipe(lmedv ~ chas + crim + zn + indus + nox + rm + 
                    age + dis + rad + tax + ptratio + b + lstat) |> 
                step_cut(age, breaks = seq(0,100,10), 
                         include_outside_range = TRUE) |> 
                step_cut(dis, breaks = c(0, 3, 10), 
                         include_outside_range = TRUE) |> 
                step_cut(b, breaks = c(0, 350, 395, 500), 
                         include_outside_range = TRUE) |> 
                step_normalize(all_predictors(), -all_nominal()) |>
                step_poly(lstat, degree = 8) |> 
                step_dummy(age, dis, b, chas)  |> 
                step_interact(terms = ~ rm:(nox + starts_with("age_") + starts_with("lstat_"))) |>  
                step_interact(terms = ~ nox:(crim + starts_with("age_") + starts_with("lstat_"))) |> 
                step_interact(terms = ~ starts_with("b_"):(crim + starts_with("age_")))
```

Definimos los modelos, con el hiperparámetro de penalización para ajustar, y combinamos la receta y el modelo en un flujo.

```{r modelo-LASSO1}
modeloLASSO <- linear_reg(mode= "regression", penalty = tune()) |>
                    set_engine("glmnet")

flujoLASSO1 <- workflow() |>
  add_recipe(recetaLASSO1) |>
  add_model(modeloLASSO)

flujoLASSO2<- workflow() |>
  add_recipe(recetaLASSO1) |>
  add_model(modeloLASSO)
```

La estimación del modelo implica ajustar el hiperparámetro $\lambda$ mediante validación cruzada en la muestra de entrenamiento. Necesitamos probar varios varios rangos de valores para el hiperparámetro.


```{r modelo-lasso1, results='asis',fig.show='hold', fig.cap="Figura 11. Ajuste de lamdba, modelo 1"}
set.seed(9753)
Boston_cv <- training(bostonPart) |> vfold_cv(v=10)

LASSO_grid <- grid_regular(penalty(range = c(0, 1), trans = NULL),   
                          levels = 51)                     

set.seed(1)
lasso1_flujo_tuned <- flujoLASSO1 |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = LASSO_grid                          
                          ) 
lasso1_flujo_tuned |> autoplot()

####
LASSO_grid <- grid_regular(penalty(range = c(0, 0.02), trans = NULL),   
                          levels = 51)                     

set.seed(1)
lasso1_flujo_tuned <- flujoLASSO1 |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = LASSO_grid                          
                          ) 

lasso1_flujo_tuned |> autoplot()
```

<!--
lasso1_flujo_tuned |> collect_metrics() |> 
  ggplot(aes(x=penalty, y=mean)) + 
              geom_line() + geom_point(color="red") + 
              geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), color="gray") 

lasso1_flujo_tuned |> collect_metrics() |> 
  ggplot(aes(x=penalty, y=mean)) + 
              geom_line() + geom_point(color="red") + 
              geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), color="gray") 
-->

<!--

-->

```{r modelo-lasso1b}
#| results: asis
lasso1_flujo_tuned |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 11. Mejores lambdas en el Modelo 1 de LASSO") |> kable_paper("hover")
# lambda1 <- 0 #
lambda1 <- lasso1_flujo_tuned |> select_best(metric = "rmse")
```

Notad que muestro dos búsquedas de valores, refinando en la segunda la zona que aparecía como más probable en la primera. En general, se deberían probar varios rangos para el hiper-parámetro buscando una forma de U para el valor con mínimo error; se puede empezar con rangos amplios y luego buscar valores en un rango más fino, entorno al valor donde se ve el mínimo. NO está claro que queráis mostrarlos todos o incluso que queráis mostrar más de uno.

```{r modelo-lasso2, results='asis',fig.show='asis', fig.cap="Figura 12. Ajuste de lamdba, modelo 2"}
LASSO_grid <- grid_regular(penalty(range = c(0, 0.2), trans = NULL),   
                          levels = 51)                     

set.seed(1)
lasso2_flujo_tuned <- flujoLASSO2 |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = LASSO_grid                          
                          ) 

lasso2_flujo_tuned |> autoplot()

####
LASSO_grid <- grid_regular(penalty(range = c(0, 0.02), trans = NULL),   
                          levels = 51)                     

set.seed(1)
lasso2_flujo_tuned <- flujoLASSO2 |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = LASSO_grid                          
                          ) 

lasso2_flujo_tuned |> autoplot()

```


```{r modelo-lasso2b}
#| results: asis
lasso2_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 12. Mejores lambdas en el Modelo 2 de LASSO") |> kable_paper("hover")
# lambda2 <- 0 
lambda2 <- lasso2_flujo_tuned |> select_best(metric = "rmse")
```

Vamos a finalizar los modelos y ver sus métricas en la muestra de prueba.


```{r modelos-LASSO-final}
#| results: asis
LASSO_final <- list() 
LASSO_final[[1]] <- flujoLASSO1 |> 
                    finalize_workflow(lambda1) |> 
                    last_fit(bostonPart)  |> 
                    collect_metrics()

LASSO_final[[2]] <- flujoLASSO2 |> 
                    finalize_workflow(lambda2) |> 
                    last_fit(bostonPart)  |> 
                    collect_metrics()

LASSO_final |> bind_rows(.id = "modelo") |> 
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 13. Métricas de LASSO") |> kable_paper("hover")
```

En este caso, ambos modelos tienen un comportamiento predictivo muy similar. Esto puede deberse a que ambos son muy similares, salvo por usar polinomios en lugar de discretizaciones. No es raro que ambos tengan un comportamiento predictivo similar, puesto que ambas formas aproximan (aunque de forma ligeramente distinta) relaciones no lineales.

## kNN

Consideremos modelos de k vecinos. Este método es suficientemente flexible para considerar posibles no linealidades en todas las variables. Vamos a ajustar el número de vecinos por validación cruzada con el precio y su logaritmo como variables dependientes. Podríamos considerar otras variantes como usar el absoluto para la distancia o usar más o menos variables explicativas en el modelo; también usar discretizaciones de variables continuas (aunque esto no es muy habitual ni útil en kNN).

NOTA: como vemos aquí, conviene empezar con un rango amplio de valores para el hiperparámetro, sin probar cada valor del rango (ej., entre 1 y 21 aumentando valores de 2 en 2), y luego refinar en la zona donde parece estar el mínimo.

```{r modelo-knn1, results='hold',fig.show='asis', fig.cap="Figura 13. Ajuste de k vecinos, modelo 1"}

knn1_receta <- training(bostonPart) |>            
  recipe(medv ~ chas + crim + zn + indus + nox + rm + 
           age + dis + rad + tax + ptratio + b + lstat) |> 
  step_normalize(all_predictors(), -all_nominal()) |>
  step_dummy(all_nominal_predictors())
  
knn1_modelo <- nearest_neighbor(mode = "regression",
                  neighbors = tune(), dist_power = 2) |> 
                set_engine("kknn")

knn1_flujo <- workflow() |>
  add_recipe(knn1_receta) |>
  add_model(knn1_modelo)

knn_grid <- grid_regular(neighbors(range = c(1, 21), trans = NULL),   
                          levels = 11)                     

set.seed(1)
knn1_flujo_tuned <- knn1_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = knn_grid                          
                          ) 

knn1_flujo_tuned  |> autoplot()

knn_grid <- grid_regular(neighbors(range = c(1, 11), trans = NULL),   
                          levels = 11)                     
###########
set.seed(1)
knn1_flujo_tuned <- knn1_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = knn_grid                          
                          ) 

knn1_flujo_tuned  |> autoplot()

```

```{r modelo-knn1b}
#| results: asis
knn1_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 14. k-vecinos óptimo en el Modelo 1") |> kable_paper("hover")
k1 <- knn1_flujo_tuned |> select_best(metric = "rmse")
```


```{r modelo-knn2, results='asis',fig.show='asis', fig.cap="Figura 14. Ajuste de k vecinos, modelo 2"}

knn2_receta <- training(bostonPart) |>            
  recipe(lmedv ~ chas + crim + zn + indus + nox + rm + 
           age + dis + rad + tax + ptratio + b + lstat) |> 
  step_normalize(all_predictors(), -all_nominal()) |>
  step_dummy(all_nominal_predictors())

knn2_flujo <- workflow() |>
  add_recipe(knn2_receta) |>
  add_model(knn1_modelo)

knn_grid <- grid_regular(neighbors(range = c(1, 11), trans = NULL),   
                          levels = 11)  

set.seed(1)
knn2_flujo_tuned <- knn2_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = knn_grid                          
                          ) 

knn2_flujo_tuned  |> autoplot()

```

```{r modelo-knn2b}
#| results: asis
knn2_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 15. k-vecinos óptimo en el Modelo 2") |> kable_paper("hover")
k2 <- knn2_flujo_tuned |> select_best(metric = "rmse")
```

El número óptimo de vecinos es `r k1` para el primer modelo y `r k2` en el segundo. Vamos a finalizar los modelos y ver sus métricas en la muestra de prueba.


```{r modelos-knn1-final}
#| results: asis
knn_final <- list() 
knn_final[[1]] <- knn1_flujo |> 
                    finalize_workflow(k1) |> 
                    last_fit(bostonPart)  |> 
                    collect_metrics()

knn_final[[2]] <- knn2_flujo |> 
                    finalize_workflow(k2) |> 
                    last_fit(bostonPart)  |> 
                    collect_metrics()

knn_final |> bind_rows(.id = "modelo") |> 
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 16. Métricas de kNN") |> kable_paper("hover")
```

Comparando estos resultados con los de la Tabla para LASSO, queda claro que kNN tiene una mejor capacidad predictiva, tanto para la variable de precio como usando el logaritmo del precio. Sin embargo, estos modelos no nos dicen mucho sobre qué factores afectan a esa predicción (no tenemos ni modelo paramétrico y contrastes, ni representación gráficas o medidas de importancia).

## Árboles de regresión

Pasamos a considerar modelos de árboles de decisión. Notad que para ajustar el coste de complejidad puede ser necesario varios rangos. Dado que hemos visto que sistemáticamente el modelo con el logaritmo del precio predice mejor, nos centramos en ese modelo (aunque esto es obviamene una elección y quizás deberían probarse más).

```{r modelo-arbol2, results='asis',fig.show='asis', fig.cap="Figura 15. Ajuste de árboles, modelo 2"}

arbol2_receta <- training(bostonPart) |>            
  recipe(lmedv ~ chas + crim + zn + indus + nox + rm + 
           age + dis + rad + tax + ptratio + b + lstat) 

arbol2_modelo <-  decision_tree(mode = "regression", 
                                     cost_complexity = tune()) |> 
                          set_engine("rpart") 

arbol2_flujo <- workflow() |>
  add_recipe(arbol2_receta) |>
  add_model(arbol2_modelo)

arbol_grid <- grid_regular(cost_complexity(range = c(0, 0.01), trans = NULL),   
                          levels = 11)                     
set.seed(1)
arbol2_flujo_tuned <- arbol2_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = arbol_grid                          
                          ) 

arbol2_flujo_tuned  |> autoplot()
```

```{r modelo-arbol2b}
#| results: asis
arbol2_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 17. Coste de complejidad  en el Modelo 1") |> kable_paper("hover")
cost2 <- arbol2_flujo_tuned |> select_best(metric = "rmse")
```

```{r modelos-arbol-final}
#| results: asis
arbol_final <- arbol2_flujo |> 
                    finalize_workflow(cost2) |> 
                    last_fit(bostonPart)  

arbol_final |>  collect_metrics() |>  
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 18. Métricas del árbol") |> kable_paper("hover")
```


Vemos que el mejor árbol de regresión es mejor que LASSO y similar a kNN (en logaritmos). Además podemos ver el flujo del árbol para hacernos una idea de qué factores están afectando a la predicción

```{r modelos-arbol-final2, results='asis', fig.show='asis', fig.cap="Figura 16a. Mejor árbol de regresión, modelo 2"}
arb <- arbol_final$.workflow[[1]] |>   
  extract_fit_parsnip() 

rpart.plot(arb$fit)
```

También podemos recurrir a medidas de importancia de las variables.

```{r modelos-arbol-final3, results='asis', fig.show='asis', fig.cap="Figura 16b. Importancia en el mejor árbol de regresión, modelo 2"}
arbol_final$.workflow[[1]] |>   
  extract_fit_parsnip() |> 
  vip(num_features = 14)
```

Estos resultado son interesantes, más allá de su capacidad predictiva, porque son informativos son las interacciones entre distintas características. Como sabemos y podemos ver, los árboles trabajan discretizando las variables continuas discretizadas. Pero además implican distintas interacciones entre rangos de las variables para el resultado final. Por ejemplo, vemos que en los niveles superiores del árbol tenemos distintos niveles de población desfavorecida en el barrio, `lstat`; de hecho, vemos también (como era esperable) que es la variable con mayor valor de la importancia. Pero según los distintos valores de `lstat` importan diferentes características: cuando `lstat` es bajo ($<10$) el precio viene determinado por el número de habitaciones de la vivienda, mientras que cuando es alto por la cantidad de contaminación o la proporición de gente de color en el vecindario.


## Random Forests

Intentamos ver si un modelo de "random forests" puede mejorar al árbol de regresión u otros modelos.

```{r modelo-rf2, results='asis',fig.show='asis', fig.cap="Figura 17. Ajuste de RF, modelo 2"}
rf2_receta <- arbol2_receta 

rf2_modelo <-  rand_forest(mode = "regression",
                                       mtry = tune(), trees = 100) |> 
                          set_engine("ranger", importance = "impurity")

rf2_flujo <- workflow() |>
  add_recipe(rf2_receta) |>
  add_model(rf2_modelo)

rf_grid <- grid_regular(mtry(range = c(1, 12), trans = NULL),   
                          levels = 12)                     

rf2_flujo_tuned <- rf2_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = rf_grid                          
                          ) 

rf2_flujo_tuned |> autoplot()
```

```{r modelo-rf2b}
#| results: asis
rf2_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 19. Ajuste de Random Forest  en el Modelo 2") |> kable_paper("hover")
mtry2 <- rf2_flujo_tuned |> select_best(metric = "rmse")
```

El número óptimo de variables a usar (aleatoriamente) en cada nodo son `r mtry2`, aunque podría ser alguna menos o algunas más, dado que los valores de la métrica de error no son significativamente diferentes.


Por último finalizamos el modelo

```{r modelos-rf-final}
#| results: asis
rf_final <- rf2_flujo |> 
                    finalize_workflow(mtry2) |> 
                    last_fit(bostonPart)  

rf_final |>  collect_metrics() |>  
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 20. Métricas de RF") |> kable_paper("hover")
```

El modelo de "Random Forest" incluso mejora claramente a "kNN". En este caso, podemos ver qué variables han resultados mejores predictores del precio.

```{r RF-vip, results='asis', fig.show='asis', fig.cap="Figura 18. Importancia en el mejor RF, modelo 2"}
rf_final$.workflow[[1]] |>   
  extract_fit_parsnip() |> 
  vip(num_features = 14)
```

Este gráfico nos indica que la variable más importante (con diferencia) es la relacionada con la "pobreza" de la zona en la que se encuentra la casa. Después nos encontramos una característica de la casa en sí (número de habitaciones, aunque realmente es la media de la zona) y la criminalidad de la zona. No existe un criterio formal para decidir en que punto parar determinar qué variables son importantes. Quizás en este caso se puede "argumentar" que hay una caída mayor de la importancia después de la contaminación de la zona, la calidad de las escuelas y la distancia al centro.

<!--
## SVM

Intentamos ver si un modelo de "random forests" puede mejorar al árbol de regresión u otros modelos.

```{r modelo-svmlin2, echo=FALSE, eval=FALSE, results='asis',fig.show='asis', fig.cap="Figura 19. Ajuste de SVM, modelo 2"}
svm2_receta <- knn2_receta

library(kernlab)
svmlin2_modelo <-  svm_linear(mode = "regression",
                                 cost = tune()) |> 
                          set_engine("kernlab")

svmlin2_flujo <- workflow() |>
  add_recipe(svm2_receta) |>
  add_model(svmlin2_modelo)

svmlin2_grid <- grid_regular(cost(range = c(0, 2), trans = NULL),   
                          levels = 4)                     

svmlin2_flujo_tuned <- svmlin2_flujo |> 
                        tune_grid(
                          resamples = Boston_cv,
                          metrics   = metric_set(rmse),
                          grid      = svmlin2_grid                          
                          ) 

svmlin2_flujo_tuned |> collect_metrics() |> 
  ggplot(aes(x=cost, y=mean)) + 
              geom_line() + geom_point(color="red") + 
              geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), color="gray") 
```

```{r modelo-svmlin2b, echo=FALSE, eval=FALSE}
#| results: asis
svmlin2_flujo_tuned  |> show_best(metric = "rmse") |> 
  kbl(digits = 4, caption = "Tabla 21. Ajuste de SVM Lineal  en el Modelo 2") |> kable_paper("hover")
costlin2 <- svmlin2_flujo_tuned |> select_best(metric = "rmse")
```

El número óptimo de variables a usar (aleatoriamente) en cada nodo son `r mtry2`, aunque podría ser alguna menos o algunas más, dado que los valores de la métrica de error no son significativamente diferentes.


Por último finalizamos el modelo

```{r modelos-svmlin-final, echo=FALSE, eval=FALSE}
#| results: asis
svmlin2_final <- svmlin2_flujo |> 
                    finalize_workflow(costlin2) |> 
                    last_fit(bostonPart)  

smvlin2_final |>  collect_metrics() |>  
  pivot_wider(names_from = .metric, values_from=.estimate) |> 
  select(-.estimator) |> 
  kbl(digits = 4, caption = "Tabla 22. Métricas de SVM lineal") |> kable_paper("hover")
```

El modelo de "Random Forest" incluso mejora claramente a "kNN". En este caso, podemos ver qué variables han resultados mejores predictores del precio.

```{r SVMlin-vip, echo=FALSE, eval=FALSE, results='asis', fig.show='asis', fig.cap="Figura 20. Importancia en el mejor SVM lineal, modelo 2"}
svmlin2_final$.workflow[[1]] |>   
  extract_fit_parsnip() |> 
  vip(num_features = 14)
```

Este gráfico nos indica que la variable más importante (con diferencia) es la relacionada con la "pobreza" de la zona en la que se encuentra la casa. Después nos encontramos una característica de la casa en sí (número de habitaciones, aunque realmente es la media de la zona) y la criminalidad de la zona. No existe un criterio formal para decidir en que punto parar determinar qué variables son importantes. Quizás en este caso se puede "argumentar" que hay una caída mayor de la importancia después de la contaminación de la zona, la calidad de las escuelas y la distancia al centro.

-->

# Conclusión

Uno de los objetivos de este informe era determinar los atributos del vecindario que mejor explicaban (no necesariamente causan) la variación en los precios de las viviendas. Los mejores modelos son regresión lineal, que incorpora relaciones no lineales, y "random forest" que también establece una relación altamente no lineal entre el precio y los factores observados. También ofrecen herramientas para interpretar los resultados. Ambos modelos coinciden en la importancia de que el barrio esté en una zona "deprimida" (mucho porcentaje de población con bajo estatus social) en la determinación del precio. Con mayor o menor importancia, también coinciden en la relevancia de la criminalidad en la zona, el número medio de habitaciones y el ratio de alumnos por profesor. Algunas características como la edad o una localización cercana al río no parecen ser importantes; esto es una diferencia respecto al análisis de covariación, donde sí se apreciaba cierta correlación. Esto nos indica que esas características están relacionadas con otras que sí son más relevantes en la fijación de precios. Los resultados del modelo de árboles también son relevantes para aprender sobre las interacciones entre características para determinar el precio de las casas. Debéis usar estos resultados para reestimar modelos más interpretables (como regresión lineal o árboles) para hacer comentarios específicos sobre el impacto de cambios **concretos** de variables particularmente relevantes sobre el precio de las casas. PENSAD en qué cuestiones pueden ser interesantes si estáis trabajando en una inmobiliaria o en una empresa de tasación de viviendas, en este caso, o en una empresa del sector apropiado para los datos que elijáis.

Por otro lado estos resultados también se podrían utilizar para predecir el precio de una nueva casa (o saber si una en el mercado está sobrevalorada).

```{r predic-final}
#| results: asis
rf_final$.workflow[[1]] |> 
  predict(new_data = tibble(chas = "Yes", crim = 0.03,  zn = 0,
                            indus = 7, nox = 0.5, rm = 2,
                            age = 10,  dis = 6, rad = 5,  
                            tax = 290, ptratio = 20, b = 390, lstat=9)) |> 
  exp()
```

Notad que debemos deshacer el logaritmo para dar una predicción en unas unidades con sentido (miles de dólares). Recordad que también podemos ofrecer un intervalo de confianza para la predicción.

Fijaos que en otros contextos o con otros datos podéis tener un caso más obvio que predecir (o no). Y podéis usar intervalos de confianza en lugar de predicción puntual para determinar si una casa está sobrevalorada o infravalorada. En cualquier caso, debéis elegir y *justificar* los valores elegidos para predecir: en este caso, una casa observada en los datos con particular interés, una casa común en la zona. En el proyecto que elijáis, también deben existir razones para que ese caso sea relevante para predecir.

Recordad que también podemos estar interesados en usar el modelo para interpetar el efecto de alguna o algunas variables. Nuevamente, tendréis que justificar por qué esas variables son interesantes y cómo se interpretan los resultados. Como hemos visto, podemos considerar la información que LASSO y/o "Random Forest" nos dan sobre las variables para estimar modelos más interpretables y extraer conclusiones.

# Apéndice A

Como se ha comentado, nuestro conjunto de datos dispone de 506 observaciones y 14 variables. A continuación se presenta una breve descripción de cada variable:

|Variable    |     Descripción
|:-----------|:--------------------------------------------------------------------
|    CRIM    |    tasa de criminalidad per cápita por ciudad
|    ZN      |    proporción de terreno residencial dividido en 
|            |       zonas para lotes de más de 25,000 pies cuadrados.
|    INDUS   |    proporción de acres de negocios no comerciales por zona
|    CHAS    |    variable "dummy"del río Charles 
|            |       (1 si la zona limita con el río; si no, 0)
|    NOX     |    concentración de óxidos nítricos (partes por 10 millones)
|    RM      |    número medio de habitaciones por vivienda
|    AGE     |    proporción de casas ocupadas por sus propietarios 
|            |       construidas antes de 1940.
|    DIS     |    distancia media ponderada a cinco centros de empleo de Boston
|    RAD     |    índice de accesibilidad a carreteras radiales
|    TAX     |    impuesto sobre el valor total de la propiedad 
|            |       (por cada $10,000)
|    PTRATIO |    número de alumnos por docente en los colegios de la zona ("town")
|    B       |    1000(Bk - 0.63)^2 donde Bk es la proporción de población de color
|            |        en la zona (0.63 es la media en la ciudad)
|    LSTAT   |    porcentaje de población desfavorecida (bajo estatus social)
|    MEDV    |    valor mediano de las viviendas ocupadas por sus propietarios 
|            |       (en miles de dólares)
