---
title: "Tema 05 - Modelización y Aprendizaje Estadístico"
subtitle: "Extraer patrones y relaciones de los datos"
author:
    - "Pedro Albarrán"
    - "Alberto Pérez"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:
        - beige
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    show-notes: false
    notes-format: html
execute:
  enabled: true
  eval: false
  echo: true
  warning: false
  message: false
  output: false
  fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---

```{r}
#| label: setup
#| include: false
#| eval: true


# Elimino todo del Entorno
rm(list = ls())

# Cargo bibliotecas necesarias
library(tidyverse)
library(kableExtra)
library(broom)
library(modelsummary)
```

```{r}
#| label: generar-datos
#| echo: false
#| eval: false

source("Tema05datos.R")
```

# Métodos Estadísticos

## Métodos Estadísticos

:::: {.columns}

::: {.column width="40%"}


- La **modelización** mediante métodos estadísticos permite

    - Encontrar patrones <!--(complejos), y cuantifificar su fortaleza -->

    - Interpretar datos <!--(información)-->

:::

::: {.column width="55%"}

![](figure/data-science-model.png){width=100% fig-align="center"}
:::

::::

. . .

- Los datos (casos observados) son una **muestra** de una **población** mayor (casos potenciales)

```{r}
#| echo: false
#| eval: false
data <- tibble(x = runif(n=25, min=0, max=20))
ggplot(data, aes(x)) + geom_vline(xintercept = 10, color = "red") +
  geom_density()
```

```{r}
#| echo: true
#| eval: false
# Población: ventas diarias ~ Uniforme(0, 20), media = 10
set.seed(9915)
tibble(x = runif(n=1e9, min=0, max=20)) |> pull(x) |> mean()

# Muestra de solo 25 días
set.seed(9915)
tibble(x = runif(n=25, min=0, max=20)) |> pull(x) |> mean()
```

- El estadístico varía entre muestras.

- ¿Cómo de fiable es este estadístico en una muestra?


## Variabilidad muestral

- Con **todas** las muestras potenciales de tamaño $\small n$ conoceríamos la **distribución muestral** de un estadístico (información calculada en una muestra)

    - Aproximación de "todas" las muestras potenciales con 1000 muestras

::::{.notes}
- Simulemos la distribución para la media en muchas muestras de $\small n=25$
::::

```{r}
set.seed(101)
nsim <- 1000
ybar <- numeric(nsim)
for (j in 1:nsim) {
  muestra <- runif(n=25, 0, 20)
  ybar[j] <- mean(muestra)
}

as_tibble(ybar) |> ggplot(aes(x=value)) + geom_density() +
  geom_vline(xintercept=10)
```

::::{.notes}
Concepto fundamental: cualquier estadístico muestral tiene incertidumbre asociada.
::::

- Tenemos una ÚNICA MUESTRA: ¿cómo cuantificar la incertidumbre?

1. *Suponiendo* que los datos son normales, la media tiene una distribución normal

2. [**Teorema Central del Límite**](https://openintro.shinyapps.io/CLT_mean/): para datos de cualquier distribución,  $\overline{Y} \overset{a}{\sim} N(\mu, \sigma^2/n)$ cuando $n \to \infty$

::::{.notes}

- *Ley de números grandes*: para un tamaño de la muestra $n$ grande, el promedio de la muestra está cerca de la media de la población <!--, y el error estándar es pequeño. -->

- *Teorema de Límite Central*: para un tamaño de la muestra $n$ grande, la distribución muestral de la media es normal.

- Permite hacer inferencia sin suponer que los datos sean normales, solo necesitamos n suficientemente grande.
::::

## Procedimiento Bootstrap

::: {style="font-size: 95%;"}

**Idea**: Usar la ÚNICA muestra como si fuera la población → generar variación "similar" a la muestral

::::{.notes}

- genera variación de remuestras a partir de una única muestra

::::

```{r}
set.seed(101)
UNICAmuestra <- tibble(x=runif(n=25, 0, 20))
```

1. Tomar muchas remuestras **con reemplazamiento** de la muestra original

::::{.notes}

- p.e., para (1,2,3) incluye los casos (1,1,2), (1,1,3), (2,2,1), etc.

- *remuestras* o muestras de Bootstrap

- ${n\choose n}= (n + n - 1)!/(n! (n-1)!)$ remuestras: la muestra original es una combinación entre muchas

::::

```{r}
#| echo: true
#| eval: false
library(rsample)
remuestras <- bootstraps(UNICAmuestra, times = 1000)
```

2. Calcular el estadístico en cada remuestra

```{r}
#| echo: true
#| eval: false
distrib <- list()
for (r in 1:1000) {
  remuestra <- remuestras$splits[[r]] |> as_tibble()
  media  <- remuestra |> pull(x) |> mean()
  sd    <-  remuestra |> pull(x) |> sd()
  distrib[[r]] <- list(medias = media, sds =sd )
}
distribDF <- distrib |> bind_rows()
```

::::{.notes}

IDEA de un bucle: saber como hacerlo en un caso, y luego hacer en general y se guarda en una lista

  remuestra <- remuestras$splits[[1]] |> as_tibble()

  -  hago todo lo que tenga que hacer,
   con todos los pasos que sean necesarios
  media  <- remuestra |> pull(x) |> mean()
  sd    <- sd(remuestra$x)
  - guardo los resultaods

::::

- La **distribución de remuestras** es "similar" a la distribución muestral sin supuestos (normalidad, TCL)

::::{.notes}

- La **distribución muestral bootstrap** NO es la distribución muestral, pero aproxima sus aspectos principales (*error estándard*) sin supuestos (normalidad, TCL)

::::

```{r}
#| echo: true
#| eval: false
distribDF |> ggplot(aes(x=medias)) + geom_density()               # distribución
mediaSE <- distribDF |> pull(medias) |> sd()                      # ES de la media
mediaIC <- distribDF |> pull(medias) |> quantile(c(0.025, 0.975)) # IC de la media
```

```{r}
#| echo: false
#| eval: false
library(boot)
set.seed(101)
UNICAmuestra <- tibble(x=runif(n=25, 0, 20))

mediaFunc <- function(datos,  indices) {
  remuestra <- datos[indices,]
  return(mean(remuestra))
}

(bootRes <- boot(data = UNICAmuestra, statistic = mediaFunc, R = 1000))
```

:::

# Aprendizaje Estadístico

## ¿Qué es el aprendizaje estadístico?

- Aprendizaje automático (*machine learning*, ML) o estadístico (*statiscal learning*): conjunto de técnicas algorítmicas para **extraer información de los datos**

* [Tipos principales](https://vitalflux.com/wp-content/uploads/2020/12/mind_map_machine_learning_3.jpg)

1. **Aprendizaje supervisado**: para cada medida de $X$ hay una *respuesta asociada* $Y$

    + Aprendemos la respuesta $Y$ de casos nuevos a partir de casos previos

2. **Aprendizaje no supervisado**: no hay una respuesta asociada, aprendemos rasgos no medidos

    + ej., observaciones similares organizadas en grupos distintos

::::{.notes}
1. **Aprendizaje supervisado**: escenarios en los que para cada observación de las mediciones $X_i$ hay una *respuesta asociada* $Y_i$ ("supervisa" el aprendizaje)

    + Aprendemos la respuesta $Y$ de casos nuevos a partir de casos previos

2. **Aprendizaje no supervisado**: no hay una respuesta asociada a las mediciones de $X_i$ para supervisar el análisis que generará un modelo.

    + Aprendemos rasgos no medidos a partir de casos "no etiquetados": ej. observaciones similares organizadas en grupos distintos (de clientes, países)

::::

## Aprendizaje supervisado

$$
\small Y = f(X) + \varepsilon
$$

- Modelo para la variable dependiente en función de

  + factores observados (predictores/características)

  + factores no observados ($\small \varepsilon$)

  + $f$ representa la relación sistemática que $X$ (género, educación, etc.) ofrece sobre $Y$ (ej. renta)

::::{.notes}

- Y= variable objetivo, variable dependiente, de respuesta
- X= independientes (predictores, características, regresores, factores)
- X= inputs, features, covariates

+ $f$ representa la información/relación sistemática que $X$ (género, educación, etc.) ofrecen sobre un resultado medido $Y$ (ej. renta)

::::

- Objetivos:

    1. Predecir el valor esperado de $\small Y$ para casos *nuevos*

    2. Comprender cómo afectan al resultado esperado de $\small Y$ cambios en los valores de las características

        + p.e., tiene la experiencia un efecto positivo o nula sobre la renta esperada

        + ¡cuidado con afirmaciones de *causalidad*!

::::{.notes}

1. a partir de otros previamente etiquetados (medidos/clasificados)

2. evaluar la calidad de nuestras predicciones e inferencias

::::

- En ambos casos, tenemos que estimar ("aprender") la $f$ desconocida

## Aprendizaje supervisado (cont.)

- Según la naturaleza de $Y$ tenemos:

  - **Problemas de Regresión**: $Y$ cuantitativa (salario, ventas, precio)

  - **Problemas de Clasificación**: $Y$ cualitativa (compra/no compra, categoría producto)

::::{.notes}

1.- Variable de respuesta cuantitativa (toma valores numéricos)

2.- variable de respuesta cualitativa (toma valores en una de $C$ categorías o clases)

::::

. . .

- Dos enfoques para especificar la relación $f$:

  1. **Modelo Paramétrico**: supone un forma de $f$ que depende de parámetros desconocidos, p.e., lineal  $f(x) =\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k$

     - Más fácil de estimar e interpretar
     - Menos flexible

  2. **Modelo No paramétrico**: Ajustar sin supuestos funcionales
     - Más flexible, mejor ajuste
     - Más difícil de interpretar

  - La capacidad predictiva de un modelo mejora si incluimos más variables explicativas (modelo más flexible)

# Regresión Lineal

## Modelo de regresión lineal

- Modelo paramétrico para problemas de regresión que supone una relación lineal:

$$\quad \small \color{blue}{ventas = \beta_0 + \beta_1 renta + \beta_2 descuento + u}$$


::::{.notes}

      + en este caso, la renta es 0 y el individuo no tiene descuento

::::

1. Constante $\small \color{blue}{\beta_0= E(ventas|renta=0, descuento=0)}$

    + valor esperado de $\small Y$, ventas, cuando todas las variables explicativas son cero

2. Pendiente de una variable continua $\small \color{blue}{\beta_1 = \frac{\delta E(ventas|renta,descuento)}{\delta{renta}}}$

    + cambio esperado de $\small Y$, ventas, cuando la variable explicativa, renta,  aumenta en una unidad, manteniendo el resto de variables constantes (en este caso, un valor dado para descuento)

    +  también $\small \beta_1 = E(ventas|renta=x+1,descuento=d)-E(ventas|renta=x,descuento=d)$

3. Para variables discretas binarias  $\small \color{blue}{\beta_2 = E(ventas|renta=x,descuento=1)-E(ventas|renta=x,descuento=0)}$

    + diferencia del valor esperado de $\small Y$, ventas, para el grupo indicado por la variable (tienen descuento) respecto al grupo de referencia (no tienen descuento), manteniendo el resto de variables (en este caso, renta) constante

## Regresión Lineal: Estimación

- Objetivo: estimar los coeficientes poblacionales desconocidos a partir de una muestra

- Los coeficientes estimados minimizan la Suma Cuadrática de Residuos, $\small \hat{e}$ (errores de estimación): es decir , las **distancias** entre

  - los datos observados: $\small y_i$

  - y los datos predichos por el modelo estimado $\small \hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 X_1+ \dots + \hat{\beta}_k X_p$

:::: {.columns}

::: {.column width="55%"}

![](figure/fig_E2b.jpeg){width=100% fig-align="center"}

:::

::: {.column width="40%"}

$$
\small SCR=\sum_{i=1}^{n} \hat{e}_i^2= \sum_{i=1}^{n} ( y_i - \hat{y}_i)^2
$$

<!-- + Por tanto, también minimiza $\small ECM = \frac{SCR}{n} = MSE$ -->

:::

::::

* Esto equivale a las condiciones derivadas de suponer $\small E(\varepsilon|X)=0$


::::{.notes}

```{r}
#| echo: true
#| eval: false
# Usar datos del ejemplo original
descuento <- read.csv("data/descuento.csv") |>
  mutate(zona = parse_factor(zona), mujer = as.factor(mujer))

modelo <- lm(ventas ~ renta + descuento, data = descuento)
summary(modelo)
```

**Interpretación:**

- Coef. de `renta`: Cambio en ventas por cada mil euros más de renta
- Coef. de `descuento`: Diferencia en ventas entre quienes tienen y no tienen descuento

::::

## Regresión Lineal: Estimación (cont.)

- Los parámetros del modelo $f(.)$ estimado, los $\small \beta_j$, son estadísticos con **variabilidad muestral**, medida por  su **error estándar** $\small se(\widehat{\beta}_j)$

    - Usados para construir intervalos de confianza y para contrastar hipótesis sobre los parámetros poblacionales, p.e., significatividad ($\small \beta_1=0$) y signo.


::::{.notes}
$$
\small se(\widehat{\beta}_j) =  \frac{\sigma^2}{(n-1)S^2_{x_j} (1 - R^2_{j})}
$$


+ $\small \sigma^2=Var(\varepsilon)$, estimada con $\small \frac{SCR}{n-k-1}$
+ $\small S^2_{x_j}=\frac{\sum (x_{ij}-\bar{x}_j)^2}{n-1}=$varianza muestral de $\small X_j$
+ $\small R^2_{j}$ es el $\small R^2$ de la regresión de $\small X_j$ sobre el resto de regresores

    + individual: $\small H_0: \beta_1=0$ con un estadístico $\small t=\frac{\widehat{\beta}_1-0}{se(\widehat{\beta}_1)}$
    + conjunta: $\small H_0: \beta_1=\beta_2=\dots=\beta_k=0$ con un estadístico $\small F$

* Medidas de la precisión del modelo: $\small MSE$ o $\small R^2=1-\frac{SCR}{SCT} = \frac{SCE}{SCT}$

::::

- La función `lm()` con la notación de fórmula crea un objeto lista con resultados

```{r}
descuento <- read.csv("data/descuento.csv") |>
        mutate(zona = parse_factor(zona), mujer = as.factor(mujer))
modelo <- lm(data = descuento, ventas ~ renta + descuento)
(sum.modelo <- summary(modelo))
nobs(modelo)
```

- La predicción $\widehat{y}$ también está sujeta a incertidumbre por la estimación: se puede calcular su error estándar e intervalos de confianzas

```{r}
pred <- predict(modelo, type = "response",
                se.fit = T, interval = "confidence")
cbind(descuento$ventas, pred$fit, pred$se.fit) |> head()
```

```{r}
#| echo: false
#| eval: false
ventas_pred <- predict(modelo, type = "response", se.fit = T)

(a <- predict(modelo, type = "response", interval = "confidence") |> head())
(b <- predict(modelo, type = "response", interval = "prediction") |> head())
```


::::{.notes}

"Confounding factor" y Causalidad

 * **"Correlación no implica causalidad"**, salvo en [ensayos científicos aleatorios](https://es.wikipedia.org/wiki/Prueba_controlada_aleatorizada) cuidadosamente controlados
   + en otros campos como marketing digital o analítica de web se denominan [pruebas A/B](https://es.wikipedia.org/wiki/Prueba_A/B) (ej. dos versiones de una misma web)


 * En general (datos observacionales), nos preocupa que otros factores que puedan ser los verdaderos determinantes de la relación observada

 En los ensayos aleatorios se controla quien recibe una intervención (tratamiento) y quien no (control)

    + En promedio, todos los demás factores están equilibrados entre los dos grupos: las diferencias pueden atribuirse a la aplicación del tratamiento

    +  Pero si los sujetos no cumplen con los tratamientos o se pierden en el seguimiento..


```{r}
#| echo: false
#| eval: false
datos |> ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm")
lm(data = datos, sales ~ discount) |> broom::tidy()

# Pero si tenemos en cuenta la renta...

datos |> mutate(renta_baja = income < 7500) |>
  ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm") +
  facet_wrap(~renta_baja)
lm(data = datos, sales ~ discount + income) |> summary()
```

::::

## Ampliando el Modelo de Regresión Lineal

* Incorporar información **cualitativa**: incluimos una variable binaria (*dummy*) para cada categoría, excepto una (grupo de referencia)

    - Cada coeficiente es el efecto diferencial de ese grupo respecto al grupo de referencia

    - Las dummies se crean automáticamente con factores (sin orden)

        - también se pueden usar la biblioteca `fastDummies`

```{r}
lm(data = descuento, ventas ~ renta + descuento + zona) |> summary()
```

. . .

- Modelizar relaciones **no lineales** entre la variable dependiente y las explicativas

- El modelo lineal asume *efecto constante* $\frac{\partial Y}{\partial X_j} = \beta_j$, PERO en la realidad observamos

  - Rendimientos decrecientes (productividad, utilidad)
  - Elasticidades variables
  - Umbrales y efectos discretos
  - Efectos heterogéneos entre grupos

- **Solución**: Transformar variables para capturar estas no linealidades

## Superando la linealidad

1. Usando **logaritmos**, modelizamos elasticiades (cambios porcentuales) y rendimientos decrecientes

```{r}
#| echo: true
#| eval: false
# Y ~ log(X): semi-elasticidad / rendimientos decrecientes
lm(data = descuento, ventas ~ log(renta) + descuento)
# log(Y) ~ log(X): elasticidad constante
lm(data = descuento, log(ventas) ~ log(renta) + descuento)
# log(Y) ~ X: semi-elasticidad / crecimiento exponencial
lm(data = descuento, log(ventas) ~ renta + descuento)
```

::::{.notes}

**Interpretación:**

- $\log(Y) \sim \log(X)$: $\beta$ = elasticidad (% cambio en $Y$ por % cambio en $X$)
- $Y \sim \log(X)$: Efecto decreciente de $X$ sobre $Y$
- $\log(Y) \sim X$: Cambio porcentual en $Y$ por unidad de $X$


Los logaritmos son fundamentales en economía: funciones de producción, demanda, ecuación de Mincer, etc.
::::

. . .

2. Usando **polinomios** (y otras funciones no lineales), modelizamos efectos que varían con el nivel de la variable

```{r}
lm(data=descuento, ventas ~ edad + I(edad^2) ) |> summary()
ggplot(descuento, aes(x=edad, y=ventas)) + geom_point() +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

- El efecto marginal de $edad$ sobre $ventas$, $\small{\frac{\partial ventas}{\partial edad} = \beta_1 + 2\beta_2 edad}$ depende de la $edad$

::::{.notes}
- Si $\beta_2 < 0$: rendimientos decrecientes (forma de U invertida)
- Si $\beta_2 > 0$: rendimientos crecientes (forma de U)

Útil para ciclo de vida, curva de Laffer, relación ingreso-felicidad, etc.
::::

## Superando la linealidad (cont.)

3. **Discretizando** variables también capturamos efectos no lineales: permite efectos "escalón" diferentes para distintos valores

```{r}
lm(data=descuento, ventas ~
     cut(edad, seq(20, 60, 5), include.lowest = TRUE)) |> summary()
ggplot(descuento, aes(x=edad, y=ventas)) + geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ cut(x, seq(20, 60, 5), include.lowest = T))
```

```{r}
#| echo: false
#| eval: false
# Directamente en fórmula (más control)
lm(ventas ~ renta + descuento +
     (edad >= 30 & edad < 40) + (edad >= 40 & edad < 50) + (edad >= 50),
   data = descuento)
```

::::{.notes}
* Se incluyen indicadores binarios para cada clase excepto grupo de referencia
  + la constante recoge el valor medio de $\small Y$ para ese grupo de referencia
  - Cada coeficiente: diferencia respecto al grupo base
  - Permite modelizar umbrales y cambios bruscos

Útil cuando sospechamos efectos discretos: tramos impositivos, niveles educativos, etc.
::::

. . .

4. Usando **interacciones** entre variables, el efecto de un regresor dependerá de otro regresor

  - La "pendiente" cambia con el valor de la otra variable: $\small{\frac{\partial ventas}{\partial mujer} = \beta_1 + \beta_3 mujer}$

  - Deben incluirse *siempre* los factores principales (NO sólo `edad:renta`)

```{r}
lm(data = descuento, ventas ~ edad*renta)  |> summary()
lm(data = descuento, ventas ~ renta*mujer) |> summary()

ggplot(descuento, aes(x=renta, y=ventas, color = mujer)) +
  geom_point() + geom_smooth(method = "lm", formula = y ~ x,
                               se = FALSE)
```

::::{.notes}

- Cuando interactuamos un regresor continuo y uno binario, permitimos que la pendiente del primero sea diferente para cada grupo

- Las interacciones son clave para capturar heterogeneidad de efectos: distintos impactos según género, edad, región, etc.


```{r}
lm(data=descuento , ventas ~ (renta + zona)*descuento) |> summary()
```

- La interacción de dos variables binarias tiene una interpretación similar, para el efecto esperado de $\small Y$

::::


::::{.notes}
**¿Cómo elegir entre modelos alternativos?**

```{r}
#| echo: true
#| eval: false
#| results: markup
library(modelsummary)
modelos <- list(
  "Lineal" = lm(ventas ~ renta + descuento, data = descuento),
  "Logarítmico" = lm(ventas ~ log(renta) + descuento, data = descuento),
  "Cuadrático" = lm(ventas ~ renta + I(renta^2) + descuento, data = descuento),
  "Con interacción" = lm(ventas ~ renta * mujer + descuento, data = descuento)
)

modelsummary(modelos,
             gof_map = c("nobs", "r.squared", "adj.r.squared", "rmse"),
             stars = TRUE)
```

1. **$R^2$ ajustado**: Penaliza complejidad
   $$\bar{R}^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}$$

2. **AIC, BIC**: Criterios de información que balancean ajuste y parsimonia

3. **Significatividad conjunta**: Test $F$ para grupos de variables

4. **Validación cruzada**: Error predictivo en datos no vistos (siguiente sección)

**Advertencia**: NO usar solo significatividad individual cuando hay múltiples términos de una variable (ej. polinomios)

La significatividad individual puede ser engañosa con polinomios o discretización debido a multicolinealidad.

::::

## Regresión Lineal: comentarios finales

- Los resultados de (todos) estos modelos informan de la relación entre los regresores y la variables dependiente

* **"Correlación no implica causalidad"**, salvo en [ensayos científicos aleatorios](https://es.wikipedia.org/wiki/Prueba_controlada_aleatorizada) controlados, también llamados [pruebas A/B](https://es.wikipedia.org/wiki/Prueba_A/B)

. . .


* La selección de variables es importante: evita sesgos o aumenta la variabilidad

```{r}
lm(data = descuento, ventas ~ descuento) |> summary()
lm(data = descuento, ventas ~ descuento + renta) |> summary()
lm(data = descuento, ventas ~ renta) |> summary()
lm(data = descuento, ventas ~ renta + educ) |> summary()
```

. . .

* Algunos sugieren "pruebas" para los problemas en el modelo lineal; eso es erróneo:

  * No linealidad: hemos visto que modeliza relaciones no lineales

  * No normalidad: innecesaria, con TCL o bootstrap.

  * Colinearidad: simplemente elimina la variable colineal.

  * Heterocedasticidad y autocorrelación: solo necesitamos errores estándar robustos

::::{.notes}

  * Outliers: pueden manejarse sin afectar la validez del modelo.

* Algunos mencionan "pruebas" para detectar los supuestos problemas del modelo lineal: no linealidad, heterocedasticidad (y autocorrelación), no normalidad, colinearidad, outliers, ...

* Este enfoque es mayormente erróneo como hemos visto: el modelo lineal permite no linealidades, no necesita normalidad (TCL, boostrap), eliminar la variable colineal soluciona el "problema", solo necesitamos usar errores estándar robustos, etc.

::::

## Regresión Logística

::::{.notes}
* La regresión lineal puede usarse respuestas binarias (no más de dos categorías),

$$
\small
\Pr(Y=1|X)=\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k = z
$$
aunque genera predicciones fuera del rango $\small [0,1]$

* Solución: aplicar al índice lineal una transformación $\small F(z)\in[0,1]$
::::

- Un modelo de regresión lineal para respuestas binarias no es adecuado porque genera predicciones fuera del rango  $\small [0,1]$


:::: {.columns}

::: {.column width="50%"}

* Solución: aplicar al índice lineal una transformación, la función logística $\small \Lambda (z)=\frac{e^z}{1+e^z}$:


:::

::: {.column width="50%"}
```{r}
#| echo: false
#| eval: true
#| fig.show: 'asis'
#| fig.height: 3
#install.packages("latex2exp")
library(tidyverse)
library(latex2exp)
Logistic <- function(x) {exp(x)/(1+exp(x))}
ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
    stat_function(fun = Logistic) + scale_x_continuous(limits = c(-5, 5), name = "z") +
  scale_y_continuous(name = TeX("$\\Lambda(z)$")) #+
  annotate("text", x = -3 , y = 0.7, label = TeX("$\\Lambda(z)=\\frac{exp(z)}{1+exp(z)}$", output='character'), parse=TRUE)
```

:::

::::


* De manera que $\small \Pr(Y=1|X)= f(x)= \color{blue}{\Lambda( \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)} \in [0,1]$

. . .

::::{.notes}
* Los coeficientes NO se interpretan como cambios en la probabilidad ante cambios unitarios en un regresor (efecto marginal sobre la probabilidad)

* PERO su signo (y significatividad) son los mismos que los del efecto marginal
::::

* Como el modelo siempre es no lineal, el coeficiente NO coincide con la magnitud del efecto de un cambio en los regresores sobre la probabilidad. PERO sí coinciden en su signo (y significatividad)


::::{.notes}
* En esta especificación, la probabilidad relativa ("odd") es
$$
\small
\frac{p(x)}{1-p(x)}=exp(\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)
$$

* Por tanto, su logaritmo ("log odd" o logit) es lineal: los coeficientes son la elasticidad de la probabilidad relativa

::::

* NO se puede minimizar la SCR; el objetivo es maximizar la probabilidad (verosimilitud) de observar los unos y ceros en los datos

::::{.notes}
$$
\small
\ell(\beta_0, \beta_1, \dots, \beta_k)=\prod_{i:y_i=1}p(x_i) \prod_{i:y_i=0} \left(1 - p(x_i)\right)
$$
::::

## Regresión Logística (cont.)

* La regresión logística se puede estimar en R con la función `glm()` (similar a `lm()`); podemos incluir variables explicativas tanto cuantitativas como cualitativas, interacciones, etc.

```{r}
#| echo: true
#| eval: false
censo <- read_csv("data/census.csv") |>
  mutate(across(where(is.character), ~parse_factor(.x)))

logit <- glm(data = censo, income ~  race + age + I(age^2) +
               log(hours_per_week)*sex +  education_1  +
               capital_gain + capital_loss, family = "binomial")
logit |>  summary()
```

* Como en el modelo lineal, podemos calcular predicciones de la probabilidad tanto en los mismos datos como en unos nuevos

```{r}
predict(logit, type="response")

logit2 <- logit <- glm(data = censo, income ~  age + education_1,
         family = "binomial")
summary(logit2)
predict(logit2, type="response",
        newdata = tibble(age=c(20,60), education_1=c(9,16)))
```

::::{.notes}
* Podemos incurrir en un sesgo por omisión de variables relevantes: ej., el efecto de `student` por omitir `balances` (con la que está correlacionada)
```{r}
glm(data = Default, default ~ student + balance, family = "binomial" ) |> summary()
```
::::

## Regresión Logística con más de dos clases


* La regresión logística se puede generalizar a situaciones con múltiples clases (modelos multinomiales) con un índice lineal para cada clase
$$
\small
\Pr(Y=c|X)=\frac{e^{\beta_{0c}+\beta_{1c}X_1+\dots+\beta_{kc}X_k}}{\sum_{l=1}^{C}e^{\beta_{0l}+\beta_{1l}X_1+\dots+\beta_{kl}X_k}}
$$

* La librería `glmnet()` permite la estimación de estos modelos

```{r}
health <- read_csv("data/health_insurance.csv") |>
  mutate(across(where(is.character), ~parse_factor(.x)))

library(glmnet)
x <- model.matrix(product ~ age + gender, data = health)
mod.glmnet <- glmnet(x = x, y = health$product, family = "multinomial",
                     lambda = 0, type.multinomial = "grouped")
coef(mod.glmnet)
predict(mod.glmnet, newx=x, type = "response")  # probabilidad de cada clase
predict(mod.glmnet, newx=x, type = "class")     # clase
```

```{r}
#| echo: false
#| eval: false

library(glmnet)
iris.x <- as.matrix(iris[1:2])
iris.y <- as.matrix(iris[5])
mod.glmnet <- glmnet(x = iris.x, y = iris.y, family = "multinomial",
                     lambda = 0, type.multinomial = "grouped")
coef(mod.glmnet)

predict(mod.glmnet, newx=iris.x, type = "response")  # probabilidad de cada clase
predict(mod.glmnet, newx=iris.x, type = "class")     # clase

d <- coef(mod.glmnet) |> reduce(cbind)
colnames(d) <- names(c)
d
```



```{r}
#| echo: false
#| eval: false

library(nnet)
mod.nnet <- multinom(
    Species ~ Sepal.Width + Petal.Length + Petal.Width, # Species ~ .
    data = iris
)
mod.nnet
```

# Evaluación de modelos (por su capacidad predictiva)

## Métricas de Error de Predicción (cuantitativa)

::: {style="font-size: 95%;"}

- Un modelo es mejor cuanto mejor se ajusten sus predicciones a las observaciones

- El error de predicción es $y - \widehat{y} = f(X) - \widehat{f}(X)  + \varepsilon$

  + $f - \widehat{f}$ = error reducible (eligiendo modelo)

  + $\varepsilon$ = error irreducible (variables no observadas)

```{r figures-side}
#| echo: false
#| eval: false
#| layout-ncol: 2
# #| fig.show: hold

#
# #| out.width="50%"
# #| fig.height=4
library(latex2exp)
curve(x^2, from = -2, to = 2, xlab = TeX("$y-\\hat{y}$"), ylab = "Coste")
curve(abs(x), add = TRUE, col = "blue")
legend('top',legend = c("x^2", "abs(x)"), text.col = c("black","blue"))

curve(0 * (x>=0) - x * (x<0), from = -2, to = 2, xlab = TeX("$y-\\hat{y}$"), ylab = "Coste", col = "red")
curve(0.25 * (x> 0 & x<0.5) + 0.75 * (x>0.5) + 0.75 * (x>1.5) + 0.25 * (x < -1), add = TRUE, col = "black")
```

:::

. . . 

::: {style="font-size: 95%;"}
- Una **función de pérdida (o coste)** evalúa cómo valoramos las desviaciones

  1. **Mean Square Error** (Error Cuadrático Medio): $\small \color{blue}{MSE(y,\widehat{y})={\frac{1}{n}\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}}$
  
      + penaliza grandes desviaciones
  
      + $\small \color{blue}{R^2}$ y $\small \color{blue}{R^2-ajustado}$: variantes solo para comparar modelos con la *misma variable dependiente*.
    
  2. **Root Mean Square Error**: $\small  \color{blue}{RMSE(y,\widehat{y})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}}$
  
      + mismas unidades que $\small y$
  
  3. **Mean Absolute Error**: $\small  \color{blue}{MAE(y,\widehat{y})=\frac{1}{n}\sum_{i=1}^{n}\left|y-\widehat{y}\right|}$

  * Otras medidas basadas en distintas funciones de pérdida: verosimilitud del modelo, $\small \color{blue}{AIC}$, $\small \color{blue}{BIC}$, etc
  
::::{.notes}

+ $R^2$-ajustado penaliza por número de variables

+ también median ABs. error

* *Correlación* lineal o de rangos entre $\small y$ y $\small \widehat{y}$
  
        + lineal ($y$ y $\widehat{y}$ pueden no tener las mismas unidades y escala como con RMSE y MAE)
        + de rangos ($y$ y $\widehat{y}$ solo tiene que tener el mismo orden relativo, no minimizar distancia entre ellas)
  
* *Coeficiente de determinación*
* AIC, BIC ajustan por el número de parámetros 
  
https://yardstick.tidymodels.org/reference/rmse.html

::::  

:::


## Seleccionar el mejor modelo. "Overfitting"

::: {style="font-size: 92%;"}

- ¿Cuál es el mejor modelo para predecir el número de visitantes en función de la temperatura?

```{r}
library(mosaicData)
RailTrail |>  ggplot(aes(x = hightemp, y = volume)) + geom_point() +  geom_smooth()
```

$$
\scriptsize
\begin{align*}
volume &= \beta_0 + \beta_1 \, \mathrm{hightemp} + \varepsilon \\
volume &= \beta_0 + \beta_1 \, \mathrm{hightemp} + \beta_2 \, \mathrm{hightemp}^2 + \varepsilon \\
&\cdots \\
volume &= \beta_0 + \beta_1 \, \mathrm{hightemp} + \dots + \beta_{22} \, \mathrm{hightemp}^{22} + \varepsilon
\end{align*}
$$

- El objetivo de un modelo es **predecir casos nuevos** (no reproducir los datos vistos)

- Las métricas ($\small MSE$, $R^2$) calculadas en los mismos *datos usados para estimar* NO informan de esto: miden *in-sample prediction*, no *out-sample prediction*

::::{.notes}
- Las métricas de error ($\small MSE$, $R^2$) calculadas para predicciones de los mismos *datos usados para estimar* el modelo NO informan de esto

::::


- Un modelo demasiado flexible puede *aprender el ruido* (características de la muestra concreta) → **sobreajuste (overfitting)**: error casi nulo en los datos que estimamos, pero grande con casos nuevos 

```{r}
RailTrail |> ggplot(aes(x = hightemp, y = volume)) + ylim(100,750) + 
  geom_point() + geom_smooth(method = 'lm', formula = y ~ poly(x,22) ) 
```

- Un buen modelo tiene equilibrio entre capacidad predictiva y complejidad: ni ajuste insuficiente (*underfitting*) ni *overfitting*

:::


## Muestras de entrenamiento y de prueba

::: {style="font-size: 92%;"}

- Dividimos la muestra para usar datos diferentes para estimar y para calcular las métricas 

  1. **Muestra de entrenamiento** (*train*) donde se ajusta el modelo
  
  2. **Muestra de prueba** (*test*) donde se calculan métricas con observaciones NO vistas antes por el modelo  → evaluamos el mejor modelo para predecir casos nuevos

::::{.notes}

![](figure/train_test.png){width=70% fig-align="center"}

::::

::: 

. . .

::: {style="font-size: 92%;"}

- Cuando aumenta la flexibilidad, el MSE siempre disminuye en la muestra de entrenamiento...

- PERO el MSE tiene forma de U en la muestra de prueba → modelo óptimo evita *sobre-ajuste* (modelo menos flexible con menor error de predicción para casos nuevos)

:::: {.columns}
::: {.column width="70%"}
![](figure/overfitting01c.png){width=80% fig-align="center"}
:::
::: {.column width="0.5%"}
$\scriptsize MSE^{train} < MSE^{test}$
:::
::::


::::{.notes}

- sobre-ajuste: parecen que su error es casi nulo en los datos que estimamos, pero cometen un gran error en observaciones nuevas 


- Mostrar ejemplos gráficos de ajuste insuficiente, correcto y sobreajuste.  

![](figure/overfitting2n.png){width=100% fig-align="center"}

![](figure/overfitting3n.png){width=100% fig-align="center"}

* Los grados de libertad (número de valores en el modelo que son libres de variar) resume la *flexibilidad* de una curva.

::::

::: 

## "Trade-off" Varianza--Sesgo

::::{.notes}

**MSE en la muestra de prueba**

$$
\small
E\left[\left(y-\widehat{f}(x)\right)^2\right] =
E\left[\left(f(x)-\widehat{f}(x) + \varepsilon
+ E\left[\widehat{f}(x)\right]-E\left[\widehat{f}(x)\right] \right)^2\right] =
$$

$$
\small
=\underbrace{\left[E\left(\widehat{f}(x)\right)-f(x)\right]^2}_{(1)} + \underbrace{E\left(\left[\widehat{f}(x)-E\left(\widehat{f}(x)\right)\right]^2\right)}_{(2)}+Var(\varepsilon)
$$


* $\small (1)=\left[Sesgo\left(\widehat{f}(x)\right)\right]^2$: error por supuestos erróneos en $f$

    + ajuste insuficiente (*underfit*) al perder relaciones relevantes entre $X$ e $Y$

* $\small (2)=Var\left(\widehat{f}(x)\right)$: sensibilidad a fluctuaciones en el  entrenamiento
    + si el algoritmo modela puro ruido en entrenamiento, ajustará bien allí, pero predecirá mal casos nuevos (*overfit*)
::::


::: {style="font-size: 95%;"}

::::{.notes}
- $\small MSE$ en la muestra de prueba es la suma de sesgo y varianza de la estimación:

::::

$$
\small
\text{Error Total}  =  \text{Sesgo}^2 + \text{Varianza} + \text{Error Irreducible}
$$

::: {.columns}

::: {.column width="50%"}

![](figure/biasvariance-tradeoff2.png){width=65% fig-align="center"}

:::


::: {.column width="50%"}

- **Sesgo**: diferencia entre predicción promedio y valor real por especificar mal $f$ 

    - modelos simples (*underfit*) que pierden relaciones relevantes (omitir variables, forma funcional incorrecta)
  
- **Varianza**: cuánto cambian las predicciones con diferentes datos

    - modelos complejos, *sobreajustados* a características de cada muestra de entrenamiento


- Al aumentar la complejidad ↓ sesgo pero ↑ varianza ⇒ encontrar una flexibilidad intermedia <!-- que equilibre ambos-->

- No pueden minimizarse simultáneamente ambas fuentes de error:  *memorización* (en entrenamiento) vs. *generalización* de resultados

:::

::::

:::

::::{.notes}

- Recuerdo de Econometría I:
1. sesgo por omisión de variables relevantes
2. aumento de varianza por inclusión de variable irrelevante

- A medida que se añaden más y más parámetros a un modelo, la complejidad del modelo aumenta y la varianza se convierte en nuestra principal preocupación, mientras que el sesgo disminuye constantemente. Por ejemplo, a medida que se añaden más términos polinómicos a una regresión lineal, mayor será la complejidad del modelo resultante.

![](figure/biasvariance-tradeoff2.png){width=22% fig-align="center"}


* Es fácil construir un modelo con bajo sesgo, pero tendrá alta varianza. Y al revés.

* El desafío es encontrar un método (ej., flexibilidad del modelo) para el cual tanto la varianza como el sesgo cuadrado sean bajos

::::

## Métricas de Error en la Clasificación

::::{.notes}
* Los modelos de clasificación NO predicen directamente la categoría, sino la *probabilidad* de que una observación pertenezca a cada categoría


* Típicamente se asigna la clase predicha como aquella con mayor probabilidad.

* En el caso binario, equivale a fijar un umbral de 0.5, pero se deberían probar varios valores del umbral

* Como no tiene sentido diferencia de clases (variables categóricas), NO se pueden calcular medidas como el MSE y otros relacionados


* Existen pseudo-$\small R^2$ como la correlación al cuadrado entre

::::

- El modelo predice la *probabilidad* de que una observación pertenezca a cada categoría ⇒ se asigna como clase predicha aquella con mayor probabilidad

  - en el caso binario, implica superar el umbral de 0.5 (pero deben probarse varios valores)


```{r}
censo <- read_csv("data/census.csv") |> 
  mutate(income = parse_factor(income))
logit <- glm(data = censo, income ~ capital_gain, 
             family = "binomial")
prob_predict <- predict(logit, type = "response")

umbral <- 0.5
cat_predict  <- if_else(prob_predict > umbral, 1, 0)
cbind(censo$income, cat_predict, prob_predict) |> head(10)
```

- NO tiene sentido diferencia de clases  ⇒  no se puede calcular MSE y similares

- **Matriz de Confusión**: tabular categorías observadas frente a las categorías predichas


```{r}
table(cat_predict, censo$income)
```


::::{.notes}
|  | **CLASE OBSERVADA** || 
|:---:|:---:|:---:|
| **CLASE PREDICHA ↓** | **POSITIVO (1)** | **NEGATIVO (0)** |
| **POSITIVO (1)** | Verdadero Positivo **[VP]** | Falso Positivo **[FP]** <br>(Error Tipo I) |
| **NEGATIVO (0)** | Falso Negativo **[FN]** <br>(Error Tipo II) | Verdadero Negativo **[VN]** |
::::

## Métricas basadas en la matriz de confusión

* Existen varias medidas derivadas de la [matriz de confusión](https://en.wikipedia.org/wiki/Confusion_matrix)

```{r}
#| echo: false
#| eval: true
#| results: 'asis'
library(tibble)
library(kableExtra)

tab_conf <- tibble(
  `CLASE PREDICHA ↓` = c("POSITIVO (1)", "NEGATIVO (0)"),
  `POSITIVO (1)`     = c("Verdadero Positivo [VP]",
                         "Falso Negativo [FN]<br>(Error Tipo II)"),
  `NEGATIVO (0)`     = c("Falso Positivo [FP]<br>(Error Tipo I)",
                         "Verdadero Negativo [VN]")
)

tab_conf |> 
  kable(
    format = "html",
    escape = FALSE,              # para que <br> funcione
    align  = c("c", "c", "c")
  ) |> 
  add_header_above(c(" " = 1, "CLASE OBSERVADA" = 2)) |> 
  kable_classic(full_width = FALSE)

```



* **Tasa de observaciones correctamente clasificadas** (exactitud o *accuracy*)

$$
\scriptsize \color{blue}{ACCUR=\frac{VP+VN}{VP+FP+VN+FN}}
$$


* No es informativo con clases infrecuentes (datos  desequilibrados): con solo 1% de fraude, predecir que nunca hay fraude implica $\scriptsize ACCUR=99\%$, PERO NO detecta fraude

::::{.notes}

*  ACCUR = 1 - TCE

* Su complemento es la *tasa de clasificación errónea* o de error en la clasificación: 
$\small TCE=\frac{FP+FN}{VP+FP+VN+FN} = \frac{1}{n}\sum_{i=1}^{n}I\left[y_i \neq \widehat{y}_i\right]$


* (datos  desequilibrados o *imbalanced* )

* Una medida global para datos *imbalanced* es la *exactitud equilibrada*: $\small \frac{TVP+TVN}{2}$

https://en.wikipedia.org/wiki/Cohen's_kappa

::::


* El **estadístico Kappa** ($\small \kappa$) es una medida similar, pero ajusta por el desequilibrio entre clases

::::{.notes}
que ajusta por lo se esperaría solo por azar (corrigiendo en parte el desequilibrio entre clases).
::::


## Métricas basadas en la matriz de confusión (cont.)

* **Tasa de verdaderos positivos** o **sensibilidad** (*recall*) 
$$
\scriptsize \color{blue}{TVP=SENSIT=\frac{VP}{VP+FN}}
$$


* **Tasa de verdaderos negativos** o **especificidad** 
$$
\scriptsize \color{blue}{TVN=ESPECIF=\frac{VN}{VN+FP}}
$$

    - **Tasa de falsos positivos**: $\scriptsize \color{blue}{TFP = 1 - TVN = 1 - ESPECIF}$

::::{.notes}

-TVP: es el porcentaje de verdaderos positivos sobre el total de positivos observados
   ej., tasa de fraude/enfermos existentes que se detectan correctamente

- TVN: es el porcentaje de verdaderos negativos sobre el total de negativos observados

  - ej: ., tasa de "otras" opciones que se clasifican correctamente


- Ejemplo: prueba diagnóstica, sensibilidad cuantos enfermos es capaz de detectar

      especificifciad, cuantos no enfermos es capaz de detectar corre
  
::::


* **Exactitud equilibrada**: (*Balanced Accuracy*) media <!--(aritmética o geométrica)--> de la sensibilidad y de la especificidad

* **Precisión**: $\scriptsize PREC=\frac{VP}{VP+FP}$

- La familia de **medidas $\small F_{\beta}$**: ratio de la importancia ponderada de la sensibilidad y de la precisión

::::{.notes}
- (*Balanced Accuracy*) es una media (aritmética o geométrica) de la sensibilidad y de la especificidad

  -  G-mean = sqrt(sensit * especif )

* La **precisión** o valor de predicción positivo es la cantidad de verdaderos positivos sobre el total de positivos predichos

$$
\scriptsize PREC=\frac{VP}{VP+FP}
$$

    + Tasa de falso descubrimiento: $\small 1-PREC$



* La familia de **medidas $\small F_{\beta}$** es una ratio de la importancia ponderada de la sensibilidad y de la precisión: $\scriptsize F_{\beta}=\frac{(1+\beta)^2 \times SENSIT \times PREC}{\beta^2 \times SENSIT + PREC}$

  + Para $\scriptsize \beta<1$<!-- ($\scriptsize >1$)-->, se da menos <!--(más)--> importancia a la sensibilidad: los falsos positivos <!--(negativos)--> se consideran más costosos

  + Para $\scriptsize \beta>1$, los falsos negativos son más costosos y para $\scriptsize \beta=1$ son igualmente costosos
  
::::


## Curva ROC ("Receiver Operating Characteristic")

::::{.notes}

https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc


* (es una curva de probabilidad)

* Reducir el umbral clasifica más elementos como positivos, por lo que aumentan tanto los falsos positivos como los verdaderos positivos.

:::: {.columns}

::: {.column width="45%"}

![](figure/ROC-AUC.svg){width=65% fig-align="center"}

:::

::: {.column width="45%"}


![](figure/ROC.png){width=75% fig-align="center"}

:::

::::

* clasificador aleatorio:  por debajo de 45º el clasificador es pesimo predice más positivos entre los negativos que entre los positivos

* Con datos *imbalanced*  puede ser más informativo graficar TFP frente a precisión


![](figure/ROCCurve.svg){width=40% fig-align="center"}
![](figure/AUC.svg){width=40% fig-align="center"}

https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5


::::

* Representa TVP (eje y) frente a TFP (eje x) en *diferentes umbrales* <!-- de clasificación*-->: reducir el umbral clasifica más elementos como positivos (verdaderos y falsos)


![](figure/ROC_AUC_combined.png){width=85% fig-align="center"}

* La curva ROC informa del grado de separabilidad: dado un nivel de TFP, el clasificador es mejor cuanto mayor sea TVP


## AUC ("area under the curve")

::::{.notes}

* AUC informa del grado de separabilidad: mayor AUC implica que el modelo es capaz de distinguir entre clases (predecir 0s y 1s correctamente)

* AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.

::::

* La AUC es el área bajo la curva ROC: ofrece una medida agregada de rendimiento entre 0 (todas las clasificaciones incorrectas) y 1 (todas correctas)


* Resume la curva ROC y permite comparar curvas que se cruzan

![](figure/ROC_intersect.png){width=55% fig-align="center"}


## Extensiones. Métricas adicionales


* Con más de dos clases, se realiza un análisis AUC-ROC para cada categoría (frente a las demás) y se promedian (ej., ponderando por casos en cada clase) 

::::{.notes}

* Con más de dos clases, se realiza un análisis AUC-ROC para cada categoría (frente a las demás): se define una variable binaria para la categoría frente a todas las demás y se promedian la ROC y la AUC (ej., ponderando por casos en cada clase) o no



https://yardstick.tidymodels.org/reference/roc_aunp.html

https://yardstick.tidymodels.org/reference/roc_aunu.html

* Cuando la variable de respuesta tiene más de dos clases,

    1. Se realiza un análisis AUC-ROC para cada categoría: se define una variable binaria para la categoría frente a todas las demás

    2. Se obtiene el promedio de tanto de la ROC como de la AUC, bien dando igual peso a cada categoría o bien ponderando el número de casos de cada una

https://yardstick.tidymodels.org/reference/pr_auc.html

https://yardstick.tidymodels.org/reference/average_precision.html


+ gain curve: https://yardstick.tidymodels.org/reference/gain_curve.html

+ mn_log_loss: https://yardstick.tidymodels.org/reference/mn_log_loss.html

::::

* Con clases desequilibradas, se puede preferir en lugar de la ROC un gráfico de precisión frente sensibilidad (*precision-recall*) y su correspondiente AUC (*PR-AUC*)


* Existen múltiples funciones de pérdida (o coste de clasificación) posibles.

  + Las relacionadas con la *curva de ganancia* consideran el coste de alcanzar un cierto nivel de sensibilidad

  + Otras se basan en la función de verosimilud o la entropía como medidas de pérdida (ej. *mean log loss*)


## Evaluación de Modelos: entrenamiento y prueba


* Para minimizar problemas de *underfit* y, sobre todo, de *overfit*, DEBEMOS **dividir aleatoriamente** el conjunto de datos en dos partes:

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px; "}

::: {}

![](figure/train_test_split2.png){width=95% fig-align="center"}

:::

::: {}

* **Entrenamiento** (80-90%): datos sobre los que se construye/estima el modelo

* **Prueba**(20-10%): se usa el modelo construido para predecir y se evalúa con datos no vistos antes

:::

::::

* ¿Por qué renunciar a parte de los datos si sabemos que un tamaño muestral grande es importante? Evaluar correctamente un modelo lo es mucho más

* La estimación del error en prueba puede ser volátil dependiendo de las observaciones incluidas en cada grupo

## Evaluación de Modelos: Validación cruzada


::::{.notes}

* Los resultados de evaluación puede verse afectados por la partición concreta obtenida (ej. incluir observaciones atípicas en la muestra de prueba)

* Se repite varias veces y de forma ordenada el proceso de remuestreo para la partición en grupos de entrenamiento y prueba (similar a *bootstrap*)


https://en.wikipedia.org/wiki/Cross-validation_(statistics)

![](figure/k-crossVal_split2.png){width=55% fig-align="center"}

::::

- La validación cruzada (*cross-validation* o *rotation estimation*) repite varias veces y de forma ordenada el proceso de partición en grupos de entrenamiento y prueba (similar a *bootstrap*) 


- Este método de evaluación de modelos:

  1. Evita que los resultados sean sensibles a una partición concreta de los datos
  
  2. Permite utilizar todas las observaciones de la muestra, tanto para estimar como para evaluar el modelo (aunque no a la vez)

. . .

* Algunas variantes de Validación Cruzada (VC) son:

    - VC de K iteraciones (*K-fold cross-validation* o K-fold CV) (el más habitual)

    - VC aleatoria (*Random cross-validation*, RCV)

    - VC dejando uno fuera (*Leave-one-out cross-validation*, LOOCV)

    - VC dejando p fuera (*Leave-p-out cross-validation*, LpOCV)


## Validación cruzada de K bloques


* Se divide, aleatoriamente y *ex-ante*, la muestra en K subconjuntos (normalmente, 5 o 10)


:::: {.columns}

::: {.column width="60%"}

![](figure/K-fold_cross_validation.jpg){heigth=150% fig-align="center"}
:::

::: {.column width="40%"}
+ Un subconjunto se usa como prueba y el K-1 restantes como entrenamiento

+ Se repite el proceso durante k iteraciones, con cada posible subconjunto de datos de prueba.
:::

::::


+ Se obtiene una métrica de error en cada iteración

+ Se promedian para obtener un único resultado de evaluación

::::{.notes}

* Evita la sensibilidad a una partición concreta de los datos y permite usar todas las observaciones tanto para estimar como para evaluar el modelo


- De Joan.domenech91 - Trabajo propio, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=17616792 

**Validación cruzada aleatoria (RCV) y LOOCV**

:::: {.columns}

::: {.column width="60%"}

![](figure/Random_cross_validation.jpg){width=100% fig-align="center"}
:::

::: {.column width="40%"}
+ **RCV**: en *cada iteración* se realiza la particion aleatoria (con reemplazamiento) entre entrenamiento y prueba

+ Las observaciones pueden "repetir" como prueba
:::

::::

:::: {.columns}

::: {.column width="60%"}

![](figure/Leave-one-out.jpg){width=100% fig-align="center"}

:::

::: {.column width="40%"}
+ **LOOCV** (*leave one out CV*): solo una observación se usa como prueba en cada iteración y el resto como entrenamiento

+ Se realizan $n$ iteraciones; se calcula una media sobre $n$ resultados
:::

::::

::::