---
# subtitle: "Técnicas para 'Big Data' en Economía - Curso 2023/24 \n\n Universidad de Alicante"
title    :  "Tema 08 - Modelización con `tidymodels`"
author:  
    - "Pedro Albarrán"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:  
        - beige # sky # serif # simple # default # moon #  
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide # concave # 
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    # width: 3000
    # height: 2000
    # margin: 0.05
execute:
  enabled: true      # (no) ejecutar code chunks
  eval: false        # por defecto, evalúa y muestra códido de code chunks
  echo: true
  warning: false    # pero no los mensajes ni warnings
  message: false
knitr:
  opts_chunk:
    results: hide     # ni muestra resultados ni figuras
    fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---


```{r setup, include=FALSE}
#```{r setup, message=FALSE, warning=FALSE, include=FALSE} 
# include=F es suficiente para no incluir mensajes, etc.

# Opciones por defecto para los fragmentos de código
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      results = "hide", fig.show="hide")
# se muestra y evalúa el código,
# no se muestran mensajes, ni avisos (warnings)
# no se muestran los resultados de código (tampoco gráficos)
#     en los códigos que considere necesarios los mostraré

# Elimino todo del Entorno (del documento)
rm(list = ls())       

# Cargo todas las bibliotecas necesarias
# (se podría hacer cuando cada una sea necesaria)
library(tidyverse)
# library(tidymodels)
# library(printr)
# library(skimr)
# library(dlookr)
# library(broom)
library(kableExtra)
# library(rpart.plot)
# library(vip)

#fijo el directorio de trabajo 
#setwd("/home/albarran/Dropbox/MAD/00.TEC")
library(rmarkdown)
#render("filename.Rmd")     
#browseURL("filename.html")
```

# Introducción

## Proceso de modelización

* `tidymodels` es una colección de paquetes para el **proceso de modelización** (NO implementa modelos) <!--en aprendizaje automático--> con los principios de `tidyverse`

<!--
<https://www.tidymodels.org/start/>
<https://www.tmwr.org/resampling.html#resampling-performance>
-->

```{r}
library(tidymodels)
```

<center>
![](figure/tidymodels_process.png){width=90%}
</center>



<!-- * Las acciones del proceso <!--(preparación de datos, entrenamiento del modelo, validación, ...)--> <!--no se ejecutan directamente: primero se define cada paso y se ejecutan todos al final -->

<!--
* `workflows`: combinar todos los pasos anteriores en un único objeto
-->

* Otros paquetes "similares": `mlr3`, `caret`, `H2O`

* `tidymodels` se llama `vetiver` en python

# Pre-procesado

## Partición inicial

* NO usamos todos los datos para estimar

* `initial_split()`: particionar los datos en prueba y entrenamiento.

    + primer argumento: datos (se pasa por tubería ` %>%`)
    
    + segundo argumento: proporción de datos dedicados a entrenamiento

<!-- Ver el objeto -->

<!-- Ej. strata -->

* `tidymodels` crea objetos (listas) con toda la información 
     y ofrece funciones para acceder a ella

* Las funciones `training()` y `testing()` acceden a cada submuestra


## Recetas de transformación

* Aunque ya hemos limpiado los datos de forma general, podemos considerar **transformaciones** específicas para algunas estimaciones

    1. Algunos modelos necesitan que las variables sean transformadas de una forma concreta (ej., estandarizar)
        
    2. Incluso para un mismo modelo, podemos considerar diferentes especificaciones diferentes: sin usar `NA` o imputándolos, usando distintas variables (ej., con o sin interacciones)

. . .

* Las recetas, `recipe()`, definen las transformaciones a aplicar:

  + Su principal argumento es una *fórmula*, que define el rol de las variables: variable dependiente y predictores

  + Se encadenan (con `%>%`) *pasos* con  `step_*` para las transformaciones

* Una referencia completa [aquí](https://recipes.tidymodels.org/reference/index.html) y [aquí](https://www.tidymodels.org/find/recipes/) 

<!-- ej. recipes::Roles -->

## Algunos pasos habituales

* Para un *modelo concreto*, podemos realizar cambios (como en `tidyverse`): `step_select()`, `step_filter()`, `step_mutate()`, ...

    * También transformaciones directas: `step_log()` 

<!--

  * `step_sqrt()`, `step_boxcox()`, ... 

  * `step_rm()`, `step_arrange()` 
  
-->


* ¿Cómo tratar los valores ausentes en *este modelo*?

  - elimina las observaciones con valores ausentes: `step_naomit()`

  - imputar los valores ausentes de los *predictores*: `step_impute_mean()`, `step_impute_linear()`, `step_impute_knn()`, etc..
  
  <!-- 
  
  * imputar media condicional mejor que incondicional: individuos son similares características 
  
  * Nota: imputar la variable dependiente no tiene mucho sentido  
  
   `step_impute_median()`, `step_impute_bag()`, `step_impute_lower()`,  `step_impute_mode()`, `step_impute_roll()`
   
   -->

. . .

* Estandarizar variables para que tengan media cero, `step_center()`, varianza uno, `step_scale()`, o ambas `step_normalize()`

<!--    
    +  `step_standardize()` 
-->

* Crear polinomios, `step_poly()`, y discretizar<!-- variables continuas-->, `step_cut()`

  <!-- `step_discretize()` -->

<!--
# https://www.tmwr.org/pre-proc-table
# 
# https://recipes.tidymodels.org/articles/Dummies.html
# 
-->


## Algunos pasos habituales (cont.)

* Para variables cualitativas, podemos agrupar categorías poco frecuentes: `step_other()`

* En general, `tidymodels` exige crear explícitamente variables binarias (dummy) para las categorías de las variables cualitativas: `step_dummy()`

  * algunos modelos necesitan dummies para *todas* las categorías: se añade la opcion `one_hot = TRUE`

. . . 

* NOTA: algunas fuentes (ej., chatGPT) sugieren eliminar variables con poca variabilidad, `step_zv()` y `step_nzv()`, o con alta correlación, `step_corr()`. Esto **NO** es estrictamente necesario para ningún modelo

    + Esto lo debemos haber detectado en el AED.


## Algunos pasos habituales (y 3)

* Las interacciones también funcionan de forma diferente en `tidymodels`

* Para variables cuantitativas, *x1* y *x2*, `step_interact(~ x2*x1)` añade **solo** una nueva variable: la inteacción *x2:x1*
  
    + con `step_interact(~ (x2 + x3)*x1)` creamos múltiples interacciones

* Para variables binarias, `step_interact(~ d1:d2)` añade la interacción

* Para variables cualitativas, hemos debido crear *antes* las dummies. Para evitar escribir manualmente cada combinación podemos usar `start_with()`:

    + `step_interact(~ starts_with("F1"):x1)` para una variable categórica *F1* y una continua *x1*

    + `step_interact(~ starts_with("F1_"):starts_with("F2_"))` para dos variables categóricas *F1* y *F2*

## Preparando la receta

::: {style="font-size: 90%;"}

* **TODOS** los pasos se pueden aplicar a una variable o varias con `all_outcomes()`, `all_predictors()`, `all_numeric()`, `all_nominal()`, `contains()`, etc.

    <!-- + `step_dummy(all_predictors(), -all_numeric())` -->

<!--
* También hay pasos de control como `check_missing()`
-->


* Se crea un objeto de receta para usarlo después con otras partes del proceso

```{r}
#| eval: false
library(mosaicData)
library(tidyverse)
set.seed(101)
railtrailPart <- RailTrail %>% 
                    mutate(dayType = parse_factor(dayType)) %>% 
                    initial_split(prop = .8)
receta1 <-railtrailPart %>% training() %>%             
  recipe(volume ~ cloudcover + precip + hightemp + dayType) %>%
  step_normalize(all_predictors(), -all_nominal()) %>%
# step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_dummy(dayType) %>% 
  step_poly(hightemp, degree = 6)  
```


<!--

dummies no se normalizan
la normalización debe ser ANTES de la creación de polinomios, logaritmos o interacciones

-->

```{r}
#| echo: false
#| eval: false
receta2 <- training(RailTrail_part) %>%            # Datos: NO crucial
  recipe(volume ~ cloudcover + precip + avgtemp + dayType) %>%
  step_poly(avgtemp, degree = tune("lambda")) %>%                              # tuning
  step_corr(all_predictors(), -all_nominal(), threshold = 0.9) %>%
  step_center(all_predictors(), -all_nominal()) %>%
  step_scale(all_numeric(), -all_outcomes(), -all_nominal()) %>% 
  step_dummy(dayType)
```

* Podría `prep()`ararse y aplicar las transformaciones con `juice()` o `bake()` (veremos otra alternativa más general)

<!-- los datos de la receta NO son importantes: se definirán al usarla luego -->

<!--

```{r}
#| echo: false
#| eval: false
receta1_prep <- receta1 %>%  prep()
re1_entrena <- receta1_prep %>% juice()                             # extraer
re1_prueba  <- receta1_prep %>% bake(railtrailPart %>% testing())   # aplicar a unos nuevos

re1_prueba %>% map(mean)   # media cero y varianza uno
re1_prueba %>% map(sd)
```
-->

:::

# Entrenamiento del modelo

## Definir el modelo


<!-- cada motor puede llamar parámetros de forma distinta-->

* `tidymodels` define un modelo de un tipo con una interfaz unificada para distintas bibliotecas <!--(con otros nombres de argumentos)-->

1. Una función que define el tipo de modelo, con argumentos específicos de ese modelo (la lista de funciones/modelos [aquí](https://www.tidymodels.org/find/parsnip/))

2. Se fija el tipo de problema (*mode*): regresión o clasificación

3. Se establece la biblioteca (*engine*) con la que se implementará


```{r}
#| eval: false
modelo_lm1  <- linear_reg(mode= "regression", engine = "lm", 
                             penalty = 0)

modelo_glm1 <- linear_reg(mode= "regression", engine = "glmnet", 
                             penalty = 0)
```



```{r}
#| echo: false
#| eval: false
modelo_lm1     <- linear_reg(mode= "regression", penalty = 0) %>%
                    set_engine("lm") 

modelo_glmnet1 <- linear_reg(penalty = 0)  %>% set_mode("regression") %>% 
                        set_engine("glmnet")

modelo_logit1 <- logistic_reg(mode= "classification", penalty = 0) %>% set_engine("glm")
```


* Se puede estimar (entrenar) el modelo con `fit()`


## Flujos de trabajo: `workflow()`

* Un flujo de trabajo combina la receta de preprocesado y la definición del modelo en un único objeto para su uso posterior


```{r}
#| eval: false
flujo_lm1 <- workflow() %>%
  add_recipe(receta1) %>%      
  add_model(modelo_lm1)
```

```{r}
#| echo: false
#| eval: false
flujo_lm1 <- workflow() %>%
  add_recipe(receta1) %>%       # add_formula() si no procesamos los datos
  add_model(modelo_lm1)
```


* Podríamos modificar un flujo existente con `update_recipe()` , `update_model()`, etc.

<!--
```{r}
#| echo: false
#| eval: false
flujo_lm2 <- flujo_lm1 %>% 
                update_recipe( receta1 %>% 
                    step_log(volume, skip=TRUE)  )
```
-->

. . .

* Usando la función `fit()` con unos datos y un flujo de trabajo,

  - la receta aplica el preprocesado a los datos definidos aquí
  
  - se estima el modelo definido con los datos procesados

```{r}
#| eval: false
flujo_lm1_est <- flujo_lm1 %>% 
                   fit(data = railtrailPart %>% training()) 
```


## Flujo de trabajo estimado

* El objeto del flujo estimado almacena información previa (receta, modelo, datos), resultados (estimaciones, predicciones) para usar de varias formas

<!-- https://workflows.tidymodels.org/reference/extract-workflow.html -->

1. Extraer la receta y aplicar la transformación a unos datos 

<!-- (p.e., comprobamos que tienen varianza 1 con `var()`) -->
```{r}
#| eval: false
receta_extr <- flujo_lm1_est %>% extract_recipe() 

receta_extr %>% bake(railtrailPart %>% training()) 
receta_extr %>% bake(railtrailPart %>% testing())
```

```{r}
#| echo: false
#| eval: false
flujo_lm1_est %>% extract_recipe() %>% bake(RailTrail_part %>% training()) %>% map(sd)
flujo_lm1_est %>% extract_recipe() %>% bake(RailTrail_part %>% testing())
```

2. Extraer los resultados de la estimación; pueden convertirse a conjunto de datos (y mostrar en tablas con `kable()`)

```{r}
#| eval: false
estim_extraida <- flujo_lm1_est %>% extract_fit_parsnip() 

estim_extraida %>% tidy()      # resultados de la estimación
estim_extraida %>% glance()    # otros detalles de la estimación
```

<!--
* `broom::augment()` calcula predicciones del modelo, residuos, etc.

```{r}
#| echo: false
#| eval: false
modelo <- flujo_lm1_est %>% extract_fit_parsnip()
modelo$fit %>% augment()      

augment(modelo_lm1_est$fit) %>% select(volume, .fitted:.std.resid)

```
-->

## Flujo de trabajo estimado (cont.)

::: {style="font-size: 90%;"}

* El proceso es igual para problemas de clasificación. 

```{r}
#| eval: false
censo <- read_csv("data/census.csv") %>% select(-1) %>% 
  mutate(across(where(is.character),~as.factor(.x)))
set.seed(8697)
censoPart <- censo %>% initial_split(prop = .8)

receta_log1 <- training(censoPart) %>%
  recipe(income ~ age + education_1 + capital_gain)
modelo_log1 <- logistic_reg(mode= "classification", engine = "glm", 
                             penalty = 0)

flujo_log1 <- workflow() %>%
  add_recipe(receta_log1) %>%      
  add_model(modelo_log1)

flujo_log1_est <- flujo_log1 %>% 
                   fit(data = censoPart %>% training()) 
```


```{r}
#| eval: false
estim_extraida_log <- flujo_log1_est %>% extract_fit_parsnip() 

estim_extraida_log %>% tidy() 
estim_extraida_log %>% glance()
```

:::

# Validación del modelo

## Predicción 

::: {style="font-size: 95%;"}

* La función `predict()` (de `parsnip`) tiene dos argumentos principales: un <!--objeto--> flujo de trabajo estimado y un conjunto de datos (nuevo como los datos de prueba u otros valores)
    
  + El flujo estimado almacena la receta para transformar los datos
  
  + También contiene los resultados de la estimación ("parámetros" del modelo) que se aplican a los datos transformados para predecir

```{r}
#| eval: false
flujo_lm1_est %>% 
  predict(new_data = railtrailPart %>% testing())
``` 


* Devuelve un conjunto de datos (*tibble*) con [predicciones](https://parsnip.tidymodels.org/reference/predict.model_fit.html):

    + para problemas de regresión: valores numéricos (`type = "numeric"`, por defecto), intervalos de confianza (`type = "conf_int"`), etc.
    + para problemas de clasificación: clases/categorías (`type = "class"`, por defecto), las probabilidades de cada categoría (`type = "prob"`)
 

<!--
* Devuelve un `tibble` (no un vector), lo que permite añadir la predicción que podemos añadir a los datos originales con `bind_cols()` 


```{r}
#| eval: false
#| echo: false
 lm1_flujo_est %>% 
  predict(new_data = RailTrail_prueb) %>% 
  bind_cols(RailTrail_prueb %>% select(volume))     # Variable predicha .pred

logit1_flujo_est %>% 
  predict(censo_part %>% testing())                # clase predicha .pred_class

logit1_flujo_est %>% 
  predict(censo_part %>% testing(), type = "prob")    # probabilidades de cada categoría
```

* Notad que se procesan *automáticamente* los datos donde se predice

-->

:::

## Métricas de error

::: {style="font-size: 95%;"}

<!--
* La función `last_fit()` ajusta el flujo en los datos de entrenamiento y obtiene predicciones en los de prueba.

 * Calcula la predicción en la muestra de prueba de los valores (para regresión) o de las clases y la probabilidad de cada clase (para clasificación) 

* Existen [funciones para calcular](https://yardstick.tidymodels.org/articles/metric-types.html) las métricas de error usando un conjunto de datos valores/clases observados y predichos

* Pero también se pueden obtener directamente las métricas
-->

* La función `last_fit()` ajusta el flujo en los datos de entrenamiento y obtiene predicciones en los de prueba, para calcular las métricas (ver [listado](https://yardstick.tidymodels.org/articles/metric-types.html)):

```{r}
#| eval: false
flujo_lm1_finalfit <- flujo_lm1 %>% 
                    last_fit(split = railtrailPart,
                             metrics = metric_set(rmse, mae)) 
flujo_lm1_finalfit %>% collect_metrics()

flujo_log1_finalfit <- flujo_log1 %>% 
                      last_fit(split = censoPart,
                             metrics = metric_set(accuracy, roc_auc))
flujo_log1_finalfit %>% collect_metrics()
```

* También podemos trabajar con las predicciones en la muestra de prueba:

```{r}
#| eval: false
predic_log1 <- flujo_log1_finalfit %>% collect_predictions() 
predic_log1 %>% roc_auc(income, `.pred_<=50K`) 
predic_log1 %>% roc_curve(income, `.pred_<=50K`) %>% autoplot()
```



```{r}
#| eval: false
#| echo: false
predic_lm1 <- flujo_lm1_finalfit %>% collect_predictions()
predic_lm1 %>% mae(truth=volume, estimate= .pred) 

predic_log1 <- flujo_log1_finalfit %>% collect_predictions() 
predic_log1 %>% roc_auc(income, `.pred_<=50K`) 
predic_log1 %>% roc_curve(income, `.pred_<=50K`) %>% autoplot()
```

* NOTA: con más de dos clases, la matriz de confusión tiene dimensión $\small k \times k$ y se calcula una ROC-AUC para cada clase frente a las demás 


<!--

* Para calcular las métricas de error de un modelo, necesitamos un conjunto de datos con valores/clases observados (*truth*) y predichos (*estimate*)

* Añadimos la columna con la predicción a los datos y se los pasamos a una función que [calcule la métrica](https://yardstick.tidymodels.org/articles/metric-types.html): una en concreto o varias a la vez

```{r}
#| eval: false
datosMetricas <- flujo_lm1_est %>% 
  predict(new_data = railtrailPart %>% testing()) %>% 
  bind_cols(railtrailPart %>% testing())  
  
datosMetricas %>% mae(truth=volume, estimate= .pred)  

datosMetricas %>% metrics(truth=volume, estimate= .pred)    

mis_metricas <- metric_set(mae, rmse)
datosMetricas %>% mis_metricas(truth=volume, estimate= .pred)    
```
-->

<!--
## Métricas de error para problemas de clasificación

::: {style="font-size: 90%;"}

* Podemos calcular métricas para clases predichas (matriz de confusión y derivadas) ...

```{r}
#| eval: false
datosMetricas_log1 <- flujo_log1_est %>% 
  predict(new_data = censoPart %>% testing()) %>% 
  bind_cols(censoPart %>% testing())
datosMetricas_log1 %>%   
  conf_mat(truth=income, estimate= .pred_class)           
mis_metricas <- metric_set(accuracy, spec, sens)      
datosMetricas_log1 %>%
  mis_metricas(truth=income, estimate= .pred_class)   
```
-->

<!--
*  Nota: para predecir clases se utiliza por defecto la clase con mayor probabilidad predicha (la más probable); es decir, en el caso binario un umbral de $\small 0.5$
-->

<!--
* ... o métricas basadas probabilidades predichas (ROC-AUC)

```{r}
#| eval: false
datosMetricas_log2 <- flujo_log1_est %>% 
  predict(new_data = censoPart %>% testing(), type = "prob") %>% 
  bind_cols(censoPart %>% testing())
datosMetricas_log2 %>% roc_auc(income, `.pred_<=50K`) 
datosMetricas_log2 %>% roc_curve(income, `.pred_<=50K`) %>% autoplot()
```
-->

<!--
o la curvas de ganancias
```{r}
#| echo: false
#| eval: false
logit1_probs %>%
  gain_curve(income, `.pred_<=50K`) %>%
  autoplot()
```

* Se pueden combinar clases y probabilidades predichas

```{r}
#| echo: false
#| eval: false
modelo_logit1_est %>% predict(logit1_prueba) %>% 
  bind_cols(logit1_prueba) %>%  
  bind_cols(logit1_probs %>% select(1:2)) %>% 
  metrics(truth=income, `.pred_<=50K`, estimate= .pred_class)
```
-->

<!--

* Con más de dos clases, la matriz de confusión tiene dimensión $\small k \times k$ y se calcula una ROC-AUC para cada clase frente a las demás 

-->

<!--

* Con más de dos clases, se predice la probabilidad de cada clase y la clase predicha es la más frecuente

  * La matriz de confusión es similar, pero de dimensiones $\small k \times k$

  * La *accuracy* sigue teniendo la misma interpretación
  
  * La ROC-AUC se calcula para cada clase frente a las demás 
-->

:::

## Validación cruzada

* En lugar de *initial_split()*, podemos usar `vfold_cv()` para crear particiones

<!--
    + ver ayuda para opciones adicionales (ej. *estratos*)
-->

```{r}
#| eval: false
set.seed(101)
railtrailPartCV <- RailTrail %>% 
                    mutate(dayType = parse_factor(dayType)) %>% 
                    vfold_cv(v=10) 
```

* Podemos acceder a la información de cada bloque (*splits*)

<!--

* Podemos extraer cada uno de los bloques (*splits*) y recuperar sus datos de entrenamiento (`analysis()`) y  prueba (`assessment()`):

```{r}
#| eval: false
#| echo: false
railtrailPartCV$splits[[1]] %>% analysis()
railtrailPartCV$splits[[1]] %>% assessment()
```
-->

```{r}
#| eval: false
#| echo: false
railtrailPartCV$splits[[1]] %>% analysis()
```

* El flujo de trabajo puede ser el mismo que ya hemos visto <!--(aunque la receta use que no sean de estas particiones)-->

* La estimación del modelo se realiza usando `fit_resamples()` <!--sobre el flujo de trabajo y la partición de validación cruzada-->

<!-- no solo a datos de entrenamiento, porque aquí varían -->

```{r}
#| eval: false
#| echo: false
lm1_flujo_cv_est <- lm1_flujo  %>% 
                        fit_resamples(RailTrail_cv)
```

<!-- 
* Se pueden cambiar varias opciones del proceso 

como las métricas calculadas y otros elementos de control 
-->

```{r}
#| eval: false
flujo_lm1_CVest <- flujo_lm1  %>% fit_resamples(
                      resamples = railtrailPartCV, 
                      metrics   = metric_set(rmse, mae) )
```

```{r}
#| eval: false
#| echo: false
flujo_lm1_CVest <- flujo_lm1  %>% 
                    fit_resamples(
                      resamples = railtrailPartCV, 
                      metrics   = metric_set(rmse, mae),
                      control   = control_resamples(save_pred = T)
                      )
```


* ... y el objeto creado contiene los valores de las métricas 

```{r}
#| eval: false
flujo_lm1_CVest %>% collect_metrics()      # promedio 
```

```{r}
#| eval: false
#| echo: false
flujo_lm1_CVest %>% collect_metrics()      # promedio sobre 10 iteraciones
flujo_lm1_CVest$.metrics %>% bind_rows()   # valores en cada iteración
```


# Modelos con hiperparámetros

## Selección de hiperparámetros: *tuning*

* Algunos parámetros, denominados **hiperparámetros**, no se aprenden junto con el resto de parámetros del modelo (ej., $\small \lambda$ en LASSO)

:::: {.columns}

::: {.column width="50%"}

* Proceso de **ajuste** (*tuning*): 

  - en una parte de la muestra de entrenamiento, se estiman los parámetros dado un valor del hiper-parámetro 

  - en la otra parte, se mide el error asociado a ese hiper-parámetro para validar el mejor valor
  
  - se elige el valor con mejor métrica de error en validación
:::

::: {.column width="50%"}

<center>
![](figure/train_test.png){width=90%} 

$\Downarrow$

![](figure/train_validate.png){width=90%}
</center>

$\hspace{0.1cm}$

<center>
![](figure/train_cross_validate.png){width=90%}
</center>

::: 
::::


<!--

<center>
![](figure/resampling.svg){width=70%}
</center>

-->

## Proceso de *tuning* 

* Usamos una partición de *initial_split()* y la misma receta anterior (aunque no era necesario antes, el pre-procesado ya estandarizaba los regresores)

* En la definición del modelo debemos identificar los hiperparámetros a ajustar

```{r}
#| eval: false
modelo_LASSO <- linear_reg(mode= "regression", engine = "glmnet",
                           penalty = tune() ) 

flujo_LASSO <- workflow() %>%
  add_recipe(receta1) %>% 
  add_model(modelo_LASSO)

flujo_LASSO %>% extract_parameter_set_dials()
```

* Definimos un conjunto de valores a probar (*grid*); p.e., con `grid_regular()` buscamos en un intervalo un número de valores ([otras opciones aquí](https://dials.tidymodels.org/reference/grid_regular.html))
```{r}
#| eval: false
LASSO_grid <- grid_regular(penalty(range = c(0, 15), trans = NULL), 
                                   levels = 51)
```

<!--

Tres estrategias 

* `grid_regular()` : combinaciones están igualmente espaciadas para cada hiperparámetro 

* `grid_random()` :  los valores son aleatorios dentro de unos límites preestablecidos

* `grid_max_entropy()`: los valores son aleatorios pero intentan cubrir todo lo posible el espacio de búsqueda

-->

## Proceso de *tuning* (cont.)

* Se usa `tune_grid()` de forma a similar a *fit_resamples()*, usando una sub-partición de la *muestra de entrenamiento* por validación cruzada

<!-- alternativa: tune_bayesian() -->

<!--
* Se obtienen remuestras por Validación Cruzada de la muestra de *entrenamiento* 
-->

```{r}
#| eval: false
set.seed(9753)
railtrail_entrenCV <- railtrailPart %>% training() %>% 
                          vfold_cv(v=10)
flujo_LASSO_ajust <- flujo_LASSO %>% 
                        tune_grid(resamples = railtrail_entrenCV, 
                                  metrics   = metric_set(rmse, mae),
                                  grid      = LASSO_grid            )
```

<!--
* La validación cruzada es computacionalmente intensiva, especialmente con varios hiperparámetros

  + se suelen realizar mediante computación en paralelo

```{r}
#| echo: false
#| eval: false
receta_lm2 <- training(RailTrail_part) %>% recipe(volume ~ avgtemp) %>%
  step_poly(avgtemp, degree = tune("lambda")) %>% 
  step_center(all_predictors()) %>% step_scale(all_predictors()) 

flujo_LASSO_tuning2 <- workflow() %>% add_recipe(receta_lm2) %>% add_model(modelo_LASSO)
flujo_LASSO_tuning2 %>% parameters() 

LASSO_grid2 <- grid_regular(penalty(range = c(0,15), trans = NULL), 
                            degree_int(range=c(1,8)), levels = 51 )

```

-->

* Podemos visualizar el ajuste y obtener los mejores valores (por la variabilidad, varios son igualmente aceptables)

<!-- select_by_one_std_err()  -->

```{r}
#| eval: false
flujo_LASSO_ajust %>% autoplot()
flujo_LASSO_ajust %>% show_best(metric = "mae")
mejor_lambda <- flujo_LASSO_ajust %>% select_best(metric = "rmse")
```


```{r}
#| eval: false
#| echo: false
flujo_LASSO_ajust %>% autoplot()

penalty <- flujo_LASSO_ajust %>% collect_metrics() %>% filter(.metric == "mae")
penalty %>% ggplot(aes(x=penalty, y=mean)) +  scale_x_log10() +
              geom_line() + geom_point(color="red") + 
              geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), color="gray")
```


* NOTA: debemos probar manualmente varios rangos y valores buscando la <!--típica forma de--> U


<!-- ## Estimación del modelo completo -->

## Finalizando y evaluando el modelo

::: {style="font-size: 95%;"}

* Actualizamos el flujo de trabajo para dar un valor a los hiperparámetros

```{r}
#| eval: false
flujo_LASSO_final <- flujo_LASSO %>% 
        finalize_workflow(mejor_lambda)  
        # finalize_workflow(parameters = list(penalty=8.5))
```

* Debemos estimar el modelo en los datos *completos* de entrenamiento, para  mostrar resultados de estimación o predecir

```{r}
#| eval: false
flujo_LASSO_final_est <-  flujo_LASSO_final %>%  
              fit(data = railtrailPart %>% training())
flujo_LASSO_final_est %>%  extract_fit_parsnip() %>% tidy()
```


* Para obtener las métricas, usamos `last_fit()`<!--: ajusta al modelo finalizado en los datos de entrenamiento y lo evalúa en los de prueba.-->

```{r}
#| eval: false
LASSO_final_fit <- flujo_LASSO_final %>% 
                    last_fit(split = railtrailPart,
                             metrics = metric_set(rmse, mae)) 
LASSO_final_fit %>% collect_metrics()
```


```{r}
#| eval: false
#| echo: false
LASSO_final_fit <- flujo_final %>% last_fit(RailTrail_part)

LASSO_final_fit %>% collect_metrics()
LASSO_final_fit %>% collect_predictions()


LASSO_final_fit <- flujo_final %>% last_fit(split   = RailTrail_part,
                                      metrics = metric_set(rmse, mae))
LASSO_final_fit %>% collect_metrics()
```

:::
