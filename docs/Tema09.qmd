---
# subtitle: "Técnicas para 'Big Data' en Economía - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Análisis de Datos Multivariantes aplicado al Marketing - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Muestreo y Análisis de Datos - Universidad de Alicante"
# subtitle: "Econometría II - Curso 2023/24 \n\n Universidad de Alicante"
title    :  "Tema 09 - Métodos basados en árboles"
author:  
    - "Pedro Albarrán"
#    - "Teresa Molina"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
   
# institute: 
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:  
        - beige # sky # serif # simple # default # moon #  
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide # concave # 
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    # width: 3000
    # height: 2000
    # margin: 0.05
execute:
  enabled: true      # (no) ejecutar code chunks
  eval: false        # por defecto, evalúa y muestra códido de code chunks
  echo: true
  warning: false    # pero no los mensajes ni warnings
  message: false
knitr:
  opts_chunk:
    results: hide     # ni muestra resultados ni figuras
    fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---


```{r setup, eval=TRUE, include=FALSE}
#```{r setup, message=FALSE, warning=FALSE, include=FALSE} 
# include=F es suficiente para no incluir mensajes, etc.

# Opciones por defecto para los fragmentos de código
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      results = "hide", fig.show="hide")
# se muestra y evalúa el código,
# no se muestran mensajes, ni avisos (warnings)
# no se muestran los resultados de código (tampoco gráficos)
#     en los códigos que considere necesarios los mostraré

# Elimino todo del Entorno (del documento)
rm(list = ls())       

# Cargo todas las bibliotecas necesarias
# (se podría hacer cuando cada una sea necesaria)
library(tidyverse)
library(tidymodels)
# library(printr)
# library(skimr)
# library(dlookr)
# library(broom)
library(kableExtra)
# library(rpart.plot)
# library(vip)

#fijo el directorio de trabajo
#setwd("/home/albarran/Dropbox/MAD/00.TEC")
library(rmarkdown)
#render("filename.Rmd")     
#browseURL("filename.html")
```

## Estratificación para asociaciones no lineales


:::: {.columns}

::: {.column width="50%"}

<!-- * Ejemplo: relación no lineal en los datos de viviendas de Boston -->

```{r}
#| echo: false
#| eval: false
# en local
# Boston <- read_csv("data/BostonHousing.csv")

# para .qmd en la nube
Boston <- read_csv("https://raw.githubusercontent.com/albarran/00datos/refs/heads/main/BostonHousing.csv")
```


* Ejemplo: relación no lineal entre las ventas y la edad


```{r}
#| echo: false
# en local
# descuento <- read.csv("data/descuento.csv") 

# para .qmd en la nube
descuento <- read_csv("https://raw.githubusercontent.com/albarran/00datos/refs/heads/main/descuento.csv")
```


* Cuantos más particiones, mejor ajuste. 

* PERO ¿dónde dividimos?, ¿cuántas particiones?

:::

::: {.column width="50%"}

```{r}
#| eval: false
#| echo: false
#| animation.hook: "gifski"
#| fig.align: 'center'
#| fig.height: 5
#| fig.width: 5

g0 <- Boston %>%  ggplot(aes(lstat, medv)) + geom_point()

g0 +  geom_smooth(method = lm, se = FALSE,
    formula = y ~ cut(x,c(0, 25, 100), include.lowest = T ))

g0 +  geom_smooth(method = lm, se = FALSE,
    formula = y ~ cut(x,c(0, 15, 25, 100), include.lowest = T ))

g0 +  geom_smooth(method = lm, se = FALSE,
  formula = y ~ cut(x,c(0, 5, 15, 25, 100), include.lowest = T ))
```

```{r}
#| echo: false
#| eval: true
library(gifski)
```


```{r}
#| eval: true
#| echo: false
#| animation.hook: "gifski"
#| fig.align: 'center'
#| fig.height: 5
#| fig.width: 5

g0 <- ggplot(descuento, aes(x=edad, y=ventas)) + geom_point() 

g0 + geom_smooth(method = "lm", se = FALSE,
              formula = y ~ cut(x, seq(20, 60, 20), include.lowest = T))

g0 + geom_smooth(method = "lm", se = FALSE,
              formula = y ~ cut(x, seq(20, 60, 10), include.lowest = T))

g0 + geom_smooth(method = "lm", se = FALSE,
              formula = y ~ cut(x, seq(20, 60, 5), include.lowest = T))

g0 + geom_smooth(method = "lm", se = FALSE,
              formula = y ~ cut(x, seq(20, 60, 2.5), include.lowest = T))

```

:::

::::

* Esta cuestión también aplica a las interacciones entre variables para capturar heterogeneidad del efecto de una variable: ¿cuántas interacciones? ¿con cuántos intervalos?

## Árboles de decisión

<!-- 
* Los métodos basados en árboles implican la estratificación o la segmentación del espacio de predicción en una serie de regiones simples


* El conjunto de reglas de división usada para segmentar puede resumirse en un árbol

* árbol de decisión es un diagrama de flujo en forma de árbol que clasifica observaciones -->

* Un árbol de decisión es un diagrama de flujo con reglas para segmentar el espacio de regresores en regiones más simples y clasificar observaciones

<!--
* Los árboles de decisión suelen estar dibujados al revés:
    + las hojas o los nodos terminales en la parte inferior del árbol
    
    +  los puntos del árbol en los que se divide el espacio de predicción se denominan nodos internos. 

    + los nodos se conectan por ramas.

* Cada rama del árbol separa las observaciones en subconjuntos cada vez homogéneos ("puros"): es más probable que compartan la misma  clase o valor.

<!--
Un ejemplo de árbol con dos variables numéricas $X_1$ y $X_2$ sería
-->

<!--
## Árboles de decisión (cont.)
-->

<center>
![](figure/tree.png){width=90%}
</center>

<!-- En el caso de una variable también tenemos un árbol:

x1 < 5   -  x1 > 5
  |             |
  |    x1 < 15 -- x1 > 15 
  |     |            |
  |     |   x1 < 25 -- x1 > 25   
  |     |      |          |
|---|------|-------|---------       
0   5     15       25

* El árbol de X1, X2 representa un modelo más complejo que lineal con X1 y X2: 
      permite interacciones entre X1 y X2   
-->


## Árboles de Regresión

* ¿Cómo construimos estos árboles para una variable cuantitativa? 

  1. Dividir el espacio de predicción, $\small x_1, x_2, \dots, x_p$ en regiones $\small J$ distintas y no superpuestas, $\small R_1, R_2, \dots, R_J$ 
  
      + cada vez más homogéneas o "puras": como todos los modelos<!--(ej., *kNN*)-->, mismas características, mismo valor esperado de $y$

  2. Cada observación que caiga en la región $\small R_j$ tiene el mismo valor predicho: la media de $\small y$ para las observaciones de entrenamiento en $\small R_j$, $\small \widehat{y}_{i\in R_j}= \bar{y}_{R_j}$

* Es INVIABLE considerar todas las particiones en $\small J$ regiones <!--para determinar la globalmente óptima-->

* Alternativas heurísticas: un algoritmo "voraz" elige una opción localmente óptima en cada paso con la esperanza de llegar a una solución general óptima 

    + (en lugar de <!--mirar adelante y --> elegir la mejor partición para un paso futuro)

<!--
  * Por simplicidad y por interpretabilidad, se divide el espacio de predicción en rectángulos de p-dimensiones o cajas

  * El objetivo es encontrar regiones que minimicen la SCR <!--suma cuadrática de los residuos-->

<!--
\[\small
\sum_j \sum_{i\in R_j} (y_i-\widehat{y}_{R_j})^2
\]
-->

<!--
donde y^Rj
es la respuesta media para las observaciones de entrenamiento en el cuadro j --> 

<!--
## ¿Cómo encontrar el árbol óptimo?

* NO podemos: -->

    
## Partición binaria recursiva 

<!--
* Sigue un enfoque de arriba a abajo
-->

1. En la parte alta, todas las observaciones pertenecen a una sola región
  
2. Se divide sucesivamente cada región en dos ramas: $\small R_1(j,s) = \{X|X_j < s\}$ y  $\small R_2(j,s) = \{X|X_j \geq s\}$
  
    * En cada nodo, se consideran *todos* los regresores $\small X_j$ y *todos* los umbrales $s$
    
    * Se calcula el error de predicción por particionar de esa manera 
    $\small SCR_{j,s} = \sum_{i\in R_1(j,s)}(y_i-\widehat{y}_{R_1})^2 + \sum_{i\in R_2(j,s)}(y_i-\widehat{y}_{R_2})^2$

    * SOLO una partición en cada iteración: se elige $\small j$ y $\small s$ con menor $\small SCR_{j,s}$ <!--en esa iteración -->
    
3. Repetimos el proceso particionando cada región de la iteración anterior 
<!-- NO el conjunto entero -->

4. Se continua hasta que se cumpla un criterio de parada; p.e., ninguna región contiene más de $5$ observaciones


## Podar un árbol (*pruning*)

<!--
* El proceso anterior lleva, en general, a árboles complejos: "overfitting"

* Alternativa 1: hacer crecer el árbol sólo si la disminución en SCR en cada división excede un umbral para obtener árboles más pequeños
    
* PERO es poco previsora: una división "inútil" al principio del árbol podría implicar gran reducción de la SCR futura.
-->

* Evitar árboles demasiados complejos ("overfitting")

* Hacer crecer un árbol "grande" con $\small T_0$ nodos terminales y **podarlo** para quedarnos con un sub-árbol con $\small T$ nodos terminales
$$
\small
min \sum_{m=1}^T \sum_{i \in R_m} (y_i-\widehat{y}_{R_m})^2 + \alpha |T|
$$
donde $\small R_m$ es la región de $\small m$-nodo terminal

*  $\small \alpha$ es el hiperparámetro de **coste de complejidad** de la poda, elegido por validación cruzada

<!---

   ------------------
   |                |   
   |                |
|------|        |--------|
1      2        |        |
            |-----|    |----|
            |     |    |    |
            3     4    5    6
            
SCR_1 + a*|6|

(SCR_1 es la menor SCR pero la penalización...)

podando rama 1,2: SCR_2 + a*|5|
podando rama 3,4: SCR_3 + a*|5|
podando rama 5,6: SCR_4 + a*|5|
podando rama 3-6: SCR_5 + a*|4|
podando rama 1-2 y 3-4: SCR_6 + a*|4|
etc...
-->


## Árboles de clasificación

* Para un árbol de clasificación, se predice que cada observación pertenece a la clase más común en la región a la que pertenece en entrenamiento

* También se pueden obtener la proporción de una clase $\small k$ dentro de una región particular $\small m$ de nodos terminales: $\small \widehat{p}_{mk}$

* La métrica usada para hacer crecer los árboles NO puede ser SCR

<!--
1. Tasa de error de clasificación (fracción de observaciones que no pertenecen a la clase más común)
\[
\small
E=1 - max_k (\widehat{p}_{mk}$)
\]

no es lo suficientemente sensible para el cultivo de árboles y son preferibles otros criterios.
-->

2. Índice de Gini: medida de la varianza entre clases $\small
G=\sum_{k=1}^{K}\widehat{p}_{mk}(1-\widehat{p}_{mk})$

3. Entropía (cruzada) $\small
D=\sum_{k=1}^{K}\widehat{p}_{mk}log(\widehat{p}_{mk})$

* Ambos son medidas de "pureza" del nodo: un valor pequeño indica que la región contiene en su mayoría observaciones de una sola clase.

<!-- Gini con dos categoria: si todo de la misma categeria p=1, (1-p)=0, Gini=0: máxima homogeneidad  --->


<!--
## Ejemplo de clasificación

```{r}
#| eval: false
censo <- read_csv("data/census.csv") %>% 
          mutate(income=as.factor(income))

library(tidymodels)

set.seed(7482)
censo_part <- censo %>% initial_split(prop = .8)

censo_receta_arbol <- training(censo_part) %>%
  recipe(income ~ age + education_1 + sex + capital_gain + capital_loss + hours_per_week) 

censo_modelo_arbol  <- decision_tree(mode = "classification", 
                                     cost_complexity = .01) %>% 
                          set_engine("rpart") 

censo_flujo_arbol <- workflow() %>% 
                      add_recipe(censo_receta_arbol) %>% 
                      add_model(censo_modelo_arbol)

```

## Ejemplo de clasificación (cont.)

```{r}
#| eval: false
censo_flujo_arbol_est  <- censo_flujo_arbol %>% 
                            fit(data = censo_part %>% training()) 

censo_flujo_arbol_est %>% 
  predict(testing(censo_part)) %>% 
  bind_cols(testing(censo_part)) %>%  
  accuracy(income, .pred_class) 

library(rpart.plot)
arbol <- censo_flujo_arbol_est %>% pull_workflow_fit() 
rpart.plot(arbol$fit)   
```

## Algoritmos para árboles e hiperparámetros

* Existen tres  bibliotecas ("motores") que implementan algoritmos para árboles en `tidymodels`: `rpart` (por defecto), `spark` y `C5.0` (solo clasificación)

* Cada algoritmo depende de varios hiperparámetros que deben elegirse mediante validación cruzada (proceso de ajuste o *tuning*)

  + `min_n`: El número mínimo de observaciones en un nodo requeridos para que se divida más.

  + Profundidad del árbol (`tree_depth`): máximo número de niveles del árbol, en `rpart` y `spark`

  + Coste de complejidad (`cost_complexity`), solo en `rpart` 
-->

## Algoritmos para árboles e hiperparámetros

* <!--Bibliotecas ("motores") que implementan --> Motores para árboles: `rpart` (defecto), `spark` y `C5.0` (solo clasificación)

* Depende de varios hiperparámetros elegidos por *tuning*

  + `min_n`: mínimo número de observaciones en un nodo parar dividirse más

  + `tree_depth`: máx. número de niveles de profundidad del árbol (no `C5.0`)

  + `cost_complexity`: coste de complejidad (sólo `rpart`) 

```{r}
#| eval: false
modelo_arbol  <- decision_tree(mode = "classification", 
                               engine = "rpart", 
                               cost_complexity = .01)
```

* Podemos visualizar un modelo árbol estimado con la biblioteca `rpart.plot` 
```{r}
#| eval: false
library(rpart.plot)
arbol <- flujo_arbol_est %>% extract_fit_parsnip() 
rpart.plot(arbol$fit)  
```



## Comentarios

* Predicción: media o clase mayoritaria de las observaciones de entrenamiento en el nodo que corresponde a una observación nueva

* Importancia de las variables: reducción total de la SCR o Gini debida a las particiones de una variable
  + variables usadas en nodos iniciales o más veces son más importantes

* Ventajas: 
    + ajusta (no paramétricamente) relaciones no lineales/complejeas
    + fáciles de explicar (más que la regresión lineal) y visualizar
    + no requieren dummies (ni transformaciones no lineales de variables continuas)

<!--

           educ<13
    |-------------------|
    |educ<6             | female
|------|           |----------|
b0     b1         b2         b3  

equivale a un modelo con interacciones:

y = b0 + b1 D(educ in 6-13) + 
        b2 * D(educ>13)*female + b3*D(educ>13)


frente a 

y = g0 + g1*educ + g2*female

aunque se puede mejorar con 

y = g0 + g1*educ +g2*female + g3*educ*female
y = g0 + g1*educ +g2*female + g3*educ^2

--->

* Desventajas: *overfit* (no robustos a cambios en los datos)<!--(grandes cambios en  árbol final)--> y bajo poder de predicción

<!--
* PERO **agregar** muchos árboles de decisión mejorar el rendimiento predictivo y mitigar algunas desventajas
-->

## *Bagging*

* Agregación de Bootstrap (*bagging*): procedimiento general para reducir la varianza, promediando $\small \{x_i\}_{i=1}^n iid \sim (\mu, \sigma^2) \Rightarrow \bar{x} \sim (\mu, \sigma^2/n)$


* Podemos tomar $B$ re-muestras del **único** conjunto de entrenamiento 

<!--
    * en cada remuestra $\small b$, entrenar y obtener una predicción: $\small \widehat{f}^b(x)$
    * promediar las predicciones $\small \widehat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B\widehat{f}^b(x)$

    * o predecir por voto mayoritario: clase más común en $\small B$ remuestras

## Agregando árboles

* *Bagging* es útil y habitual en el contexto de los árboles de decisión
-->

<!--
* Una aplicación popular consiste en un "bosque" de árboles de decisión 
-->

* En lugar de un único árbol complejo, se combinan muchos 
      árboles diversos que pueden reflejar patrones sutiles 
      
* La variación muestral en las condiciones de "entrenamiento" se genera mediante "bootstrap"

<center>      
![](./figure/random-forest.png){width=55%}
</center>

<!--

* El número de árboles, B, no es un parámetro crítico: B no implica "overfitting"
-->

<!--
Out of Bag Error Estimation

It can be shown that, on average, each bagging tree makes use of around two-thirds of the training observations. The remaining third of the observations that are not used to fit a given bagged tree are referred to as the out-of-bag observations. An approximation of the test error of the bagged model can be obtained by taking each of the out-of-bag observations, evaluating the B/3
predictions from those trees that did not use the given out-of-bag prediction, taking the mean/mode of those predictions, and comparing it to the value predicted by the bagged model, yielding the out-of-bag error. When B is sufficiently large, out-of-bag error is nearly equivalent to leave-one-out cross validation.
-->

<!--
## Agregando árboles (cont.)

<center>      
![](./figure/random-forest-machine-learning.png){width=40%}
</center>      


-->

## "Random forest"

* Es un algoritmo específico de agregación de árboles que introduce aleatorización para **eliminar correlación** entre los árboles.

    * Se construyen varios árboles en muestras de entrenamiento de bootstrap

    * En cada partición de un árbol, se seleccionan aleatoriamente  $\small m \approx \sqrt{k}$ regresores del total 

    * Se mitiga la influencia de regresores fuertes, permitiendo una mayor diversidad en los árboles agregados

<!--
    + cada división sólo considera una minoría de los regresores disponibles
-->


* Mejor predicción a expensas de la interpretación $\Rightarrow$ medidas de importancia variable: reducción promedio en SCR / Gini para un regresor en los $\small B$  árboles

<!--
+ Con la noción de **importancia**, se determina qué variables parecen ser las más influyentes de manera consistente en los distintos árboles
-->

* La importancia tiene un papel análogo al de los p-valores (sin una inferencia estadística formal), para ayudar a generar hipótesis.


## Algoritmos para RF e hiperparámetros

* Motores principales para RF en R: `ranger`, `randomForest` y `spark`

<!--
* En todos los casos depende del números de árboles a considerar en total y de  dos hiperpárametros, con distintos nombres y valores por defecto
-->
<!-- ver descripción en ayuda -->

* Depende de `trees`, números de árboles, a considerar en la agregación (no genera "overfitting") y de dos hiperparámetros

  * `mtry`: número de variables a considerar en cada partición

  * `min_n`: igual que en árboles

```{r}
#| eval: false
library(ranger)
modelo_RF  <- rand_forest(mode = "regression", trees = 100,
                          mtry = 3) %>% 
              set_engine("ranger", importance = "permutation")
              # importance = "impurity" en clasificación

RF <- flujo_RF_est %>% extract_fit_parsnip()
RF$fit$variable.importance
library(vip)
RF %>% vip()
```


<!--
## "Random forest": ejemplo de regresión
```{r}
#| eval: false
#install.packages("ranger")
library(mosaicData)
set.seed(9753)
RailTrail_part <- RailTrail %>% initial_split(prop = .8)

RailTrail_receta_RF <- training(RailTrail_part) %>%
  recipe(volume ~ cloudcover + precip + avgtemp + weekday) 

RailTrail_modelo_RF  <- rand_forest(mode = "regression",
                                       mtry = 3, trees = 100) %>% 
                          set_engine("ranger", importance = "impurity")

RailTrail_flujo_RF <- workflow() %>% add_recipe(RailTrail_receta_RF) %>% 
                      add_model(RailTrail_modelo_RF)

```

## "Random forest": ejemplo de regresión (cont.)

```{r}
#| echo: false
#| eval: false
RailTrail_flujo_RF_est  <- RailTrail_flujo_RF %>% fit(training(RailTrail_part))

RailTrail_flujo_RF_est %>% predict(testing(RailTrail_part)) %>% 
  bind_cols(testing(RailTrail_part)) %>% metrics(volume, .pred) 


RailTrail_flujo_RF_est_pull <- pull_workflow_fit(RailTrail_flujo_RF_est)$fit

RailTrail_flujo_RF_est_pull$variable.importance


library(vip)
pull_workflow_fit(RailTrail_flujo_RF_est) %>% vip()
```


## "Random forest": ejemplo de clasificacion


```{r}
#| eval: false
library(tidymodels)
set.seed(7482)
censo_part <- censo %>% initial_split(prop = .8)

censo_receta_RF <- training(censo_part) %>%
  recipe(income ~ age + education_1 +  capital_gain + capital_loss + hours_per_week)

censo_modelo_RF  <- rand_forest(mode = "classification",
                                       mtry = 3, min_n=10, trees = 100) %>% 
                          set_engine("ranger", importance = "impurity") 

censo_flujo_RF <- workflow() %>% add_recipe(censo_receta_RF) %>% 
                      add_model(censo_modelo_RF)

censo_flujo_RF_est  <- censo_flujo_RF %>% fit(data = censo_part %>% training()) 

censo_flujo_RF_est %>% predict(testing(censo_part)) %>% 
  bind_cols(testing(censo_part)) %>%  accuracy(income, .pred_class) 
```

-->