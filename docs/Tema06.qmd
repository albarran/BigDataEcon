---
# subtitle: "Técnicas para 'Big Data' en Economía - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Análisis de Datos Multivariantes aplicado al Marketing - Curso 2023/24 \n\n Universidad de Alicante"
# subtitle: "Muestreo y Análisis de Datos - Universidad de Alicante"
# subtitle: "Econometría II - Curso 2023/24 \n\n Universidad de Alicante"
title    :  "Tema 06 - Modelización. Aprendizaje estadístico"
author:  
    - "Pedro Albarrán"
#    - "Teresa Molina"
institute: "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
   
# institute: 
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
#     - "Dpto. de Fundamentos del Análisis Económico. Universidad de Alicante"
format:
  beamer:
    handout: false
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:  Boadilla # Copenhagen # CambridgeUS #
    outertheme: miniframes
    colortheme: crane
    section-titles: false
    fontsize: 10pt
    header-includes: |
        \setbeamertemplate{footline}
        {
        \leavevmode%
        \hbox{%
        \begin{beamercolorbox}[wd=.30\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
        \usebeamerfont{author in head/foot}\insertshortauthor%
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
        \usebeamerfont{title in head/foot}\insertshorttitle%
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.15\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
        \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}}%
        }
       # - \setbeamertemplate{navigation symbols}{}
       # - \setbeamertemplate{caption}[numbered]
       # - \setbeamertemplate{headline}[page number]
       # - \setbeameroption{show notes}
       # - \setbeameroption{show notes on second screen}
  revealjs:
    logo: figure/by-nc-sa2.png
    titlegraphic: figure/by-nc-sa.png
    theme:  
        - beige # sky # serif # simple # default # moon #  
        - custom.scss
    smaller: true
    scrollable: true
    embed-resources: true
    slide-number: true
    show-slide-number: all
    transition: slide # concave # 
    background-transition: fade
    progress: true
    height: 800
    width: 1200
    # width: 3000
    # height: 2000
    # margin: 0.05
execute:
  enabled: true      # (no) ejecutar code chunks
  eval: false        # por defecto, evalúa y muestra códido de code chunks
  echo: true
  warning: false    # pero no los mensajes ni warnings
  message: false
knitr:
  opts_chunk:
    results: hide     # ni muestra resultados ni figuras
    fig.show: hide
lang: es
strip-comments: true
toc: true
toc-depth: 1
toc-expand: false
toc-title: "Contenidos"
css: styles.css
---

```{r setup, include=FALSE}
#```{r setup, message=FALSE, warning=FALSE, include=FALSE} 
# include=F es suficiente para no incluir mensajes, etc.

# Opciones por defecto para los fragmentos de código
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      results = "hide", fig.show="hide")
# se muestra y evalúa el código,
# no se muestran mensajes, ni avisos (warnings)
# no se muestran los resultados de código (tampoco gráficos)
#     en los códigos que considere necesarios los mostraré

# Elimino todo del Entorno (del documento)
rm(list = ls())       

# Cargo todas las bibliotecas necesarias
# (se podría hacer cuando cada una sea necesaria)
library(tidyverse)
# library(tidymodels)
# library(printr)
# library(skimr)
# library(dlookr)
# library(broom)
library(kableExtra)
# library(rpart.plot)
# library(vip)

#fijo el directorio de trabajo
#setwd("/home/albarran/Dropbox/MAD/00.TEC")
library(rmarkdown)
#render("filename.Rmd")     
#browseURL("filename.html")
```

# Métodos Estadísticos

## Métodos Estadísticos

:::: {.columns}

::: {.column width="40%"}


- La **modelización** mediante métodos estadísticos permite
     
    - Encontrar patrones <!--(complejos), y cuantifificar su fortaleza -->
     
    - Interpretar datos <!--(información)-->

::: 

::: {.column width="55%"}

<center>
![](figure/data-science-model.png)
</center>
::: 

::::

. . . 


* Los datos (casos observados) son una **muestra** de una **población** mayor (casos potenciales)

<!--
<center>
![](figure/pop-muestra.png){width=45%}
</center>
-->

<!--
## Muestreo de la población. Variabilidad muestral

* Sabiendo el número de ventas en un día o visitas por hora a nuestra web podemos planificar la gestión de una tienda, carga de la web, etc.
-->

* Las ventas salen de una población teórica uniforme entre 0 y 20 (media = 10) 

<!--$\chi^2_{(10)}$, con media poblacional 10:-->

```{r}
#| echo: false
#| eval: false
data <- tibble(x = rchisq(n=1e6, df=10))
ggplot(data, aes(x)) + geom_vline(xintercept = 10, color = "red") +
  geom_density() 
```

```{r}
#| echo: true
#| eval: false
tibble(x = runif(n=1e7, min = 0, max = 20)) %>% pull(x) %>% mean()
```


```{r}
#| echo: false
data <- tibble(x = round(rchisq(n=1e6, df=4)))
ggplot(data, aes(x)) + geom_vline(xintercept = 4, color = "red") +
  geom_histogram(aes(y = ..density..), bins = 200) 
```

```{r}
#| echo: false
#| eval: false

data <- tibble(x = round(rchisq(n=1e6, df=4)))
ggplot(data, aes(x)) + geom_vline(xintercept = mean(data$x), color = "red") +
  geom_histogram(aes(y = ..density..), bins = 200) + 
  stat_function(fun = dnorm, col = "green",
                args = list(mean = mean(data$x), sd = sd(data$x)))
```

<!--
* Disponemos de una muestra con información de solo $n=25$ días

```{r}
set.seed(1510)  
muestra <- tibble(x = rnbinom(n=25, size=7, prob=.6))
muestra %>% summary()
```
-->

<!-- OJO: números pseudo-aleatorios y semilla -->

<!--
## Incertidumbre por la distribución del muestreo
* Un *estadístico* es un valor calculado a partir de una muestra (ej., la media). ¿Cómo de fiable es?
-->

* ¿Cómo de fiable es un *estadístico* (ej., la media) calculado en una *muestra*? 


```{r}
#| echo: true
#| eval: false
set.seed(9915)
tibble(x = runif(n=25, min = 0, max = 20)) %>% pull(x) %>% mean()
```

## Variabilidad muestral

* Con **todas** las muestras potenciales de tamaño <!--muestral--> $\small n$ conoceríamos la **distribución muestral** de un estadístico (información calculada en una muestra) 

    * Simulemos la distribución para la <!--el estadístico--> media en muchas muestras de $\small n=25$

:::: {.columns}

::: {.column width="45%"}

```{r}
set.seed(101)
nsim <- 1000
N <- 25
ybar <- numeric(nsim)
```

:::

::: {.column width="55%"}

```{r}
#| echo: false
#| eval: false
for (j in 1:nsim) { 
  muestra <- round(rchisq(n=N, df=4)) 
  ybar[j] <- mean(muestra) 
}
summary(ybar) 
```

```{r}
for (j in 1:nsim) { 
  muestra <- runif(n=N, 0, 20) 
  ybar[j] <- mean(muestra) 
}
```

:::


::::

```{r}
#| echo: false
#| eval: false

set.seed(101)
Muestras_df <- 1:100 %>% map_dfr(~tibble(x = rnbinom(n=25, size=7, prob=.6)) %>% 
                    summarize(media=mean(x), sd=sd(x)))

Muestras <- list()
set.seed(101)
for(i in 1:100){
  Muestras[[i]] <- tibble(x = rnbinom(n=25, size=7, prob=.6))  %>% 
        summarize(media = mean(x), sd = sd(x))
}
Muestras_df <- Muestras %>% bind_rows()
```

<!--
* Existe una gran variabilidad del estadístico muestral: es poco fiable
-->

<!-- ## Distribuciones muestrales -->

. . . 


<!-- * **Distribución muestral** es la distribución del estadístico en **todas** las muestras potenciales de tamaño muestral $n$ -->

  <!-- - nos interesan su *forma*, *error estándar* (variabilidad), etc. -->

<!--
* Nos interesan su *forma*, *error estándar* (variabilidad de la distribución muestral), *intervalo de confianza*, etc.
-->


<!--
    + veríamos cómo varía el valor del estadístico en todas las muestras
-->

<!--
    +  si conocemos las distribución poblacional que genera las muestras, podemos disponer de muestras y ver cómo varía el valor del estadístico
    
    + aquí lo hemos aproximado con 100 muestras 
    
-->

```{r}
as_tibble(ybar) %>% ggplot(aes(x=value)) + geom_density() +
  geom_vline(xintercept = 10)
```

<!--
* Otra forma es el *intervalo de confianza del 95%*

```{r}
#| echo: false
#| eval: false

mean(Muestras_df$media) + sd(Muestras_df$media) * 1.96 * c(-1,1) 
```
-->


* ¿Es posible *cuantificar la incertidumbre* con UNA MUESTRA? 

1. *Suponiendo* que los datos son normales, la media tiene una distribución normal

2. [**Teorema Central del Límite**](https://openintro.shinyapps.io/CLT_mean/): para datos de cualquier distribución,  $\overline{Y} \overset{a}{\sim} N(\mu, \sigma^2/n)$ cuando $n \to \infty$ 
    
    
    <!-- + Hemos visto ejemplos [aquí](https://openintro.shinyapps.io/CLT_mean/)  -->
    
<!--
* ¿Cómo cambia  cuando varia el tamaño muestral?

-->

<!--
## Distribuciones muestrales (cont.)

<!--
* La fiabilidad de un estadístico muestral se mide  por 

  1. la media del estadístico (media de la distribución muestral): debe estar cerca del verdadero (en la población). 

  2. el error estándar del estadístico (error estándar de la distribución muestral); debe ser pequeño. 
-->

<!--
* ¿Cómo cambia la media y el error estándar del estadístico cuando varia el tamaño muestral?

```{r, eval=FALSE}
Muestras_porN <- list()
nvec <- c(25, 50, 100, 200)
for(j in seq_along(nvec)){
  n <- nvec[j]
  Muestras <- list()   # bucle anterior, generalizando n y conservándolo en summarize
  set.seed(101)
  for(i in 1:100){
    Muestras[[i]] <- tibble(x = rlnorm(n = n, meanlog = 4.403, sdlog = 0.409) - 87)  %>%   
        summarize(media = mean(x), sd = sd(x), n = n)      
  }
  Muestras_porN[[j]] <- Muestras %>% bind_rows()
}
Muestras_Final <- Muestras_porN %>% bind_rows()
```

```{r}
#| echo: false
#| eval: false

saveRDS(Muestras_Final, file="data/Muestras_Final.rds")
```
 
```{r}
Muestras_Final <- readRDS("data/Muestras_Final.rds")
```
 
-->

<!--
## Distribuciones muestrales: LGN y TCL

```{r}
#| echo: false
#| eval: false

Muestras_Final %>% ggplot(aes(x=media)) + geom_density() + facet_wrap(~n)
Muestras_Final %>% group_by(n) %>% 
  summarize(media_de_media = mean(media), error = sd(media))
```

- *Ley de números grandes*: para un tamaño de la muestra $n$ grande, el promedio de la muestra está cerca de la media de la población <!--, y el error estándar es pequeño. -->

<!--
- *Teorema de Límite Central*: para un tamaño de la muestra $n$ grande, la distribución muestral de la media es normal.

* $\mbox{Error estándar}(\bar{X}_n) = \frac{\sigma}{\sqrt{n}},$
    donde $\sigma$ es la desviación estándar de la población. 

    * Si **aumentamos el tamaño de la muestra en $n$, el error estándar disminuirá**.

* Demostración práctica: [aquí](https://openintro.shinyapps.io/CLT_mean/)

-->

## Procedimiento *Bootstrap* 

::: {style="font-size: 88%;"} 


<!-- * En la práctica, sólo tenemos UNA muestra de tamaño $n$ (no la población) -->

* **Idea**: pensar en la ÚNICA muestra como si fuera la población 


```{r}
#| echo: true
#| eval: false
set.seed(101)
UNICAmuestra <- tibble(x=runif(n=25, 0, 20))

```

<!-- La muestra es representativa de la población ==> repetir el proceso de extracción de muestras con la única muestra

<!-- Bootstrap genera variabilidad que simula/aproxima la variabilidad muestral -->


1. Tomar muchas *remuestras* (muestras de Bootstrap) con **reemplazamiento**<!--de la original-->

    - p.e., para (1,2,3) incluye los casos (1,1,2), (1,1,3), (2,2,1), etc.
<!--  
    * ${n\choose n}= (n + n - 1)!/(n! (n-1)!)$ remuestras: la muestra original es una combinación entre muchas
-->

```{r}
#| echo: false
#| eval: false
library(rsample)
df <- bootstraps(as_tibble(1:3),times = 15)
```


```{r}
#| echo: true
#| eval: false
library(rsample)
nboot <- 1000
remuestras <- bootstraps(UNICAmuestra, times = nboot)
```

2. En cada remuestra, se puede calcular cualquier estadístico 
    
```{r}
#| echo: true
#| eval: false
distrib <- list()
for (r in 1:nboot) {
  remuestra_r <- remuestras$splits[[r]] %>% as_tibble()
  media_r  <- remuestra_r %>% pull(x) %>% mean()
  sd_r     <- sd(remuestra_r$x)
  distrib[[r]] <- list(medias = media_r, sds =sd_r ) 
}
distribDF <- distrib %>% bind_rows()
```
    
:::


## Procedimiento *Bootstrap* (cont.)

::: {style="font-size: 88%;"} 
    
* Este procedimiento permite generar variación (de remuestras) a partir de una única muestra 

* La **distribución muestral bootstrap** NO es la distribución muestral, pero aproxima sus aspectos principales <!--(*error estándard*)--> sin supuestos (normalidad, TCL)

```{r}
#| echo: true
#| eval: false
distribDF %>% ggplot(aes(x=medias)) + geom_density()

# Error estándar de la media
mediaSE <- distribDF %>% pull(medias) %>% sd()

# Intervalos de confianza al 95% para la media
mediaIC <- distribDF %>% pull(medias) %>% quantile(c(0.025, 0.975))
```

<!--una distribución de valores de los ensayos bootstrap -->
<!-- aproxima para vlaores moderados de n -->

* Puede aplicarse a **cualquier estadístico** (media, varianzas, regresión, etc.)

<!-- es un método estadístico que nos  permite aproximar la distribución muestral sin acceso a la población.-->

```{r}
#| echo: false
#| eval: false

boot_df <- 1:1000 %>% map_dfr(~muestra %>% slice_sample(n=25, replace = TRUE) %>% 
                                summarize(media=mean(x)))
boot_df %>% ggplot(aes(x = media)) + geom_density()  # densidad bootstrap de la media
sd(boot_df$media)                                    # estimación del error estándar
c(sort(boot_df$media)[25], sort(boot_df$media)[975]) # Intervalo de confianza al 95%
```

:::

<!-- ## Procedimiento *Bootstrap* (cont.) -->

::: {style="font-size: 90%;"} 


```{r}
#| echo: false
#| eval: false
library(boot)
set.seed(101)
UNICAmuestra <- tibble(x=runif(n=25, 0, 20))

mediaFunc <- function(datos,  indices) {
  remuestra <- datos[indices,]
  return(mean(remuestra))
}

(bootRes <- boot(data = UNICAmuestra, statistic = mediaFunc, R = 1000))



```

```{r}
#| echo: false
#| eval: false
library(rsample)

set.seed(101)
UNICAmuestra <- tibble(x=runif(n=25, 0, 20))

nboot <- 1000
remuestras <- bootstraps(UNICAmuestra, times = nboot)

distrib <- list()
for (i in 1:nboot) {
  remuestrai <- remuestras$splits[[i]] %>% as_tibble()
  mediai  <- remuestrai %>% pull(x) %>% mean()
  sdi     <- sd(remuestrai$x)
  distrib[[i]] <- list(medias = mediai, sds =sdi ) 
}

distribDF <- distrib %>% bind_rows()
```

```{r}
#| echo: false
#| eval: false
distribDF %>% ggplot(aes(x=medias)) + geom_density()
# Error estándar de la media
mediaSE <- distribDF %>% pull(medias) %>% sd()
# Intervalos de confianza al 95% para la media
mediaIC <- distribDF %>% pull(medias) %>% quantile(c(0.025, 0.975))
```





```{r}
#| echo: false
#| eval: false
set.seed(101)
muestra <- rchisq(n=25, df=10)
nboot <- 1000
boot_muestras <- list()

for (i in 1:nboot) {
  data1 <- muestra %>% 
              as_tibble() %>% 
              slice_sample(n=25, replace = TRUE) 
  m <- mean(data1$value)
  s <- sd(data1$value)
  boot_muestras[[i]] <-  list(media = m, sd = s)
}
boot_datos <- boot_muestras %>% bind_rows()
```

```{r}
#| echo: false
#| eval: false
distribF %>% ggplot(aes(x = medias)) + 
  geom_density()                        # densidad bootstrap de la media
sd(distribF$media)                    # estimación del E.S. de la media
# I.C. al 95% de la media y de la DT
c(sort(distribF$media)[25], sort(distribF$media)[975]) 
c(sort(distribF$sd)[25], sort(distribF$sd)[975])       
```


```{r}
#| echo: false
#| eval: false

set.seed(101)
boot_df <- list()
for (i in 1:1000) {
  data1 <- muestra %>% slice_sample(n=25, replace = TRUE) 
  m <- mean(data1$x)
  s <- sd(data1$x)
  boot_df[[i]] <-  list(media = m, sd = s)
}
boot_df %>% bind_rows() %>% ggplot(aes(x = media)) + geom_density()  

set.seed(101)
mis_estads <- function(n) {
  data1 <- muestra %>% slice_sample(n=n, replace = TRUE) 
  sol <- list(media = mean(data1$x), sd = sd(data1$x))
  return(sol)
}
boot_df <- 1:1000 %>% map_dfr(~mis_estads(25))
boot_df %>% bind_rows() %>% ggplot(aes(x = media)) + geom_density()
```
<!--
## *Bootstrap* con $B = 1000$ para `datos1` 
```{r, eval=FALSE}
n <- nrow(datos1)
boot <- list()
set.seed(101)
for(i in 1:1000){
    boot[[i]] <- datos1 %>% 
      sample_n(size = n, replace = TRUE) %>%
      summarize(media = mean(x))
  }
boot_df <- boot %>% bind_rows()
```

```{r}
#| echo: false
#| eval: false

saveRDS(boot_df, file="data/boot_df.rds")
```
 
```{r}
boot_df <- readRDS("data/boot_df.rds")
```
 

<!-- 
- `sample_n()`  para seleccion filas de forma *aleatoria* de una tabla,
-->

<!--
```{r}
boot_df %>% ggplot(aes(x = media)) + geom_density() # densidad bootstrap de la media
sd(boot_df$media)                                   # estimación bootstrap del error estándar
```
 
```{r,}
#| echo: false
#| eval: false

boot_df %>% ggplot(aes(x = media)) + 
  geom_density() + 
  labs(title = "Distribución Bootstrap de la media de retraso en `datos1`")
```


<!--
* Aunque podemos obtener un mejor intervalo de confianza basado

```{r}
mean(boot_df$media) + c(-1,1) * 1.96 * sd(boot_df$media)
```
-->

<!--
* Sin suponer normalidad o TCL, se obtienen los percentiles del intervalo en la distribución muestral bootstrap:

```{r}
c(sort(boot_df$media)[25], sort(boot_df$media)[975])
```
-->


<!--
#### Errores estándar Bootstrap para muestras grandes

El siguiente código toma muestras para la población `SF` sólo una vez para cada uno de los tamaños muestrales 25--200, y utiliza la muestra (sin referencia a la población) para estimar el error estándar. 

```{r eval=FALSE}
Trials.bootstrap <-list()
nvec = c(25,50,100,200)
for(j in seq_along(nvec)){
  n <- nvec[j]
  sample_df <- SF %>% sample_n(size = n)
  Trials_n <- list() 
  for(i in 1:1000){
    Trials_n[[i]] <- sample_df %>% 
      sample_n(size = n, replace = TRUE) %>%
      summarize(mean = mean(arr_delay), n = n)
  }
  Trials.bootstrap[[j]] <- bind_rows(Trials_n)
}
bind_rows(Trials.bootstrap) %>% 
  group_by(n) %>% 
  summarize(error = sd(mean))
```

Estas estimaciones del error estándar, calculadas mediante bootstrapping de la muestra, se aproximan bastante el error estándar, calculado a partir de la población. 

En este ejemplo, el estadístico muestral es la media. En la práctica, los estadísticos de interés son más complejos, por ejemplo, la estimación del coeficiente en el modelo de regresión lineal. El procedimiento de bootsrapping se puede aplicar a casi todas las situaciones, para cuantificar la incertidumbre en un estadístico.
-->

:::



# Aprendizaje Estadístico

## Aprendizaje Estadístico o Automático

* Aprendizaje automático (*machine learning*, ML) o estadístico (*statiscal learning*): conjunto de técnicas algorítmicas para extraer información de los datos

<!--
<center>
![](figure/ML2.jpg){width=65%}
</center>


## Aprendizaje supervisado vs. no supervisado
-->



<!--
<center>    
![](figure/imgSuper-Unsuper.png){width=55%} 
</center> 
-->

<!--
https://www.favouriteblog.com/wp-content/uploads/2017/07/Types-of-Learning.png):
-->

* [Tipos principales](https://vitalflux.com/wp-content/uploads/2020/12/mind_map_machine_learning_3.jpg) 

1. **Aprendizaje supervisado**: escenarios en los que para cada observación de las mediciones $X_i$ hay una *respuesta asociada* $Y_i$ ("supervisa" el aprendizaje)

    + Aprendemos la respuesta de casos nuevos a partir de casos previos 


2. **Aprendizaje no supervisado**: no hay una respuesta asociada a las mediciones de $X_i$ para supervisar el análisis que generará un modelo.

    + Aprendemos rasgos no medidos a partir de casos "no etiquetados": ej. observaciones similares organizadas en grupos distintos <!--(de clientes, países)-->
    

<!--

supervisado conjunto de datos X Y (variable especial) Y=F(X) aprender F(), relación, para predecir

no supervisado: conjunto datos X identifcar patrones- geiser = tipos de clientes

-->

## Aprendizaje supervisado

::: {style="font-size: 90%;"} 


$$
\small Y = f(X) + \varepsilon
$$

* Modelo para la variable dependiente (de respuesta) en función de factores observados (predictores/características) y no observados ($\small \varepsilon$)

    <!--Y= variable objetivo)-->
    <!--X= independientes (predictores, características, regresores, factores)-->
    <!--X= inputs, features, covariates -->
    
    + $f$ representa la información/relación sistemática que $X$ (género, educación, etc.) ofrecen sobre un resultado medido $Y$ (ej. renta)

* Objetivos:  

1. Predecir el valor esperado de $\small Y$ para casos *nuevos* <!-- a partir de otros previamente etiquetados  (medidos/clasificados)--> 
    
2. Comprender cómo afectan al resultado esperado de $\small Y$ cambios en los valores de las características

    + p.e., tiene la experiencia un efecto positivo o nula sobre la renta esperada

    + ¡cuidado con afirmaciones de *causalidad*!

<!--
    + evaluar la calidad de nuestras predicciones e inferencias
-->

* En ambos casos, tenemos que estimar ("aprender") la $f$ desconocida

:::


## Aprendizaje supervisado (cont.)


<!--    
## Aprendizaje supervisado: estimar $f$ desconocida
-->

* Dos formas de especificar la relación $f$

  * **Modelo paramétrico:** supone un forma de $f$ que depende de parámetros desconocidos, p.e., lineal  $f(x) =\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k$
  
  * **Modelo no paramétrico:** ajustar $f$ a los datos sin supuestos funcionales
  
    - más flexibilidad y mejor ajuste, pero más difícil de estimar e interpretar    

* La capacidad predictiva de un modelo mejora si incluimos más variables explicativas (modelo más flexible)


<!--
    + Es más sencillo estimar parámetros que una función arbitraria

  * A mayor flexibilidad, mejor ajuste, PERO existe una disyuntiva entre precisión de la predicción e **interpretabilidad** (inferencia)
-->
  
<!--
* A veces solo nos interesa predecir el resultado a partir de unos factores
-->    

<!--
* Preferimos un método más restrictivo si no solo interesa predecir, sino *entender* la manera en que $X$ afecta a $Y$ 

    + variables relevantes y su signo y magnitud, 
    + generar hipótesis, etc.
-->

<!-- ## Problemas de "regresión" y de clasificación -->

<!--
* El método de aprendizaje supervisado más adecuado para un problema depende de si la respuesta es cualitativa o cuantitativa.
-->

* Dependiendo de la naturaleza de la variable dependiente, tenemos:

    1.- **Problemas de Regresión**: variable de respuesta cuantitativa (toma valores numéricos)

    2.- **Problemas de Clasificación**: variable de respuesta cualitativa (toma valores en una de $C$ categorías o clases)

```{r}
#| echo: false
#| eval: false
library(mosaicData)
mod <- lm(volume ~ poly(hightemp,3), data = RailTrail)
cbind(RailTrail$volume, mod$fitted, RailTrail$hightemp) %>% head()
```
    

```{r}
#| echo: false
#| eval: false
censo <- read_csv("data/census.csv") %>%
  mutate(income = as.integer(factor(income))-1)
logit <- glm(income ~ capital_gain, data = censo, family = "binomial")
cbind(censo$income, predict(logit, type = "response")) %>% head()
```


<!--
No trataremos todas las técnicas ni podemos entrar en el fondo de cada técnica.  

El objetivo es proporcionar una visión general de alto nivel de las técnicas y modelos empleados habitualmente y así comprender los objetivos generales del aprendizaje automático.
-->

<!--
## Ejemplo de regresión

* Predecir el número de usuarios (`volume`):

```{r}
library(mosaicData)
RailTrail %>% ggplot(aes(x = hightemp, y = volume)) + 
  geom_point() + geom_smooth(method = 'lm', formula = y ~ poly(x,3) ) +
  coord_cartesian(ylim = c(100,750))
```

* `volume` "supervisa" el ajuste del modelo

* Podemos usar el modelo para predecir `volume`

```{r}
RailTrail.fit <- RailTrail %>% 
  mutate(
    lm.fit = lm(volume ~ poly(hightemp,3), data = .)$fitted) 
head(select(RailTrail.fit, volume, lm.fit, hightemp))
```

```{r}
#| echo: false
RailTrail.fit %>%
  ggplot() + 
  geom_point(aes(x = hightemp, y = volume)) + 
  geom_line(aes(x = hightemp, y = loess.fit), color = "blue")
```
-->

<!--
## Ejemplo de clasificación

* Clasificación del tipo de flor en los [datos Iris](https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos)

```{r} 
iris %>% ggplot(aes(x = Petal.Length, y = Species, color = Species))  +
  geom_point()
```

* Clasificación rudimentaria en función de longitud del pétalo:

  * Setosa, si < 2 
    
  * Versicolor, si >2 y <5
  
  * Virginica, si > 5
     
```{r}
#| echo: false
iris.crude <- iris %>% mutate(true.Species = Species) %>%
  mutate(
    predicted.Species = 
      ifelse(Petal.Length < 2, "setosa", 
             ifelse(Petal.Length < 5, "vesicolor", 
                    "virginica"))
    ) 

iris.crude %>% 
  ggplot(aes(x = Petal.Length, y = true.Species,  
             color = true.Species, shape = predicted.Species)) + 
  geom_point()
```


* La predicción es mucho mejor si se utilizan más variables


* La visualización tiene un poder limitado: debemos usar métodos de clasificación basados en modelos o algoritmos.

-->

<!--
## Ejemplo de clasificación

* Factores para predecir si un cliente potencial es de alto ingreso
```{r}
censo <- read_csv("data/census.csv") %>%
  mutate(income = as.integer(factor(income))-1)
```

* Ajustamos un modelo logístico (logit) simple
```{r}
modelo_logistico <- glm(income ~ capital_gain, data = censo, family = "binomial")
summary(modelo_logistico)
cbind(censo$income, predict(modelo_logistico, type = "response")) %>% head()
```


* La predicción mejora si incluimos más variables explicativas (modelo más flexible)
-->

<!--
```{r, eval=FALSE, echo=FALSE}
modelo_logistico2 <- glm(income ~ capital_gain + age + education + sex, 
                         data = censo, family = "binomial")
summary(modelo_logistico2)
cbind(censo$income, predict(modelo_logistico2, type = "response")) %>% head()
```
-->

<!--
## Ejemplo de aprendizaje no supervisado

<!--
Usamos técnicas en el aprendizaje no supervisado cuando no hay ninguna variable de respuesta. Simplemente tenemos un conjunto de observaciones $X$, y queremos entender las relaciones entre ellos.
-->

<!--

* *Clustering*  (agrupamiento o particionamiento): identificar grupos desconocidos de casos a partir de características observadas

<!--
Tiempo de espera entre erupciones y sobre duración de la erupción para el géiser Old Faithful en el Parque Nacional de Yellowstone, EE.UU.

![https://en.wikipedia.org/wiki/Old_Faithful](figure/OldFaithful1948.jpg)
-->

<!--
* Tiempo de espera entre erupciones y sobre duración de la erupción para el géiser Old Faithful
```{r echo=FALSE, eval=FALSE}
library(tidyverse)
head(faithful)
```


```{r}
faithful%>% ggplot(aes(y = eruptions, x = waiting)) + 
  geom_point()
```

* Se pueden apreciar dos "grupos" o *clusters* o tipos de erupciones.

```{r}
faithful.clustered <- 
  faithful %>% mutate(cluster = factor(kmeans(x = ., centers = 2)$cluster))
faithful.clustered %>% ggplot(aes(y = eruptions, x = waiting)) + 
  geom_point(aes(color = cluster))
```

-->



## Modelo de Regresión Lineal

<!--
::: {style="font-size: 85%;"} 

* Es un modelo paramétrico para problemas de regresión que supone una relación lineal entre $\small Y$ y $\small X$: $\small \color{blue}{Y=\beta_0+\beta_1 X_1+ \dots + \beta_k X_k + \varepsilon}$

* Los parámetros del modelo se interpretan como

    * $\small \beta_0$ (constante): valor esperado de $\small Y$ cuando $\small X_1=X_2=\dots=X_k=0$

    * $\small \beta_j$ (pendiente de la línea): cambio medio en $\small Y$ por un incremento de una unidad en $\small X_j$ (para $\small j=1,...,k$), *ceteris paribus*

:::

. . .

::: {style="font-size: 85%;"} 

* Ejemplo: $\small ventas = \beta_0 + \beta_1 renta + \beta_2 descuento + u$

    * $\small \beta_0= E(ventas|renta=0, descuento=0)$: ventas esperadas cuando la renta es 0 y el individuo no tiene descuento.

    * $\small \beta_1 = \frac{\delta E(ventas|renta,descuento)}{\delta{renta}}$: aumento esperado de las ventas cuando la renta aumenta en una unidad, dado un valor de descuento
    
      +  $\small \beta_1 = E(ventas|renta=x+1,descuento=d)-E(ventas|renta=x,descuento=d)$

    * $\small \beta_2 = E(ventas|renta=x,descuento=1)-E(ventas|renta=x,descuento=0)$: ventas medias de los individuos con descuento respecto a los que no lo tienen, dada un valor de la renta

:::
--->

::: {style="font-size: 85%;"}

* Es un modelo paramétrico para problemas de regresión que supone una relación lineal entre $\small Y$ y $\small X$: $\quad \small \color{blue}{ventas = \beta_0 + \beta_1 renta + \beta_2 descuento + u}$


<!-- * Los parámetros del modelo se interpretan como: -->

1. La constante $\small \color{blue}{\beta_0= E(ventas|renta=0, descuento=0)}$: valor esperado de $\small Y$, ventas, cuando todas las variables explicativas son cero

    + en este caso, la renta es 0 y el individuo no tiene descuento

2. La pendiente de una variable continua $\small \color{blue}{\beta_1 = \frac{\delta E(ventas|renta,descuento)}{\delta{renta}}}$: cambio esperado de $\small Y$, ventas, cuando la variable explicativa, renta,  aumenta en una unidad, manteniendo el resto de variables constantes (en este caso, un valor dado para descuento)
    
      +  en algunos caso también puede verse como $\small \beta_1 = E(ventas|renta=x+1,descuento=d)-E(ventas|renta=x,descuento=d)$

3. Para variables discretas binarias  $\small \color{blue}{\beta_2 = E(ventas|renta=x,descuento=1)-E(ventas|renta=x,descuento=0)}$: diferencia del valor esperado de $\small Y$, ventas, para el grupo indicado por la variable (tienen descuento) respecto al grupo de referencia (no tienen descuento), mateniendo el resto de variables (en este caso, renta) constante

:::

## Regresión Lineal: Estimación

::: {style="font-size: 95%;"} 

* Objetivo: estimar los coeficientes poblacionales desconocidos a partir de una muestra 


* El error de estimación o **residuo** es $\small \hat{e}_i = y_i - \hat{y}_i$, donde la predicción a partir del modelo estimado es $\small \hat{y}_i=\hat{\beta}_0+\hat{\beta}_1 X_1+ \dots + \hat{\beta}_k X_p$ 

* Los coeficientes estimados son los que minimizan la Suma Cuadrática de Residuos: la suma total de distancias entre los datos observados y predichos  


:::: {.columns}

::: {.column width="55%"}

<center>
![](figure/fig_E2b.jpeg){width=100%}
</center>

::: 

::: {.column width="40%"}

$$
\small SCR=\sum_{i=1}^{n} \hat{e}_i^2= \sum_{i=1}^{n} ( y_i - \hat{y}_i)^2
$$

<!-- + Por tanto, también minimiza $\small ECM = \frac{SCR}{n} = MSE$ -->

::: 

::::

* Esto equivale a las condiciones derivadas de suponer $\small E(\varepsilon|X)=0$

:::
   
## Regresión Lineal: Estimación (cont.)

* Los parámetros del modelo $f(.)$ estimado, en este caso los $\small \beta_j$  son estadísticos con **variabilidad muestral**, medida por  su **error estándar** $\small se(\widehat{\beta}_j)$ <!--mide la precisión del coeficiente estimado -->

<!--
:::: {style="display: flex;"}

::: {}

$$
\small se(\widehat{\beta}_j) =  \frac{\sigma^2}{(n-1)S^2_{x_j} (1 - R^2_{j})}
$$
:::

::: {}
\ \ \ 
:::

::: {}

+ $\small \sigma^2=Var(\varepsilon)$, estimada con $\small \frac{SCR}{n-k-1}$
+ $\small S^2_{x_j}=\frac{\sum (x_{ij}-\bar{x}_j)^2}{n-1}=$varianza muestral de $\small X_j$
+ $\small R^2_{j}$ es el $\small R^2$ de la regresión de $\small X_j$ sobre el resto de regresores

<!-- \frac{\sigma^2}{SCT_j (1 - R^2_{j})} = -->

<!--
:::

::::
-->

* Se usan para construir intervalos de confianza y <!--estadísticos--> para contrastar hipótesis sobre los parámetros poblacionales, p.e., significatividad ($\small \beta_1=0$)

<!--    
    + individual: $\small H_0: \beta_1=0$ con un estadístico $\small t=\frac{\widehat{\beta}_1-0}{se(\widehat{\beta}_1)}$
    + conjunta: $\small H_0: \beta_1=\beta_2=\dots=\beta_k=0$ con un estadístico $\small F$
-->

<!-- * Medidas de la precisión del modelo: $\small MSE$ o $\small R^2=1-\frac{SCR}{SCT} = \frac{SCE}{SCT}$  -->

* En R, usamos la función `lm()`

```{r}
descuento <- read.csv("data/descuento.csv") %>% 
              mutate(zona = parse_factor(zona))
modelo <- lm(data = descuento, ventas ~ renta + descuento)
summary(modelo)
```

* La predicción $\widehat{y}$ también está sujeta a incertidumbre por la estimación: se puede calcular su error estándar e intervalos de confianzas

```{r}
ventas_pred <- predict(modelo, type = "response")
cbind(descuento$ventas, ventas_pred) %>% head()
```

<!--   
## Regresión Lineal: Precisión del modelo

* Medición de la precisión del modelo: 

  + RMSE o variantes como el error estandar de la regresión, $\small \sqrt{\frac{SCR}{n-k-1}}$: son   medidas absolutas en las mismas unidades que $Y$ 
  + $\small R^2=1-\frac{SCR}{SCT} = \frac{SCE}{SCT}$ es una medida relativa: proporción de la varianza explicada por el modelo

 
* La predicción $\widehat{y}$ también están sujetas a incertidumbre por la estimación

  + Existen formulas para calcular su error estándar (a partir de la matriz de varianza y covarianza de los coeficientes estimados) 
  + Por tanto, también se puede calcular un intervalo de confianza

```{r}
res <- lm(data = datos, sales ~ discount + income)
cbind(datos$sales, res$fitted.values) %>% head()
```
-->

<!--
## Modelización Condicional. Causalidad 

* La población NO suele ser *homogenea*: más visitas ciertos días, horas, etc.
-->

<!--
* Los retrasos son más frecuentes a determinadas horas 

```{r eval=FALSE}
SF %>%
  ggplot(aes(x = hour, y = arr_delay)) +
  geom_boxplot(aes(group = hour)) + geom_smooth(method = "lm")  +
  coord_cartesian(ylim = c(-30, 120))

mod1 <- lm(arr_delay ~ hour, data = SF)
broom::tidy(mod1)
```

* En particular, los grandes retrasos (superior a 60 minutos)
```{r eval=FALSE}
SF %>% mutate(gran_retraso = arr_delay > 60) %>%
  ggplot(aes(x = hour, fill= gran_retraso)) + 
  geom_bar(position = "fill")                  # para representar frecuencia
```

## Modelización Condicional (cont.)

<!--
* También tenemos variación por mes, día de la semana, aeropuerto de origen, aerolínea, etc.
-->

<!-- * Un modelo de regresión permite incluir todas las **variables explicativas** de la variación de la variable dependiente -->

<!--
```{r echo=TRUE, eval=FALSE}
library(lubridate)
SF2 <- SF %>% 
  mutate(
    fecha  = as.Date(time_hour), 
    diasem = as.factor(wday(day, label = TRUE)),
    estaci = ifelse(month %in% 6:7, "verano", "otros meses")
  )

mod2 <- lm(arr_delay ~ hour + origin + carrier + as.factor(month) + dow, data = SF2)
broom::tidy(mod2)
```

* Para una variable de respuesta binaría como `gran_retraso` puede utilizar la *regresión logística* (logit)

-->

<!--
## "Confounding factor" y Causalidad

 * **"Correlación no implica causalidad"**, salvo en [ensayos científicos aleatorios](https://es.wikipedia.org/wiki/Prueba_controlada_aleatorizada) cuidadosamente controlados 
   + en otros campos como marketing digital o analítica de web se denominan [pruebas A/B](https://es.wikipedia.org/wiki/Prueba_A/B) (ej. dos versiones de una misma web) 


 * En general (datos observacionales), nos preocupa que otros factores que puedan ser los verdaderos determinantes de la relación observada 
 
-->

<!--
* En los ensayos aleatorios se controla quien recibe una intervención (tratamiento) y quien no (control)

    + En promedio, todos los demás factores están equilibrados entre los dos grupos: las diferencias pueden atribuirse a la aplicación del tratamiento

    +  Pero si los sujetos no cumplen con los tratamientos o se pierden en el seguimiento...
-->

<!--
 ## "Confounding factor": Descuentos y ventas
-->

```{r}
#| echo: false
#| eval: false

# Evitar notación científica
options(scipen = 999)

# Configuración inicial
rm(list = ls())
set.seed(404)

# Número de observaciones
n <- 404

# Generación de variables
renta <- runif(n, min = 500, max = 3000)  # Renta entre 500 y 25000
descuento <- plogis(0-.75*(renta-500)/2500)
descuento <- as.integer(descuento > quantile(descuento, 0.75))

zona <- sample(c("pueblo", "ciudad", "capital"), size = n, replace = TRUE, prob = c(0.2, 0.5, 0.3))


# Creación de la variable de ventas que depende de renta y descuento
ventas <- -500 + 0.95 * renta -0.0002 * renta^2 + 100 * descuento + 
  100 * descuento * (renta < 2500) + 150 * descuento * (renta < 1000) +
  100 * (zona == "ciudad") + 500 * (zona=="capital") + 
  rnorm(n, mean = 0, sd = 200)

# Crear data frame
datos <- data.frame(ventas, renta, descuento, zona)

# Exportar el data frame a un archivo CSV
write.csv(datos, "data/descuento.csv", row.names = FALSE)

# Regresiones
modelo1 <- lm(ventas ~ renta + I(renta^2) + descuento + descuento:renta  + zona , data = datos)
summary(modelo1)

modelo2 <- lm(ventas ~ descuento, data = datos)
summary(modelo2)

modelo2 <- lm(ventas ~ descuento + renta, data = datos)
summary(modelo2)

```

<!--

```{r}
#| echo: false
#| eval: false
datos <- read_csv("data/discount.csv")SA
```

* El porcentaje medio de descuentos afecta negativamente a las ventas 

```{r}
#| echo: false
#| eval: false

datos %>% ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm")
lm(data = datos, sales ~ discount) %>% broom::tidy()
```

```{r}
#| echo: false
#| eval: false

datos %>% ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm")
lm(data = datos, sales ~ discount) %>% summary()
```

 * Pero si tenemos en cuenta la renta... 

```{r}
#| echo: false
#| eval: false

datos %>% mutate(renta_baja = income < 7500) %>%  
  ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm") +
  facet_wrap(~renta_baja)
lm(data = datos, sales ~ discount + income) %>% summary()
```
-->

## Regresión Lineal: superando la linealidad

* Se pueden incluir transformaciones no lineales de las variables del modelo 

<!--, como logaritmos o incluir términos polinomiales-->

* La interpretación del cambio de un regresor sobre $\small Y$ es diferente
```{r}
library(ISLR)
data(Carseats)
lm(data=Carseats %>% filter(Sales != 0), log(Sales) ~ poly(Advertising,2))
```
* Otra forma de permitir no linealidades es discretizando variables continuas: permite efectos "escalón" diferentes para distintos valores

```{r}
lm(data=Carseats , Sales ~ cut_width(Advertising, 10)) %>% summary()
```

* Se incluyen indicadores binarios para cada clase excepto uno
  + la constante recoge el valor medio de $\small Y$ para ese grupo de referencia
  + cada coeficiente recoge el efecto adicional de su grupo sobre $\small Y$ 


```{r}
#| echo: false
#| eval: false
datos <- read_csv("data/discount.csv")
datos %>% mutate(renta_baja = income < 7500) %>%  
  ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm") +
  facet_wrap(~renta_baja)
datos2 <- datos %>% mutate(renta_baja = income < 7500)

lm(data=datos2, sales ~ discount + renta_baja) %>% summary()

lm(data=datos2, sales ~ discount + renta_baja + discount:renta_baja) %>% summary()

lm(data=datos2, sales ~ discount * renta_baja ) %>% summary()


lm(data=datos2, sales ~ discount:renta_baja) %>% summary()
```

<!--
## Regresión Lineal: Variables cualitativas

* Sólo dos valores: se incluye un regresor un indicador binario $\small D$
  + la constante recoge el valor esperado de $\small Y$ para el grupo con $\small D=0$
  + el coeficiente del indicador indica el efecto adicional (respecto al otro grupo) en el valor esperado de $\small Y$ para el grupo con $\small D=1$

```{r,}
#| echo: false
#| eval: false

library(ISLR)
lm(data=Carseats, Sales ~ Advertising + Urban)
```
* Con varias categorías: se incluyen indicadores binarios para cada clase excepto uno, el grupo de referencia
  + la constante recoge el valor medio de $\small Y$ para el grupo de referencia
  + cada coeficiente recoge el efecto adicional sobre $\small Y$ de su grupo

```{r}
library(ISLR)
lm(data=Carseats , Sales ~ Urban + ShelveLoc)
```

* Aunque R los genera automáticamente para factores, también se pueden crear con `if_else()` o con el paquete `fastDummies` 

<!--
Now, before summarizing this R tutorial, it may be worth mentioning that there are other options to recode categorical data to dummy variables. For instance, we could have used the model.matrix function, the dummies package, and the step_dummy (recipes package).
-->


<!--
## Superando el supuesto de linealidad

* Se pueden incluir transformaciones no lineales de las variables del modelo, como logaritmos o incluir términos polinomiales

* La interpretación del cambio de un regresor sobre $\small Y$ es diferente
```{r}
lm(data=Carseats %>% filter(Sales != 0), log(Sales) ~ poly(Advertising,2))
```

<!-- https://stackoverflow.com/questions/24198013/significance-of-i-keyword-in-lm-model-in-r -->


<!--
* Otra forma de permitir no linealidades es discretizando variables continuas: permite efectos "escalón" diferentes para distintos valores

```{r}
lm(data=Carseats , Sales ~ cut_width(Advertising, 10)) %>% summary()
```
-->

## Regresión Lineal: superando la linealidad (cont.)

<!--
https://www.econometrics-with-r.org/8-3-interactions-between-independent-variables.html
-->

```{r}
#| echo: false
datos <- read_csv("data/discount.csv")
datos2 <- datos %>% mutate(renta_baja = income < 7500)
datos2 %>%  
  ggplot(aes(x=discount, y=sales)) + geom_point() + geom_smooth(method = "lm") +
  facet_wrap(~renta_baja)
lm(data = datos2, sales ~ discount * renta_baja) %>% summary()

datos3 <- datos %>% mutate(grupos_renta = if_else(income < 5000, 1, 
                                                  if_else(income < 12000,2, 3)))
table(datos3$grupos_renta)
lm(data = datos3, sales ~ discount * as.factor(grupos_renta)) %>% summary()
res <- lm(data = datos3, sales ~ discount * as.factor(grupos_renta)) 
confint(res)
```


* También podemos incluir interacciones entre variables: el efecto de un regresor dependerá de otro regresor

```{r}
lm(data=Carseats , Sales ~ Advertising*Income)
```

* Principio jerárquico: al incluir una interacción *siempre* deben incluirse los factores principales (NO sólo `Advertising:Income`)

* Cuando interactuamos un regresor continuo y uno binario, permitimos que la pendiente del primero sea diferente para cada grupo

```{r}
lm(data=Carseats , Sales ~ (Income + Advertising)*Urban)
```

* La interacción de dos variables binarias tiene una interpretación similar <!--, para el efecto esperado de $\small Y$-->

```{r}
lm(data=Carseats , Sales ~ ShelveLoc*Urban)
```

## "Problemas" del Modelo de Regresión Lineal

* *No linealidad*: incluir transformaciones no lineales <!--de los regresores o la variable dependiente-->

* *Correlación de los errores*: afecta a los errores estándar, no la estimación
  + usar errores estándar robustos o modelizar la dinámica
  
* *Heterocedasticidad*: ídem, usar errores estándar robustos <!-- o pesos -->
  + los gráficos de los residuos frente a un regresor o valores predichos: ¿heterocedasticidad o no linealidad? 
  <!--logaritmos o raíz cuadrada cuando hay una amplifiación de los residuos -->
  
* *Outliers* en la variable de respuesta o en los regresores
<!--: se puede intentar detectar con algunos criterios ("arbitrarios": ej. $\small \left|\frac{\hat{e}_i}{se(\hat{e}_i)}\right|>3$)-->

* *Colinearidad*: indica que no es posible separar el efecto de cada regresor: eliminar alguno o recombinarlos

* *No normalidad*: TCL, Bootstrap,...

* El único supuesto realmente importante es $\small E[\varepsilon|X]=0$

<!--
    afecta a la varianza del estimador de un coeficiente, pero NO a significatividad conjunta
    + realmente indica que no es posible separar adecuadamente el efecto de varios regresores por separado: eliminar alguno o recombinarlos -->

<!--
  + se suele usar criterios como VIF (*variance inflation factor*=1/(1-R^2_j)$=ratio de la varianza del estimador en regresion multiple / reg. simple) 
  si VIF>5 o 10
-->


<!--
## Selección de variables 

<!--
* En el análisis exploratorio, encontramos variables significativamente correlacionadas con la variable de respuesta
-->


<!--
* NO se pueden examinar todos los posibles modelos que incluyen combinaciones de los regresores significativamente correlacionados <!--(incluyendo interacciones) -->


<!--
* Procesos automatizados frecuentemente utilizados:

    + Selección hacia adelante (*forward selection*): añadir uno a uno los regresores más correlacionados <!--(criterio de parada en la mejora de SCR o p-valores.-->

<!--
    + Selección hacia atrás (*backward selection*): comenzando con todos, se va eliminando el regresor con mayor p-valor <!--(hasta criterio de parada)-->

<!--
    + Selección mixta: se añaden regresores uno a uno, pero en cada iteración se puede eliminar alguno si su p-valor excede un umbral

* No tienen criterio riguroso ni llevan a la misma solución. Además, se suelen utilizar en la misma muestra donde se estima

-->

## Regresión Logística

* La regresión lineal puede usarse respuestas binarias (no más de dos categorías),
<!--
$$
\small
\Pr(Y=1|X)=\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k = z
$$
-->
aunque genera predicciones fuera del rango $\small [0,1]$

<!--
* Además, no es adecuado con más de dos categorías
-->
  
* Solución: aplicar al índice lineal una transformación $\small F(z)\in[0,1]$


:::: {style="display: flex;"} 

::: {}

*  La función logística: $\small \Lambda (z)=\frac{e^z}{1+e^z}$
::: 

::: {}
```{r, echo=FALSE, fig.show='asis', fig.height=3}
#install.packages("latex2exp")
library(latex2exp)
Logistic <- function(x) {exp(x)/(1+exp(x))}
ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
    stat_function(fun = Logistic) + scale_x_continuous(limits = c(-5, 5), name = "z") +
  scale_y_continuous(name = TeX("$\\Lambda(z)$")) #+
  annotate("text", x = -3 , y = 0.7, label = TeX("$\\Lambda(z)=\\frac{exp(z)}{1+exp(z)}$", output='character'), parse=TRUE)
```

:::

::::
* De manera que $\small \Pr(Y=1|X)= p(x)= \Lambda( \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)$


## Regresión Logística (cont.)

* Los coeficientes NO se interpretan como cambios en la probabilidad ante cambios unitarios en un regresor (efecto marginal sobre la probabilidad)

* PERO su signo (y significatividad) son los mismos que los del efecto marginal 

<!--
* En esta especificación, la probabilidad relativa ("odd") es 
$$
\small
\frac{p(x)}{1-p(x)}=exp(\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)
$$

* Por tanto, su logaritmo ("log odd" o logit) es lineal: los coeficientes son la elasticidad de la probabilidad relativa

## Regresión Logística: estimación 
-->

* Como NO tiene sentido minimizar la SCR, el objetivo es maximizar la probabilidad (verosimilitud) de observar los unos y ceros en los datos

<!--
$$
\small
\ell(\beta_0, \beta_1, \dots, \beta_k)=\prod_{i:y_i=1}p(x_i) \prod_{i:y_i=0} \left(1 - p(x_i)\right)
$$
-->

* La regresión logística pertenece a la familia de modelos lineales generalizados (GLM, en inglés) 

<!--
* En R, la regresión logística se puede estimar con la función `glm()`
-->

* Se pueden incluir como variables explicativas tanto variables cuantitativas como cualitativas, e incurrir en sesgo por omisión de variables

```{r}
glm(data = Default, default ~ student, family = "binomial" ) %>% summary()
glm(data = Default, default ~ student + balance, family = "binomial" ) %>% summary()
```

## Regresión Logística: Predicciones

* El objeto de R de `glm()` contiene valores predichos, que son probabilidades de $\small Y=1$

```{r}
logit <- glm(data = Default, default ~ balance*student, family = "binomial" ) 
cbind(Default$default, logit$fitted)
```

<!--
* También podemos usar la función `predict()` aplicada al objeto de `glm()` para predecir

    - el índice lineal subyacente
    
    - la probabilidad
    
```{r}
cbind(Default$default, logit$fitted, predict(logit), predict(logit, type="response"))
```
-->

* También se puede predecir usando una muestra distinta de la usada para estimar o con valores concretos de los regresores

```{r}
logit <- glm(data = Default, default ~ balance, family = "binomial" )
predict(logit, newdata = tibble(balance=c(0,100)), type="response")
```


<!--
## Variables explicativas

* Se pueden incluir como variables explicativas tanto variables cuantitativas como cualitativas

```{r}
glm(data = Default, default ~ student, family = "binomial" ) %>% summary()
glm(data = Default, default ~ student + balance, family = "binomial" ) %>% summary()

```


* También transformaciones no lineales de estas e interacciones


* Podemos incurrir en un sesgo por omisión de variables relevantes: ej., el efecto de `student` por omitir `balances` (con la que está correlacionada)
```{r}
glm(data = Default, default ~ student + balance, family = "binomial" ) %>% summary()
```
-->

## Regresión logística con más de dos clases

* La regresión logística se puede generalizar a situaciones con múltiples clases (modelos multinomiales) con un índice lineal para cada clase
$$
\small
\Pr(Y=c|X)=\frac{e^{\beta_{0c}+\beta_{1c}X_1+\dots+\beta_{kc}X_k}}{\sum_{l=1}^{C}e^{\beta_{0l}+\beta_{1l}X_1+\dots+\beta_{kl}X_k}}
$$

* La librería `glmnet()` permite la estimación de estos modelos

::: {style="font-size: 90%;"} 

```{r}
library(glmnet)
x <- model.matrix(Species ~ Sepal.Length + Sepal.Width, data = iris)
mod.glmnet <- glmnet(x = x, y = iris$Species, family = "multinomial", 
                     lambda = 0, type.multinomial = "grouped")
coef(mod.glmnet) 
predict(mod.glmnet, newx=x, type = "response")  # probabilidad de cada clase
predict(mod.glmnet, newx=x, type = "class")     # clase
```

```{r}
#| echo: false
#| eval: false

library(glmnet)
iris.x <- as.matrix(iris[1:2])
iris.y <- as.matrix(iris[5])
mod.glmnet <- glmnet(x = iris.x, y = iris.y, family = "multinomial", 
                     lambda = 0, type.multinomial = "grouped")
coef(mod.glmnet) 

predict(mod.glmnet, newx=iris.x, type = "response")  # probabilidad de cada clase
predict(mod.glmnet, newx=iris.x, type = "class")     # clase

d <- coef(mod.glmnet) %>% reduce(cbind) 
colnames(d) <- names(c)
d
```



```{r}
#| echo: false
#| eval: false

library(nnet)
mod.nnet <- multinom(
    Species ~ Sepal.Width + Petal.Length + Petal.Width, # Species ~ .
    data = iris
)
mod.nnet
```

:::

# Evaluación de modelos (por su capacidad predictiva)

## Error de predicción

* Un modelo es mejor si sus predicciones se ajusten mejor a las observaciones

* El error de predicción es $y - \widehat{y} = f(X) - \widehat{f}(X)  + \varepsilon$

  + $f - \widehat{f}$ = error reducible (eligiendo modelo)

  + $\varepsilon$ = error irreducible (variables no observadas) 

* La **función de pérdida (o coste)** evalúa cómo valoramos las desviaciones 
```{r figures-side}
#| echo: false
#| layout-ncol: 2
#| fig.show: hold
#| eval: true

#
# #| out.width="50%"
# #| fig.height=4
library(latex2exp)
curve(x^2, from = -2, to = 2, xlab = TeX("$y-\\hat{y}$"), ylab = "Coste")
curve(abs(x), add = TRUE, col = "blue")
legend('top',legend = c("x^2", "abs(x)"), text.col = c("black","blue"))

curve(0 * (x>=0) - x * (x<0), from = -2, to = 2, xlab = TeX("$y-\\hat{y}$"), ylab = "Coste", col = "red")
curve(0.25 * (x> 0 & x<0.5) + 0.75 * (x>0.5) + 0.75 * (x>1.5) + 0.25 * (x < -1), add = TRUE, col = "black")
```

## Métricas de error de predicción (cuantitativa)

* **Mean Square Error** (Error Cuadrático Medio): $\small MSE(y,\widehat{y})={\frac{1}{n}\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$

    + penaliza grandes desviaciones

    + $\small R^2$ y $\small R^2$-ajustado son variantes del MSE, y solo sirven para comparar modelos con la *misma variable dependiente*.
    
  <!-- $R^2$-ajustado penaliza por número de variables -->

* **Root Mean Square Error**: $\small RMSE(y,\widehat{y})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$

    + mismas unidades que $\small y$

* **Mean Absolute Error**: $\small MAE(y,\widehat{y})=\frac{1}{n}\sum_{i=1}^{n}\left|y-\widehat{y}\right|$

<!--     + también mediana -->

 <!--
* *Correlación* lineal o de rangos entre $\small y$ y $\small \widehat{y}$ 
 

      + lineal ($y$ y $\widehat{y}$ pueden no tener las mismas unidades y escala como con RMSE y MAE) 
      + de rangos ($y$ y $\widehat{y}$ solo tiene que tener el mismo orden relativo, no minimizar distancia entre ellas)
      
* *Coeficiente de determinación*
-->

* Otras medidas basadas en distintas funciones de pérdida, la verosimilitud del modelo ($\small AIC$, $\small BIC$), etc

<!-- AIC, BIC ajustan por el número de parámetros -->

<!-- https://yardstick.tidymodels.org/reference/rmse.html -->



## Seleccionar el mejor modelo

<!--
<center>
![](figure/overfitting01c.png){width=85%}
</center>
-->


```{r}
#| echo: false
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() + 
  geom_smooth(method = "lm", formula = y ~ poly(x,2), se = FALSE) + 
  coord_cartesian(ylim = c(10,30))
```

* ¿Podemos predecir el número de visitantes en función de la temperatura?

```{r}
library(mosaicData)
data(RailTrail)
RailTrail %>% ggplot(aes(x = hightemp, y = volume)) + 
  geom_point() +  geom_smooth()
```

* ¿Cuál es el mejor modelo?

  * $\small volume = \beta_0 + \beta_1 hightemp + \varepsilon$

  * $\small volume = \beta_0 + \beta_1 hightemp + \beta_2 hightemp^2 + \varepsilon$
  
  * ...

  * $\small volume = \beta_0 + \beta_1 hightemp + \dots + \beta_{22} hightemp^{22} + \varepsilon$

```{r}
RailTrail %>% ggplot(aes(x = hightemp, y = volume)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = y ~ poly(x,22) ) + 
  coord_cartesian(ylim = c(100,750))
```



## Muestras de entrenamiento y de prueba

* Las métricas de error (ej., $\small MSE$) suelen calcularse para predicciones de los mismos datos usados para ajustar/estimar el modelo (*in-sample prediction*)  

    + Esta muestra se denomina **muestra de entrenamiento** (*training sample*)

* PERO queremos saber qué tal se predicen *casos nuevos* (*out-sample prediction*)

* Usar las métricas en muestras de entrenamiento implica problemas de **"overfitting"**: sobreajuste a las características de la muestra concreta

    * <!--escenarios en los que--> Un modelo menos flexible podría tener menor error de predicción con casos nuevos

<!--
    * Los grados de libertad (número de valores en el modelo que son libres de variar) resume la *flexibilidad* de una curva.
-->

* Debemos calcular las métricas de error con observaciones que el modelo NO ha usado antes: **muestra de prueba** (*test sample*)

<center>
![](figure/train_test.png)
</center>


## "Overfitting"

::: {style="font-size: 85%;"} 

<!--

:::: {.columns}

::: {.column width="45%"}
![](figure/overfitting2n.png){width=99%}  

:::

::: {.column width="5%"}
\ \ 
::: 

::: {.column width="45%"}
![](figure/overfitting3n.png){width=99%} 
:::

::::

-->

<center>
![](figure/overfitting01c.png){width=70%}
</center>


<!--
* Independientemente de los datos y método, a medida que aumenta la flexibilidad 
-->
* Siempre que aumenta la flexibilidad, el MSE
    + disminuye en la muestra de entrenamiento 

    + tiene forma de U en la muestra de prueba
 
* Nota: el MSE en entrenamiento es siempre menor que en prueba

:::

## MSE en la muestra de prueba

$$
\small
E\left[\left(y-\widehat{f}(x)\right)^2\right] =
E\left[\left(f(x)-\widehat{f}(x) + \varepsilon 
+ E\left[\widehat{f}(x)\right]-E\left[\widehat{f}(x)\right] \right)^2\right] =
$$

$$
\small
=\underbrace{\left[E\left(\widehat{f}(x)\right)-f(x)\right]^2}_{(1)} + \underbrace{E\left(\left[\widehat{f}(x)-E\left(\widehat{f}(x)\right)\right]^2\right)}_{(2)}+Var(\varepsilon)
$$


* $\small (1)=\left[Sesgo\left(\widehat{f}(x)\right)\right]^2$: error por supuestos erróneos en $f$

    + ajuste insuficiente (*underfit*) al perder relaciones relevantes entre $X$ e $Y$

* $\small (2)=Var\left(\widehat{f}(x)\right)$: sensibilidad a fluctuaciones en el  entrenamiento
    + si el algoritmo modela puro ruido en entrenamiento, ajustará bien allí, pero predecirá mal casos nuevos (*overfit*)
  
<!--
Recuerdo de Econometría I:
1. sesgo por omisión de variables relevantes
2. aumento de varianza por inclusión de variable irrelevante
-->
  
## "Trade-off" Varianza--Sesgo

:::: {.columns}

::: {.column width="45%"}

<center>
![](figure/biasvariance-tradeoff2.png){width=85%}
</center>


:::


::: {.column width="45%"}
* El sesgo se reduce y la varianza aumenta con la complejidad del modelo $\Rightarrow$  encontrar un método (ej., flexibilidad) para el que ambos sean bajos


<!--
A medida que se añaden más y más parámetros a un modelo, la complejidad del modelo aumenta y la varianza se convierte en nuestra principal preocupación, mientras que el sesgo disminuye constantemente. Por ejemplo, a medida que se añaden más términos polinómicos a una regresión lineal, mayor será la complejidad del modelo resultante. 
-->

<!--
<center>
![](figure/biasvariance-tradeoff2.png){width=22%}
</center>

-->

<!--
SESGO-VARIANZA VISTO EN ECONOMETRIA I: omitir variable relevante (modelos menos flexible), crea sesgo.
                                       incluir variable no relevante (mas flexibe), aumenta varianza
-->

<!--
## "Trade-off" Varianza--Sesgo (cont.)

* Es fácil construir un modelo con bajo sesgo, pero tendrá alta varianza. Y al revés.
-->

<!--
* El desafío es encontrar un método (ej., flexibilidad del modelo) para el cual tanto la varianza como el sesgo cuadrado sean bajos
-->


* NO  es posible minimizar simultáneamente ambas fuentes de error:  *memorización* (en entrenamiento) vs. *generalización* de resultados

:::

::::

## Medir el Error en la Clasificación

* Los modelos de clasificación NO predicen directamente la categoría, sino la *probabilidad* de que una observación pertenezca a cada categoría

<!--
* Típicamente se asigna la clase predicha como aquella con mayor probabilidad. 

* En el caso binario, equivale a fijar un umbral de 0.5, pero se deberían probar varios valores del umbral
-->

* La clase predicha será aquella con mayor probabilidad. En el caso binario, implicar superar el umbral de 0.5 (se deben probar varios valores)


```{r}
censo <- read_csv("data/census.csv") %>%
  mutate(income = parse_factor(income))
logit <- glm(income ~ capital_gain, data = censo, 
             family = "binomial")
prob.predict <- predict(logit, type = "response")

umbral <- 0.5
cat.predict  <- if_else(prob.predict > umbral, 1, 0) 
cbind(censo$income, cat.predict, prob.predict) %>% head(10)
```

* Como no tiene sentido diferencia de clases (variables categóricas), NO se pueden calcular medidas como el MSE y otros relacionados

<!--
* Existen pseudo-$\small R^2$ como la correlación al cuadrado entre 

-->
## Matriz de Confusión

* Tabular categorías observadas frente a las categorías predichas 

```{r echo=FALSE, eval=TRUE, results='asis'}
library(kableExtra)
library(tidyverse)
tab <- tibble(` ` = c("POSITIVO (1)", " ", "NEGATIVO (0)", " "),
              `POSITIVO (1)` = c("Verdadero Positivo [VP]", " ", "Falso Negativo [FN]", "(Error Tipo II)"),
              `NEGATIVO (0)` = c("Falso Positivo [FP]", "(Error Tipo I)", "Verdadero Negativo [VN]", " "))

tab %>% kbl(align = "c", col.names = c(".", "POSITIVO (1)", "NEGATIVO (0)")) %>%  
  kable_paper("striped", full_width = T) %>%
  add_header_above(c(" " = 1, "CLASE OBSERVADA" = 2), bold = TRUE, font_size = "x-large") %>% 
  collapse_rows(columns = 1, valign = "middle")  %>%
  pack_rows("CLASE PREDICHA", 1, 4)
```

```{r}
table(cat.predict, censo$income)
```

* Existen varias medidas derivadas de la [matriz de confusión](https://en.wikipedia.org/wiki/Confusion_matrix)

<!--
* Una medida global para datos *imbalanced* es la *exactitud equilibrada*: $\small \frac{TVP+TVN}{2}$
-->



## Métricas con la matriz de confusión

* **Tasa de observaciones correctamente clasificadas** (exactitud o *accuracy*) 

$$
\scriptsize ACCUR=\frac{VP+VN}{VP+FP+VN+FN} 
$$

<!-- = 1 - TCE$ -->

<!--
* Su complemento es la *tasa de clasificación errónea*<!-- o de error en la clasificación --><!--: <!-- $\scriptsize  TCE=\frac{FP+FN}{VP+FP+VN+FN}$
-->

<!--
<center>
$\small TCE=\frac{FP+FN}{VP+FP+VN+FN} = \frac{1}{n}\sum_{i=1}^{n}I\left[y_i \neq \widehat{y}_i\right]$
</center>
-->

* No es informativo cuando algunas clases son infrecuentes (datos <!--*imbalanced* o--> desequilibrados)

  + si hay poco fraude/enfermos (ej., 5%), predecir que nunca hay fraude implica $\scriptsize ACCUR=95\%$, PERO NO detecta fraude/enfermedad

<!--
    - si apenas hay fraude/enfermos (ej., 5%), predecir *siempre* que no hay fraude/individuo sano implica una alta ACCUR=95% 
    
    - PERO es incapaz detectar fraude/enfermedad
-->

<!-- fraude o enfermedad -->


* El **estadístico Kappa** ($\small \kappa$) es una medida similar, pero que ajusta por lo se esperaría solo por azar (corrigiendo en parte el desequilibrio entre clases).

<!-- https://en.wikipedia.org/wiki/Cohen's_kappa -->


## Métricas con la matriz de confusión (cont.)

* La **tasa de verdaderos positivos** o **sensibilidad** (*recall*) es el porcentaje de verdaderos positivos sobre el total de positivos observados 
$$
\scriptsize TVP=SENSIT=\frac{VP}{VP+FN}
$$

    - ej., tasa de fraude/enfermos existentes que se detectan correctamente


<!-- probabilidad de detección, potencia  -->

* La **tasa de verdaderos negativos** o **especificidad** es el porcentaje de verdaderos negativos sobre el total de negativos observados
$$
\scriptsize TVN=ESPECIF=\frac{VN}{VN+FP}
$$

    - ej., tasa de "otras" opciones que se clasifican correctamente

    - **Tasa de falsos positivos**: $\scriptsize TFP = 1 - TVN = 1 - ESPECIF$

<!--

Ejemplo: prueba diagnóstica, sensibilidad cuantos enfermos es capaz de detectar

especificifciad, cuantos no enfermos es capaz de detectar corre

-->



## Métricas con la matriz de confusión (y 3)

* La **exactitud equilibrada** (*Balanced Accuracy*) es una media <!--(aritmética o geométrica)--> de la sensibilidad y de la especificidad

<!-- G-mean = sqrt(sensit * especif ) -->

* La **precisión** o valor de predicción positivo es la cantidad de verdaderos positivos sobre el total de positivos predichos 

$$
\scriptsize PREC=\frac{VP}{VP+FP}
$$

<!--
    + Tasa de falso descubrimiento: $\small 1-PREC$ 
-->

* La familia de **medidas $\small F_{\beta}$** es una ratio de la importancia ponderada de la sensibilidad y de la precisión: $\scriptsize F_{\beta}=\frac{(1+\beta)^2 \times SENSIT \times PREC}{\beta^2 \times SENSIT + PREC}$

  + Para $\scriptsize \beta<1$<!-- ($\scriptsize >1$)-->, se da menos <!--(más)--> importancia a la sensibilidad: los falsos positivos <!--(negativos)--> se consideran más costosos 

  + Para $\scriptsize \beta>1$, los falsos negativos son más costosos y para $\scriptsize \beta=1$ son igualmente costosos

## Curva ROC ("Receiver Operating Characteristic")

::: {style="font-size: 88%;"} 

<!--
https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc
-->

* Representa TVP (eje y) frente a TFP (eje x) en *diferentes umbrales* <!-- de clasificación*-->: reducir el umbral clasifica más elementos como positivos (verdaderos y falsos)

<!--(es una curva de probabilidad)-->

<!--
* Reducir el umbral clasifica más elementos como positivos, por lo que aumentan tanto los falsos positivos como los verdaderos positivos.
-->

<!--
:::: {.columns}

::: {.column width="45%"}

<center>
![](figure/ROC-AUC.svg){width=65%}
</center>

:::

::: {.column width="45%"}


<center>
![](figure/ROC.png){width=75%}
</center>


:::

::::

-->

<center>
![](figure/ROC_AUC_combined.png){width=85%}
</center>


* La curva ROC informa del grado de separabilidad: dado un nivel de TFP, el clasificador es mejor cuanto mayor sea TVP

<!--
clasificador aleatorio:  por debajo de 45º el clasificador es pesimo predice más positivos entre los negativos que entre los positivos
-->


<!--
* Con datos *imbalanced*  puede ser más informativo graficar TFP frente a precisión 
-->

<!--

![](figure/ROCCurve.svg){width=40%}
![](figure/AUC.svg){width=40%}

https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
-->

:::

## AUC ("area under the curve")

<!--
* AUC informa del grado de separabilidad: mayor AUC implica que el modelo es capaz de distinguir entre clases (predecir 0s y 1s correctamente)
-->

* La AUC es el área bajo la curva ROC: ofrece una medida agregada de rendimiento entre 0 (todas las clasificaciones incorrectas) y 1 (todas correctas) 

<!--
AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.
-->

* Resume la curva ROC y permite comparar curvas que se cruzan

<center>
![](figure/ROC_intersect.png){width=55%}
</center>



## Extensiones. Métricas adicionales


* Con más de dos clases, se realiza un análisis AUC-ROC para cada categoría (frente a las demás) <!--: se define una variable binaria para la categoría frente a todas las demás--> y se promedian<!-- la ROC y la AUC--> (ej., ponderando por casos en cada clase) <!--o no-->

<!--
https://yardstick.tidymodels.org/reference/roc_aunp.html

https://yardstick.tidymodels.org/reference/roc_aunu.html

-->


<!--
* Cuando la variable de respuesta tiene más de dos clases, 

    1. Se realiza un análisis AUC-ROC para cada categoría: se define una variable binaria para la categoría frente a todas las demás

    2. Se obtiene el promedio de tanto de la ROC como de la AUC, bien dando igual peso a cada categoría o bien ponderando el número de casos de cada una

-->

* Con clases desequilibradas, se puede preferir en lugar de la ROC un gráfico de precisión frente sensibilidad (*precision-recall*) y su correspondiente AUC (*PR-AUC*)

<!--
https://yardstick.tidymodels.org/reference/pr_auc.html

https://yardstick.tidymodels.org/reference/average_precision.html
-->

* Existen múltiples funciones de pérdida (o coste de clasificación) posibles.

  + Las relacionadas con la *curva de ganancia* consideran el coste de alcanzar un cierto nivel de sensibilidad

  + Otras se basan en la función de verosimilud o la entropía como medidas de pérdida (ej. *mean log loss*)

<!--
+ gain curve: https://yardstick.tidymodels.org/reference/gain_curve.html 

https://yardstick.tidymodels.org/reference/gain_capture.html

+ mn_log_loss: https://yardstick.tidymodels.org/reference/mn_log_loss.html

-->


## Evaluación de Modelos: entrenamiento y prueba


* Para minimizar problemas de *underfit* y, sobre todo, de *overfit*, DEBEMOS **dividir aleatoriamente** el conjunto de datos en dos partes:

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px; "}

::: {}

<center>
![](figure/train_test_split2.png){width=95%}
</center>

::: 

::: {}

* **Entrenamiento** (80-90%): datos sobre los que se construye/estima el modelo

* **Prueba**(20-10%): se usa el modelo construido para predecir y se evalúa con datos no vistos antes

::: 

::::

* ¿Por qué renunciar a parte de los datos si sabemos que un tamaño muestral grande es importante? Evaluar correctamente un modelo lo es mucho más

* La estimación del error en prueba puede ser volátil dependiendo de las observaciones incluidas en cada grupo

## Evaluación de Modelos: Validación cruzada

<!--
* Los resultados de evaluación puede verse afectados por la partición concreta obtenida (ej. incluir observaciones atípicas en la muestra de prueba)
-->

* Para evitar que los datos sean sensibles a una partición concreta, se usa validación cruzada (*cross-validation* o *rotation estimation*)

* Se repite varias veces y de forma ordenada el proceso de remuestreo para la partición en grupos de entrenamiento y prueba (similar a *bootstrap*)

* Permite utilizar todas las observaciones de la muestra, tanto para estimar como para evaluar el modelo (aunque no a la vez)

<!--
<center>
![](figure/k-crossVal_split2.png){width=55%}
</center>
-->
<!--
* Entre las variantes más habituales se encuentran:

    - Validación cruzada de K iteraciones (*K-fold cross-validation* o K-fold CV)
    
    - Validación cruzada aleatoria (*Random cross-validation*, RCV)
    
    - Validación cruzada dejando uno fuera (*Leave-one-out cross-validation*, LOOCV)
    
    - Validación cruzada dejando p fuera (*Leave-p-out cross-validation*, LpOCV)
-->

<!--
https://en.wikipedia.org/wiki/Cross-validation_(statistics)
-->

## Validación cruzada de K bloques


* Se divide, aleatoriamente y *ex-ante*, la muestra en K subconjuntos (normalmente, 5 o 10)


:::: {.columns}

::: {.column width="60%"}
<!-- De Joan.domenech91 - Trabajo propio, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=17616792 -->
<center>
![](figure/K-fold_cross_validation.jpg){heigth=150%}
</center>
:::

::: {.column width="40%"}
+ Un subconjunto se usa como prueba y el K-1 restantes como entrenamiento

+ Se repite el proceso durante k iteraciones, con cada posible subconjunto de datos de prueba. 
:::

::::


+ Se obtiene una métrica de error en cada iteración; se promedian para obtener un único resultado de evaluación


* Es el tipo más habitual de validación cruzada



## Validación cruzada aleatoria (RCV) y LOOCV

:::: {.columns}

::: {.column width="60%"}
<center>
![](figure/Random_cross_validation.jpg)
</center>
:::

::: {.column width="40%"}
+ **RCV**: en *cada iteración* se realiza la particion aleatoria (con reemplazamiento) entre entrenamiento y prueba

+ Las observaciones pueden "repetir" como prueba
:::

::::



:::: {.columns}

::: {.column width="60%"}
<center>
![](figure/Leave-one-out.jpg)
</center>

:::

::: {.column width="40%"}
+ **LOOCV** (*leave one out CV*): solo una observación se usa como prueba en cada iteración y el resto como entrenamiento

+ Se realizan $n$ iteraciones; se calcula una media sobre $n$ resultados
:::

::::

